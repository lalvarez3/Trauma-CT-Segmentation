{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djzZ-bffGr8X"
      },
      "source": [
        "#### Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "biFwQV_yGr8X",
        "outputId": "caa6e216-4409-4de4-bc5a-ceec09bf6af7",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONAI version: 0.8.1\n",
            "Numpy version: 1.21.5\n",
            "Pytorch version: 1.11.0\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
            "MONAI rev id: 71ff399a3ea07aef667b23653620a290364095b1\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: 0.4.2\n",
            "Nibabel version: 3.2.2\n",
            "scikit-image version: 0.19.2\n",
            "Pillow version: 9.0.1\n",
            "Tensorboard version: 2.8.0\n",
            "gdown version: 4.4.0\n",
            "TorchVision version: 0.12.0\n",
            "tqdm version: 4.64.0\n",
            "lmdb version: 1.3.0\n",
            "psutil version: 5.9.0\n",
            "pandas version: 1.4.2\n",
            "einops version: 0.4.1\n",
            "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from monai.data.image_reader import ImageReader, ITKReader\n",
        "from ipywidgets.widgets import *\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning\n",
        "from monai.utils import set_determinism\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    AddChanneld,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    EnsureType,\n",
        "    EnsureChannelFirstd,\n",
        "    RandFlipd,\n",
        "    RandRotated,\n",
        "    Resized\n",
        ")\n",
        "import wandb\n",
        "from monai.networks.nets import UNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceFocalLoss, GeneralizedDiceLoss\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, list_data_collate, decollate_batch, Dataset\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from monai.data import DataLoader\n",
        "import os\n",
        "import glob\n",
        "from monai.transforms.spatial.array import Resize\n",
        "\n",
        "from copy import deepcopy\n",
        "from enum import Enum\n",
        "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
        "\n",
        "from monai.config import DtypeLike, KeysCollection\n",
        "from monai.config.type_definitions import NdarrayOrTensor\n",
        "from monai.networks.layers import AffineTransform\n",
        "from monai.networks.layers.simplelayers import GaussianFilter\n",
        "from monai.transforms.croppad.array import CenterSpatialCrop, SpatialPad\n",
        "from monai.transforms.inverse import InvertibleTransform\n",
        "from monai.transforms.spatial.array import (\n",
        "    Resize,\n",
        ")\n",
        "from monai.transforms.transform import MapTransform, RandomizableTransform\n",
        "from monai.transforms.utils import create_grid\n",
        "from monai.utils import (\n",
        "    InterpolateMode,\n",
        "    ensure_tuple_rep,\n",
        ")\n",
        "from monai.utils.deprecate_utils import deprecated_arg\n",
        "from monai.utils.enums import TraceKeys\n",
        "from monai.utils.module import optional_import\n",
        "from monai.utils.type_conversion import convert_data_type, convert_to_dst_type\n",
        "from monai.apps import load_from_mmar\n",
        "from monai.apps.mmars import RemoteMMARKeys\n",
        "from monai.networks.utils import copy_model_state\n",
        "from monai.optimizers import generate_param_groups\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from pl_bolts.callbacks import PrintTableMetricsCallback\n",
        "from monai.visualize import matshow3d, blend_images\n",
        "import imageio\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InterpolateMode(Enum):\n",
        "    NEAREST = \"nearest\"\n",
        "    LINEAR = \"linear\"\n",
        "    BILINEAR = \"bilinear\"\n",
        "    BICUBIC = \"bicubic\"\n",
        "    TRILINEAR = \"trilinear\"\n",
        "    AREA = \"area\"\n",
        "\n",
        "\n",
        "InterpolateModeSequence = Union[\n",
        "    Sequence[Union[InterpolateMode, str]], InterpolateMode, str\n",
        "]\n",
        "\n",
        "class ResizedC(MapTransform, InvertibleTransform):\n",
        "\n",
        "    backend = Resize.backend\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        keys: KeysCollection,\n",
        "        spatial_size: Union[Sequence[int], int],\n",
        "        size_mode: str = \"all\",\n",
        "        mode: InterpolateModeSequence = InterpolateMode.AREA,\n",
        "        align_corners: Union[Sequence[Optional[bool]], Optional[bool]] = None,\n",
        "        allow_missing_keys: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(keys, allow_missing_keys)\n",
        "        self.mode = ensure_tuple_rep(mode, len(self.keys))\n",
        "        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))\n",
        "        self.resizer = Resize(spatial_size=spatial_size, size_mode=size_mode)\n",
        "        self.spatial_size = spatial_size\n",
        "\n",
        "    def __call__(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = dict(data)\n",
        "        for key, mode, align_corners in self.key_iterator(\n",
        "            d, self.mode, self.align_corners\n",
        "        ):\n",
        "            self.push_transform(\n",
        "                d,\n",
        "                key,\n",
        "                extra_info={\n",
        "                    \"mode\": mode.value if isinstance(mode, Enum) else mode,\n",
        "                    \"align_corners\": align_corners\n",
        "                    if align_corners is not None\n",
        "                    else TraceKeys.NONE,\n",
        "                },\n",
        "            )\n",
        "            init_slice = int(d[key].shape[-1]*0.15)\n",
        "            end_slice = int(d[key].shape[-1]*0.1)\n",
        "            # Reduce Size in Memory\n",
        "            if key == \"label\":\n",
        "                d[key] = d[key].astype(np.int8)\n",
        "                if d[key].shape[-1] > 600: d[key] = d[key][:,:,:,init_slice:-end_slice] #\n",
        "\n",
        "                if d[\"image_meta_dict\"].get(\"PatientName\", None) and d[\"image_meta_dict\"][\"PatientName\"].startswith(\"NI\") and len(d[key].shape) != 4:\n",
        "                    # print(d[key].shape)\n",
        "                    liver_channel = np.where((d[key] != 6), 0, d[key])\n",
        "                    liver_channel = np.where((liver_channel == 6), 1, liver_channel)\n",
        "                    # liver_channel = np.expand_dims(liver_channel, 0)\n",
        "                    w, h, z = self.spatial_size\n",
        "                    liver_channel = self.resizer(liver_channel, align_corners=align_corners)\n",
        "                    background = np.ones((1, z, w, h), dtype=np.float16) - liver_channel\n",
        "                    empty_injures = np.zeros((1, z, w, h), dtype=np.float16)\n",
        "                    resized = [background, liver_channel, empty_injures]\n",
        "                    d[key] = np.stack(resized).astype(np.int8).squeeze()\n",
        "\n",
        "                else:\n",
        "                    label = d[key]\n",
        "                    w, h, z = self.spatial_size\n",
        "                    resized = list()\n",
        "                    background = np.ones((1, w, h, z), dtype=np.int8)\n",
        "                    for i, channel in enumerate([0, 2]):  # TODO: desharcodead\n",
        "                        resized.append(\n",
        "                            self.resizer(\n",
        "                                np.expand_dims(label[channel, :, :, :], 0),\n",
        "                                align_corners=align_corners,\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                    background -= resized[0] # + resized[1]\n",
        "                    resized = [background] + resized\n",
        "                    d[key] = np.stack(resized).astype(np.int8).squeeze()\n",
        "            else:\n",
        "                if d[key].shape[-1] > 600: d[key] = d[key][:,:,:,init_slice:-end_slice]\n",
        "                d[key] = self.resizer(d[key], align_corners=align_corners)\n",
        "\n",
        "        keys = ['spacing', 'original_affine', 'affine', 'spatial_shape', 'original_channel_dim', 'filename_or_obj']\n",
        "        new_label_metadata = dict()\n",
        "        for key in keys:\n",
        "            new_label_metadata[key] = d[\"label_meta_dict\"].get(key, 0)\n",
        "\n",
        "        d[\"label_meta_dict\"] = new_label_metadata\n",
        "\n",
        "        if \"PatientID\" not in d[\"image_meta_dict\"]:\n",
        "            d[\"image_meta_dict\"][\"PatientID\"] = \"0\"\n",
        "        if \"PatientName\" not in d[\"image_meta_dict\"]:\n",
        "            d[\"image_meta_dict\"][\"PatientName\"] = \"0\"\n",
        "        if \"SliceThickness\" not in d[\"image_meta_dict\"]:\n",
        "            d[\"image_meta_dict\"][\"SliceThickness\"] = \"0\"\n",
        "        return d\n",
        "\n",
        "    def inverse(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = deepcopy(dict(data))\n",
        "        for key in self.key_iterator(d):\n",
        "            transform = self.get_most_recent_transform(d, key)\n",
        "            orig_size = transform[TraceKeys.ORIG_SIZE]\n",
        "            mode = transform[TraceKeys.EXTRA_INFO][\"mode\"]\n",
        "            align_corners = transform[TraceKeys.EXTRA_INFO][\"align_corners\"]\n",
        "            # Create inverse transform\n",
        "            inverse_transform = Resize(\n",
        "                spatial_size=orig_size,\n",
        "                mode=mode,\n",
        "                align_corners=None\n",
        "                if align_corners == TraceKeys.NONE\n",
        "                else align_corners,\n",
        "            )\n",
        "            # Apply inverse transform\n",
        "            d[key] = inverse_transform(d[key])\n",
        "            # Remove the applied transform\n",
        "            self.pop_transform(d, key)\n",
        "\n",
        "        return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class adaptOverlay(MapTransform, InvertibleTransform):\n",
        "\n",
        "    backend = Resize.backend\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        keys: KeysCollection,\n",
        "        size_mode: str = \"all\",\n",
        "        mode: InterpolateModeSequence = InterpolateMode.AREA,\n",
        "        align_corners: Union[Sequence[Optional[bool]], Optional[bool]] = None,\n",
        "        allow_missing_keys: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(keys, allow_missing_keys)\n",
        "        self.mode = ensure_tuple_rep(mode, len(self.keys))\n",
        "        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))\n",
        "\n",
        "    def __adapt_overlay__(self, overlay_path, mha_path, label):\n",
        "        import SimpleITK as sitk\n",
        "        if label.shape[-1] == 6:\n",
        "            return label\n",
        "        # Load the mha\n",
        "        mha_data = sitk.ReadImage(mha_path)\n",
        "        mha_org = mha_data.GetOrigin()[-1]\n",
        "        # Load the mha image\n",
        "        mha_img = sitk.GetArrayFromImage(mha_data)\n",
        "        original_z_size = mha_img.shape[0]\n",
        "\n",
        "        # Load the overlay\n",
        "        overlay_data = sitk.ReadImage(overlay_path)\n",
        "        overlay_org = overlay_data.GetOrigin()[-1]\n",
        "\n",
        "        overlay_init = np.abs(1/mha_data.GetSpacing()[-1]*(mha_org-overlay_org) )\n",
        "\n",
        "        lower_bound = int(overlay_init)\n",
        "        upper_bound = label.shape[-1]\n",
        "        zeros_up = lower_bound\n",
        "        zeros_down = original_z_size - (upper_bound + lower_bound)\n",
        "        new = list()\n",
        "\n",
        "        if zeros_up > 0:\n",
        "            new.append(np.zeros((label.shape[0], label.shape[1], zeros_up), dtype=label.dtype))\n",
        "\n",
        "        new.append(label)\n",
        "\n",
        "        if zeros_down > 0:\n",
        "            new.append(np.zeros((label.shape[0], label.shape[1], zeros_down), dtype=label.dtype))\n",
        "\n",
        "        label = np.concatenate(new, axis=2)\n",
        "\n",
        "        return label\n",
        "\n",
        "\n",
        "    def __call__(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = dict(data)\n",
        "        for key, mode, align_corners in self.key_iterator(\n",
        "            d, self.mode, self.align_corners\n",
        "        ):\n",
        "            self.push_transform(\n",
        "                d,\n",
        "                key,\n",
        "                extra_info={\n",
        "                    \"mode\": mode.value if isinstance(mode, Enum) else mode,\n",
        "                    \"align_corners\": align_corners\n",
        "                    if align_corners is not None\n",
        "                    else TraceKeys.NONE,\n",
        "                },\n",
        "            )\n",
        "            # Reduce Size in Memory\n",
        "            if key == \"label\":\n",
        "                d[key] = d[key].astype(np.int8)\n",
        "                if d[\"image_meta_dict\"].get(\"PatientName\", None) and d[\"image_meta_dict\"][\"PatientName\"].startswith(\"NI\"):\n",
        "                    file_path = d[\"label_meta_dict\"][\"filename_or_obj\"]\n",
        "                    data_path = d[\"image_meta_dict\"][\"filename_or_obj\"]\n",
        "                    d[key] = self.__adapt_overlay__(file_path, data_path, d[key])\n",
        "        return d\n",
        "\n",
        "    def inverse(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = deepcopy(dict(data))\n",
        "        for key in self.key_iterator(d):\n",
        "            transform = self.get_most_recent_transform(d, key)\n",
        "            orig_size = transform[TraceKeys.ORIG_SIZE]\n",
        "            mode = transform[TraceKeys.EXTRA_INFO][\"mode\"]\n",
        "            align_corners = transform[TraceKeys.EXTRA_INFO][\"align_corners\"]\n",
        "            # Create inverse transform\n",
        "            inverse_transform = Resize(\n",
        "                spatial_size=orig_size,\n",
        "                mode=mode,\n",
        "                align_corners=None\n",
        "                if align_corners == TraceKeys.NONE\n",
        "                else align_corners,\n",
        "            )\n",
        "            # Apply inverse transform\n",
        "            d[key] = inverse_transform(d[key])\n",
        "            # Remove the applied transform\n",
        "            self.pop_transform(d, key)\n",
        "\n",
        "        return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RemoveDicts(MapTransform, InvertibleTransform):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        keys: KeysCollection,\n",
        "        allow_missing_keys: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(keys, allow_missing_keys)\n",
        "\n",
        "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = dict(data)\n",
        "        for key in self.key_iterator(d):\n",
        "            self.push_transform(d, key)\n",
        "        a = {\"image\": d[\"image\"], \"label\": d[\"label\"], \"path\": d[\"image_meta_dict\"][\"filename_or_obj\"]}\n",
        "        d = a\n",
        "        return d\n",
        "\n",
        "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
        "        d = deepcopy(dict(data))\n",
        "        for key in self.key_iterator(d):\n",
        "            d[key] = d[key]\n",
        "            # Remove the applied transform\n",
        "            self.pop_transform(d, key)\n",
        "        return d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "PRETRAINED = True\n",
        "TRANSFER_LEARNING = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WzCJRUGGr8a"
      },
      "source": [
        "#### Define the LightningModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spleen_data\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = \"spleen_data\"\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(pytorch_lightning.LightningModule):\n",
        "    def __init__(self, train_img_size, val_img_size):\n",
        "        super().__init__()\n",
        "        self.train_img_size = train_img_size\n",
        "        self.val_img_size = val_img_size\n",
        "        if PRETRAINED:\n",
        "            print(\"using a pretrained model.\")\n",
        "            unet_model = load_from_mmar(\n",
        "                item=mmar[RemoteMMARKeys.NAME],\n",
        "                mmar_dir=root_dir,\n",
        "                # map_location=device,\n",
        "                pretrained=True,\n",
        "            )\n",
        "            self._model = unet_model\n",
        "            # copy all the pretrained weights except for variables whose name matches \"model.0.conv.unit0\"\n",
        "            if TRANSFER_LEARNING:\n",
        "                pretrained_dict, updated_keys, unchanged_keys = copy_model_state(\n",
        "                    self._model, unet_model,#  exclude_vars=\"model.[0-2].conv.unit[0-3]\"\n",
        "                )\n",
        "                print(\n",
        "                    \"num. var. using the pretrained\",\n",
        "                    len(updated_keys),\n",
        "                    \", random init\",\n",
        "                    len(unchanged_keys),\n",
        "                    \"variables.\",\n",
        "                )\n",
        "                self._model.load_state_dict(pretrained_dict)\n",
        "                # stop gradients for the pretrained weights\n",
        "                for x in self._model.named_parameters():\n",
        "                    if x[0] in updated_keys:\n",
        "                        x[1].requires_grad = True\n",
        "                params = generate_param_groups(\n",
        "                    network=self._model,\n",
        "                    layer_matches=[lambda x: x[0] in updated_keys],\n",
        "                    match_types=[\"filter\"],\n",
        "                    lr_values=[1e-4],\n",
        "                    include_others=False,\n",
        "                )\n",
        "                self.params = params\n",
        "\n",
        "        else:\n",
        "            self._model = UNet(\n",
        "                spatial_dims=3,\n",
        "                in_channels=1,\n",
        "                out_channels=2,\n",
        "                channels=(16, 32, 64, 128, 256),\n",
        "                strides=(2, 2, 2, 2),\n",
        "                num_res_units=2,\n",
        "                norm=Norm.BATCH,\n",
        "            )\n",
        "        self.loss_function = DiceFocalLoss(\n",
        "            softmax=True, include_background=False, smooth_dr=1e-7, smooth_nr=1e-7\n",
        "        )\n",
        "        self.post_pred = Compose(\n",
        "            [EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)]\n",
        "        )\n",
        "        self.post_label = Compose(\n",
        "            [EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)]\n",
        "        )\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "        self.best_val_dice = 0\n",
        "        self.best_val_epoch = 0\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # set up the correct data path\n",
        "        train_images = sorted(\n",
        "            glob.glob(\n",
        "                os.path.join(\n",
        "                    \"U:\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\", \"imagesTr\", \"*.nii.gz\"\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        train_labels = [img.replace('imagesTr', 'labelsTr') for img in train_images]\n",
        "        # train_labels = sorted(\n",
        "        #     glob.glob(\n",
        "        #         os.path.join(\n",
        "        #             \"U:\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\", \"labelsTr\", \"*.nii.gz\"\n",
        "        #         )\n",
        "        #     )\n",
        "        # ) \n",
        "        \n",
        "        data_dicts = [\n",
        "            {\"image\": image_name, \"label\": label_name}\n",
        "            for image_name, label_name in zip(train_images, train_labels)\n",
        "        ]\n",
        "        train_files, val_files = data_dicts, data_dicts\n",
        "        print(\"validation files\", val_files)\n",
        "        print(\"training files\", train_files)\n",
        "        print(\"len(train_files)\", len(train_files))\n",
        "\n",
        "        # set deterministic training for reproducibility\n",
        "        set_determinism(seed=0)\n",
        "\n",
        "        # define the data transforms\n",
        "\n",
        "        train_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                # CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                Resized(keys=[\"image\", \"label\"], spatial_size=self.train_img_size),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                # RandRotated(keys=[\"image\", \"label\"], range_x=(-15, 15), range_y=(-15, 15), range_z=(-15, 15)),\n",
        "                RemoveDicts(keys=[\"image\", \"label\"]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # define the data transforms\n",
        "        val_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"], reader=ITKReader),\n",
        "                # adaptOverlay(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                Resized(keys=[\"image\", \"label\"], spatial_size=self.val_img_size),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.train_ds = CacheDataset(\n",
        "            data=train_files,\n",
        "            transform=train_transforms,\n",
        "            cache_rate=0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "\n",
        "        self.val_ds = CacheDataset(\n",
        "            data=val_files,\n",
        "            transform=val_transforms,\n",
        "            cache_rate=0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n",
        "        if PRETRAINED:\n",
        "            optimizer = torch.optim.Adam(self.params, lr=5e-4, weight_decay=1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            self.train_ds, batch_size=2, shuffle=True, num_workers=0\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            self.val_ds, batch_size=1, shuffle=False, num_workers=0\n",
        "        )\n",
        "        return val_loader\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        output = self.forward(images)\n",
        "        loss = self.loss_function(output, labels)\n",
        "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        #  the function is called after every epoch is completed\n",
        "\n",
        "        # calculating average loss\n",
        "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "\n",
        "        # logging using tensorboard logger\n",
        "        self.log(\"dice loss\", avg_loss)\n",
        "        lnp.lnp(f\"Dice Loss: {avg_loss}\")\n",
        "\n",
        "        self.logger.experiment.log({\"dice loss\": avg_loss})\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3)])\n",
        "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 2\n",
        "        outputs = sliding_window_inference(\n",
        "            images, roi_size, sw_batch_size, self.forward\n",
        "        )\n",
        "        predicition ={\"output\": outputs, \"image\": images, \"label\": labels}\n",
        "        outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
        "\n",
        "        # if batch_idx % 2 == 0:\n",
        "        # self.logger.experiment.log({\"DICOM gif\": wandb.Image(img_path), \"epoch\": self.current_epoch})\n",
        "\n",
        "        labels = [\n",
        "            post_label(torch.unsqueeze(i, 0)).squeeze() for i in decollate_batch(labels)\n",
        "        ]\n",
        "        dice_metric = self.dice_metric(y_pred=outputs, y=labels)\n",
        "        # figure = computeROC(predicition)\n",
        "        # self.logger.experiment.log({\"ROC\": figure, \"epoch\": self.current_epoch})\n",
        "        return {\"dice_metric\": dice_metric, \"val_number\": len(outputs), \"prediction\": predicition}\n",
        "\n",
        "    def _convert2Class(self, y_pred):\n",
        "        return torch.max(y_pred, dim=1)\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        mean_val_dice = self.dice_metric.aggregate().item()\n",
        "        self.dice_metric.reset()\n",
        "        tensorboard_logs = {\n",
        "            \"dice_metric\": mean_val_dice,\n",
        "        }\n",
        "\n",
        "        predictions = [x[\"prediction\"] for x in outputs]\n",
        "\n",
        "        path_gif = make_gif(predictions)\n",
        "        self.logger.experiment.log({\"DICOM gif\": wandb.Image(path_gif)})\n",
        "            \n",
        "        if mean_val_dice > self.best_val_dice:\n",
        "            self.best_val_dice = mean_val_dice\n",
        "            self.best_val_epoch = self.current_epoch\n",
        "        print(\n",
        "            f\"current epoch: {self.current_epoch} \"\n",
        "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
        "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
        "            f\"at epoch: {self.best_val_epoch}\"\n",
        "        )\n",
        "        self.log(\"dice_metric\", mean_val_dice)\n",
        "        return {\"log\": tensorboard_logs, \"predictions\": predictions}\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "mmar = {\n",
        "    RemoteMMARKeys.ID: \"clara_pt_liver_and_tumor_ct_segmentation_1\",\n",
        "    RemoteMMARKeys.NAME: \"clara_pt_liver_and_tumor_ct_segmentation\",\n",
        "    RemoteMMARKeys.FILE_TYPE: \"zip\",\n",
        "    RemoteMMARKeys.HASH_TYPE: \"md5\",\n",
        "    RemoteMMARKeys.HASH_VAL: None,\n",
        "    RemoteMMARKeys.MODEL_FILE: os.path.join(\"models\", \"model.pt\"),\n",
        "    RemoteMMARKeys.CONFIG_FILE: os.path.join(\"config\", \"config_train.json\"),\n",
        "    RemoteMMARKeys.VERSION: 1,\n",
        "}\n",
        "\n",
        "def save_checkpoint(state, name):\n",
        "    file_path = \"checkpoints/\"\n",
        "    if not os.path.exists(file_path): \n",
        "        os.makedirs(file_path)\n",
        "    epoch = state[\"epoch\"]\n",
        "    save_dir = file_path + name + str(epoch)\n",
        "    torch.save(state, save_dir)\n",
        "    print(f\"Saving checkpoint for epoch {epoch} in: {save_dir}\")\n",
        "\n",
        "def save_state_dict(state, name):\n",
        "    file_path = \"checkpoints/\"\n",
        "    if not os.path.exists(file_path):\n",
        "        os.makedirs(file_path)\n",
        "\n",
        "    save_dir = file_path + f\"{name}_best\"\n",
        "    torch.save(state, save_dir)\n",
        "    print(f\"Best accuracy so far. Saving model to:{save_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Log_and_print:\n",
        "    def __init__(self, run_name, tb_logger=None):\n",
        "        self.tb_logger = tb_logger\n",
        "        self.run_name = run_name\n",
        "        self.str_log = \"run_name\" + \"\\n  \\n\"\n",
        "\n",
        "    def lnp(self, tag):\n",
        "        print(self.run_name, time.asctime(), tag)\n",
        "        self.str_log += str(time.asctime()) + \" \" + str(tag) + \"  \\n\"\n",
        "\n",
        "    def dump_to_tensorboard(self):\n",
        "        if not self.tb_logger:\n",
        "            print(\"No tensorboard logger\")\n",
        "        self.tb_logger.experiment.add_text(\"log\", self.str_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Gif Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_gif(predictions):\n",
        "    volumes = []\n",
        "    for prediction in predictions:\n",
        "        selected = prediction\n",
        "        print('true label:', selected['label'].shape)\n",
        "        pred = torch.argmax(selected['output'], dim=1).detach().cpu().numpy()\n",
        "        true_label = torch.sum(selected['label'][:,1:,:,:,:], dim=1).detach().cpu().numpy()\n",
        "        image = selected['image'][0].cpu().numpy()\n",
        "        print('true label:', true_label.shape)\n",
        "        \n",
        "        blended_true_label = blend_images(image, true_label)\n",
        "        blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
        "\n",
        "        blended_prediction = blend_images(image, pred)\n",
        "        blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
        "\n",
        "        volume_pred = blended_final_prediction[:,:,:,:]\n",
        "        volume_label = blended_final_true_label[:,:,:,:]\n",
        "        volume_pred = np.squeeze(volume_pred).permute(3,0,1,2)\n",
        "        volume_label = np.squeeze(volume_label).permute(3,0,1,2)\n",
        "        volume = torch.hstack((volume_pred, volume_label)).numpy()\n",
        "        volumes.append(volume)\n",
        "    volume = np.hstack((volumes))\n",
        "    data = volume.astype(np.float64) / np.max(volume) # normalize the data to 0 - 1\n",
        "    data = 255 * data # Now scale by 255\n",
        "    volume = data.astype(np.uint8)\n",
        "    path_to_gif = f'gifs\\\\prediction.gif'\n",
        "    if not os.path.exists(\"gifs\\\\\"):\n",
        "        os.mkdir(\"gifs\\\\\")\n",
        "    imageio.mimsave(path_to_gif, volume)\n",
        "    return path_to_gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def computeROC(predicition):\n",
        "    pred = torch.argmax(predicition['output'], dim=1).detach().cpu().numpy()\n",
        "    label = torch.sum(predicition['label'][:,1:,:,:,:], dim=1).detach().cpu().numpy()\n",
        "    from sklearn.metrics import roc_curve, auc # roc curve tools\n",
        "    ground_truth_labels = label.ravel() # we want to make them into vectors|\n",
        "    score_value = 1-pred.ravel()/255.0 # we want to make them into vectors\n",
        "    fpr, tpr, _ = roc_curve(ground_truth_labels,score_value)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    ax.plot([0, 1], [0, 1], 'k--')\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title('ROC Curve')\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhnD1E-uGr8c"
      },
      "source": [
        "## Run the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SEED = 0\n",
        "IMG_SIZE = (160,160,160)\n",
        "VAL_SIZE = (256,256,256)\n",
        "SAVE_PATH = \"lightning_logs/\"\n",
        "run_idx = len(os.listdir(\"wandb\"))\n",
        "RUN_NAME = f\"Predict_Segmentation_{run_idx+1}\"\n",
        "pytorch_lightning.seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    wandb.finish()\n",
        "except:\n",
        "    print(\"Wandb not initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predict_Segmentation_65 Wed May 25 17:53:47 2022 Loggers start\n",
            "Predict_Segmentation_65 Wed May 25 17:53:47 2022 ts_script: 1653494027.3077605\n"
          ]
        }
      ],
      "source": [
        "lnp = Log_and_print(RUN_NAME)\n",
        "lnp.lnp(\"Loggers start\")\n",
        "lnp.lnp(\"ts_script: \" + str(time.time()))\n",
        "\n",
        "# Start Wandb\n",
        "wandb_logger = pytorch_lightning.loggers.WandbLogger(\n",
        "    project=\"traumaIA\",\n",
        "    # save_dir=SAVE_DIR + \"log/\",\n",
        "    name=RUN_NAME,\n",
        "    # version=\"fixed_version\",\n",
        "    # sync_tensorboard=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CALLBACKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predict_Segmentation_65 Wed May 25 17:53:49 2022 MAIN callbacks\n",
            "Predict_Segmentation_65 Wed May 25 17:53:49 2022 checkpoint_dirpath: lightning_logs/checkpoints/\n",
            "Predict_Segmentation_65 Wed May 25 17:53:49 2022 checkpoint_filename: lightning_logs_Predict_Segmentation_65\n"
          ]
        }
      ],
      "source": [
        "lnp.lnp(\"MAIN callbacks\")\n",
        "l_callbacks = []\n",
        "cbEarlyStopping = pytorch_lightning.callbacks.early_stopping.EarlyStopping(\n",
        "    monitor=\"dice_metric\", patience=30, mode=\"max\"\n",
        ")\n",
        "l_callbacks.append(cbEarlyStopping)\n",
        "\n",
        "\n",
        "checkpoint_dirpath = SAVE_PATH + \"checkpoints/\"\n",
        "checkpoint_filename = SAVE_PATH[:-1] + \"_\" + RUN_NAME\n",
        "lnp.lnp(\"checkpoint_dirpath: \" + checkpoint_dirpath)\n",
        "lnp.lnp(\"checkpoint_filename: \" + checkpoint_filename)\n",
        "cbModelCheckpoint = pytorch_lightning.callbacks.ModelCheckpoint(\n",
        "    monitor=\"dice_metric\", mode=\"max\", dirpath=checkpoint_dirpath, filename=checkpoint_filename, \n",
        ")\n",
        "l_callbacks.append(cbModelCheckpoint)\n",
        "\n",
        "l_callbacks.append(PrintTableMetricsCallback())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predict_Segmentation_65 Wed May 25 17:56:45 2022  Start Trainining process...\n",
            "using a pretrained model.\n",
            "2022-05-25 17:56:47,127 - INFO - Expected md5 is None, skip md5 check for file spleen_data\\clara_pt_liver_and_tumor_ct_segmentation_4.1.zip.\n",
            "2022-05-25 17:56:47,128 - INFO - File exists: spleen_data\\clara_pt_liver_and_tumor_ct_segmentation_4.1.zip, skipped downloading.\n",
            "2022-05-25 17:56:47,131 - INFO - Non-empty folder exists in spleen_data\\clara_pt_liver_and_tumor_ct_segmentation, skipped extracting.\n",
            "2022-05-25 17:56:47,133 - INFO - \n",
            "*** \"clara_pt_liver_and_tumor_ct_segmentation\" available at spleen_data\\clara_pt_liver_and_tumor_ct_segmentation.\n",
            "2022-05-25 17:56:47,252 - INFO - *** Model: <class 'monai.networks.nets.unet.UNet'>\n",
            "2022-05-25 17:56:47,291 - INFO - *** Model params: {'dimensions': 3, 'in_channels': 1, 'out_channels': 3, 'channels': [16, 32, 64, 128, 256], 'strides': [2, 2, 2, 2], 'num_res_units': 2, 'norm': 'batch'}\n",
            "2022-05-25 17:56:47,314 - INFO - \n",
            "---\n",
            "2022-05-25 17:56:47,315 - INFO - For more information, please visit https://ngc.nvidia.com/catalog/models/nvidia:med:clara_pt_liver_and_tumor_ct_segmentation\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation files [{'image': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\imagesTr\\\\TRMLIV_000_0000.nii.gz', 'label': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\labelsTr\\\\TRMLIV_000_0000.nii.gz'}, {'image': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\imagesTr\\\\TRMLIV_070_0000.nii.gz', 'label': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\labelsTr\\\\TRMLIV_070_0000.nii.gz'}, {'image': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\imagesTr\\\\TRMLIV_073_0000.nii.gz', 'label': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\labelsTr\\\\TRMLIV_073_0000.nii.gz'}]\n",
            "training files [{'image': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\imagesTr\\\\TRMLIV_000_0000.nii.gz', 'label': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\labelsTr\\\\TRMLIV_000_0000.nii.gz'}, {'image': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\imagesTr\\\\TRMLIV_070_0000.nii.gz', 'label': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\labelsTr\\\\TRMLIV_070_0000.nii.gz'}, {'image': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\imagesTr\\\\TRMLIV_073_0000.nii.gz', 'label': 'U:lauraalvarez\\\\nnunet\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task501_LiverTrauma\\\\labelsTr\\\\TRMLIV_073_0000.nii.gz'}]\n",
            "len(train_files) 3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "464b80361da24c159e686ad798412381",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "true label: torch.Size([1, 1, 256, 256, 256])\n",
            "true label: (1, 256, 256, 256)\n",
            "true label: torch.Size([1, 1, 256, 256, 256])\n",
            "true label: (1, 256, 256, 256)\n",
            "true label: torch.Size([1, 1, 256, 256, 256])\n",
            "true label: (1, 256, 256, 256)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric\n",
            "───────────\n",
            "0.7903408408164978\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 0 current mean dice: 0.7903\n",
            "best mean dice: 0.7903 at epoch: 0\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 VALIDATE RESULTS\n",
            "{'dice_metric': 0.7903408408164978}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# initialise the LightningModule\n",
        "lnp.lnp(\" Start Trainining process...\")\n",
        "net = Net(train_img_size=IMG_SIZE, val_img_size=VAL_SIZE)#.load_from_checkpoint(\"lightning_logs\\checkpoints\\lightning_logs_Segmentation_14.ckpt\", train_img_size=IMG_SIZE, val_img_size=VAL_SIZE)\n",
        "wandb_logger.watch(net)\n",
        "\n",
        "# set up loggers and checkpoints\n",
        "log_dir = os.path.join(root_dir, \"logs\")\n",
        "\n",
        "# initialise Lightning's trainer.\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    gpus=[0],\n",
        "    max_epochs=40,\n",
        "    logger=wandb_logger,\n",
        "    enable_checkpointing=True,\n",
        "    num_sanity_val_steps=0,\n",
        "    log_every_n_steps=1,\n",
        "    callbacks=l_callbacks,\n",
        ")\n",
        "\n",
        "# train\n",
        "result_validation = trainer.validate(net)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7c8ccbe3e4041bebe2075050c7bd5f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='33.388 MB of 33.388 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dice_metric</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dice_metric</td><td>0.79034</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>trainer/global_step</td><td>0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">Predict_Segmentation_65</strong>: <a href=\"https://wandb.ai/lalvarez/traumaIA/runs/m2mjxcpj\" target=\"_blank\">https://wandb.ai/lalvarez/traumaIA/runs/m2mjxcpj</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20220525_175355-m2mjxcpj\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Run learning rate finder\n",
        "lr_finder = trainer.tuner.lr_find(net)\n",
        "\n",
        "# Results can be found in\n",
        "lr_finder.results\n",
        "\n",
        "# Plot with\n",
        "fig = lr_finder.plot(suggest=True)\n",
        "fig.show()      \n",
        "\n",
        "# Pick point based on plot, or get suggestion\n",
        "new_lr = lr_finder.suggestion()\n",
        "\n",
        "# update hparams of the model\n",
        "net.hparams.lr = new_lr\n",
        "\n",
        "# Fit model\n",
        "trainer.fit(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DG9mdV0IGr8d",
        "outputId": "e6ed0616-1270-44ae-b685-4e75b4088066",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train completed, best_metric: 0.0686 at epoch 2\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f\"train completed, best_metric: {net.best_val_dice:.4f} \"\n",
        "    f\"at epoch {net.best_val_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{}]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoGZ_9q3Gr8e"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoUeVHa5Gr8e"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "spleen_segmentation_3d_lightning.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f756ff16342a8157e6f46b879d688029dcc5bc6cd621b2b84934bbdd850a743a"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('env_trauma')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
