{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djzZ-bffGr8X"
      },
      "source": [
        "#### Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "biFwQV_yGr8X",
        "outputId": "caa6e216-4409-4de4-bc5a-ceec09bf6af7",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONAI version: 0.8.1\n",
            "Numpy version: 1.22.3\n",
            "Pytorch version: 1.11.0\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
            "MONAI rev id: 71ff399a3ea07aef667b23653620a290364095b1\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 3.2.2\n",
            "scikit-image version: 0.19.2\n",
            "Pillow version: 9.0.1\n",
            "Tensorboard version: 2.9.0\n",
            "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "TorchVision version: 0.12.0\n",
            "tqdm version: 4.64.0\n",
            "lmdb version: 1.3.0\n",
            "psutil version: 5.9.0\n",
            "pandas version: 1.4.2\n",
            "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from monai.data.image_reader import ImageReader, ITKReader\n",
        "from ipywidgets.widgets import *\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning\n",
        "from monai.utils import set_determinism\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    AddChanneld,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    EnsureType,\n",
        "    EnsureChannelFirstd,\n",
        "    RandFlipd,\n",
        "    RandRotated,\n",
        "    ToTensord,\n",
        "    Resized\n",
        ")\n",
        "import wandb\n",
        "from monai.networks.nets import UNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceFocalLoss, GeneralizedDiceLoss\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, list_data_collate, decollate_batch, Dataset, LMDBDataset\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from monai.data import DataLoader\n",
        "import os\n",
        "import glob\n",
        "from monai.transforms.spatial.array import Resize\n",
        "\n",
        "from copy import deepcopy\n",
        "from enum import Enum\n",
        "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
        "\n",
        "from monai.config import DtypeLike, KeysCollection\n",
        "from monai.config.type_definitions import NdarrayOrTensor\n",
        "from monai.networks.layers import AffineTransform\n",
        "from monai.networks.layers.simplelayers import GaussianFilter\n",
        "from monai.transforms.croppad.array import CenterSpatialCrop, SpatialPad\n",
        "from monai.transforms.inverse import InvertibleTransform\n",
        "from monai.transforms.spatial.array import (\n",
        "    Resize,\n",
        ")\n",
        "from monai.transforms.transform import MapTransform, RandomizableTransform\n",
        "from monai.transforms.utils import create_grid\n",
        "from monai.utils import (\n",
        "    InterpolateMode,\n",
        "    ensure_tuple_rep,\n",
        ")\n",
        "from monai.utils.deprecate_utils import deprecated_arg\n",
        "from monai.utils.enums import TraceKeys\n",
        "from monai.utils.module import optional_import\n",
        "from monai.utils.type_conversion import convert_data_type, convert_to_dst_type\n",
        "from monai.apps import load_from_mmar\n",
        "from monai.apps.mmars import RemoteMMARKeys\n",
        "from monai.networks.utils import copy_model_state\n",
        "from monai.optimizers import generate_param_groups\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from pl_bolts.callbacks import PrintTableMetricsCallback\n",
        "from monai.visualize import matshow3d, blend_images\n",
        "import imageio\n",
        "print_config()\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InterpolateMode(Enum):\n",
        "    NEAREST = \"nearest\"\n",
        "    LINEAR = \"linear\"\n",
        "    BILINEAR = \"bilinear\"\n",
        "    BICUBIC = \"bicubic\"\n",
        "    TRILINEAR = \"trilinear\"\n",
        "    AREA = \"area\"\n",
        "\n",
        "\n",
        "InterpolateModeSequence = Union[\n",
        "    Sequence[Union[InterpolateMode, str]], InterpolateMode, str\n",
        "]\n",
        "\n",
        "class ResizedC(MapTransform, InvertibleTransform):\n",
        "\n",
        "    backend = Resize.backend\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        keys: KeysCollection,\n",
        "        spatial_size: Union[Sequence[int], int],\n",
        "        size_mode: str = \"all\",\n",
        "        mode: InterpolateModeSequence = InterpolateMode.AREA,\n",
        "        align_corners: Union[Sequence[Optional[bool]], Optional[bool]] = None,\n",
        "        allow_missing_keys: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(keys, allow_missing_keys)\n",
        "        self.mode = ensure_tuple_rep(mode, len(self.keys))\n",
        "        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))\n",
        "        self.resizer = Resize(spatial_size=spatial_size, size_mode=size_mode)\n",
        "        self.spatial_size = spatial_size\n",
        "\n",
        "    def __call__(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = dict(data)\n",
        "        for key, mode, align_corners in self.key_iterator(\n",
        "            d, self.mode, self.align_corners\n",
        "        ):\n",
        "            self.push_transform(\n",
        "                d,\n",
        "                key,\n",
        "                extra_info={\n",
        "                    \"mode\": mode.value if isinstance(mode, Enum) else mode,\n",
        "                    \"align_corners\": align_corners\n",
        "                    if align_corners is not None\n",
        "                    else TraceKeys.NONE,\n",
        "                },\n",
        "            )\n",
        "            init_slice = int(d[key].shape[-1]*0.15)\n",
        "            end_slice = int(d[key].shape[-1]*0.1)\n",
        "            # Reduce Size in Memory\n",
        "            if key == \"label\":\n",
        "                d[key] = d[key].astype(np.int8)\n",
        "                if d[key].shape[-1] > 600: d[key] = d[key][:,:,:,init_slice:-end_slice] #\n",
        "\n",
        "                if d[\"image_meta_dict\"].get(\"PatientName\", None) and d[\"image_meta_dict\"][\"PatientName\"].startswith(\"NI\") and len(d[key].shape) != 4:\n",
        "                    # print(d[key].shape)\n",
        "                    liver_channel = np.where((d[key] != 6), 0, d[key])\n",
        "                    liver_channel = np.where((liver_channel == 6), 1, liver_channel)\n",
        "                    # liver_channel = np.expand_dims(liver_channel, 0)\n",
        "                    w, h, z = self.spatial_size\n",
        "                    liver_channel = self.resizer(liver_channel, align_corners=align_corners)\n",
        "                    background = np.ones((1, z, w, h), dtype=np.float16) - liver_channel\n",
        "                    empty_injures = np.zeros((1, z, w, h), dtype=np.float16)\n",
        "                    resized = [background, liver_channel, empty_injures]\n",
        "                    d[key] = np.stack(resized).astype(np.int8).squeeze()\n",
        "\n",
        "                else:\n",
        "                    label = d[key]\n",
        "                    w, h, z = self.spatial_size\n",
        "                    resized = list()\n",
        "                    background = np.ones((1, w, h, z), dtype=np.int8)\n",
        "                    for i, channel in enumerate([0, 2]):  # TODO: desharcodead\n",
        "                        resized.append(\n",
        "                            self.resizer(\n",
        "                                np.expand_dims(label[channel, :, :, :], 0),\n",
        "                                align_corners=align_corners,\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                    background -= resized[0] # + resized[1]\n",
        "                    resized = [background] + resized\n",
        "                    d[key] = np.stack(resized).astype(np.int8).squeeze()\n",
        "            else:\n",
        "                if d[key].shape[-1] > 600: d[key] = d[key][:,:,:,init_slice:-end_slice]\n",
        "                d[key] = self.resizer(d[key], align_corners=align_corners)\n",
        "\n",
        "        keys = ['spacing', 'original_affine', 'affine', 'spatial_shape', 'original_channel_dim', 'filename_or_obj']\n",
        "        new_label_metadata = dict()\n",
        "        for key in keys:\n",
        "            new_label_metadata[key] = d[\"label_meta_dict\"].get(key, 0)\n",
        "\n",
        "        d[\"label_meta_dict\"] = new_label_metadata\n",
        "\n",
        "        if \"PatientID\" not in d[\"image_meta_dict\"]:\n",
        "            d[\"image_meta_dict\"][\"PatientID\"] = \"0\"\n",
        "        if \"PatientName\" not in d[\"image_meta_dict\"]:\n",
        "            d[\"image_meta_dict\"][\"PatientName\"] = \"0\"\n",
        "        if \"SliceThickness\" not in d[\"image_meta_dict\"]:\n",
        "            d[\"image_meta_dict\"][\"SliceThickness\"] = \"0\"\n",
        "        return d\n",
        "\n",
        "    def inverse(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = deepcopy(dict(data))\n",
        "        for key in self.key_iterator(d):\n",
        "            transform = self.get_most_recent_transform(d, key)\n",
        "            orig_size = transform[TraceKeys.ORIG_SIZE]\n",
        "            mode = transform[TraceKeys.EXTRA_INFO][\"mode\"]\n",
        "            align_corners = transform[TraceKeys.EXTRA_INFO][\"align_corners\"]\n",
        "            # Create inverse transform\n",
        "            inverse_transform = Resize(\n",
        "                spatial_size=orig_size,\n",
        "                mode=mode,\n",
        "                align_corners=None\n",
        "                if align_corners == TraceKeys.NONE\n",
        "                else align_corners,\n",
        "            )\n",
        "            # Apply inverse transform\n",
        "            d[key] = inverse_transform(d[key])\n",
        "            # Remove the applied transform\n",
        "            self.pop_transform(d, key)\n",
        "\n",
        "        return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "class adaptOverlay(MapTransform, InvertibleTransform):\n",
        "\n",
        "    backend = Resize.backend\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        keys: KeysCollection,\n",
        "        size_mode: str = \"all\",\n",
        "        mode: InterpolateModeSequence = InterpolateMode.AREA,\n",
        "        align_corners: Union[Sequence[Optional[bool]], Optional[bool]] = None,\n",
        "        allow_missing_keys: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(keys, allow_missing_keys)\n",
        "        self.mode = ensure_tuple_rep(mode, len(self.keys))\n",
        "        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))\n",
        "\n",
        "    def __adapt_overlay__(self, overlay_path, mha_path, label):\n",
        "        import SimpleITK as sitk\n",
        "        if label.shape[-1] == 6:\n",
        "            return label\n",
        "        # Load the mha\n",
        "        mha_data = sitk.ReadImage(mha_path)\n",
        "        mha_org = mha_data.GetOrigin()[-1]\n",
        "        # Load the mha image\n",
        "        mha_img = sitk.GetArrayFromImage(mha_data)\n",
        "        original_z_size = mha_img.shape[0]\n",
        "\n",
        "        # Load the overlay\n",
        "        overlay_data = sitk.ReadImage(overlay_path)\n",
        "        overlay_org = overlay_data.GetOrigin()[-1]\n",
        "\n",
        "        overlay_init = np.abs(1/mha_data.GetSpacing()[-1]*(mha_org-overlay_org) )\n",
        "\n",
        "        lower_bound = int(overlay_init)\n",
        "        upper_bound = label.shape[-1]\n",
        "        zeros_up = lower_bound\n",
        "        zeros_down = original_z_size - (upper_bound + lower_bound)\n",
        "        new = list()\n",
        "\n",
        "        if zeros_up > 0:\n",
        "            new.append(np.zeros((label.shape[0], label.shape[1], zeros_up), dtype=label.dtype))\n",
        "\n",
        "        new.append(label)\n",
        "\n",
        "        if zeros_down > 0:\n",
        "            new.append(np.zeros((label.shape[0], label.shape[1], zeros_down), dtype=label.dtype))\n",
        "\n",
        "        label = np.concatenate(new, axis=2)\n",
        "\n",
        "        return label\n",
        "\n",
        "\n",
        "    def __call__(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = dict(data)\n",
        "        for key, mode, align_corners in self.key_iterator(\n",
        "            d, self.mode, self.align_corners\n",
        "        ):\n",
        "            self.push_transform(\n",
        "                d,\n",
        "                key,\n",
        "                extra_info={\n",
        "                    \"mode\": mode.value if isinstance(mode, Enum) else mode,\n",
        "                    \"align_corners\": align_corners\n",
        "                    if align_corners is not None\n",
        "                    else TraceKeys.NONE,\n",
        "                },\n",
        "            )\n",
        "            # Reduce Size in Memory\n",
        "            if key == \"label\":\n",
        "                d[key] = d[key].astype(np.int8)\n",
        "                if d[\"image_meta_dict\"].get(\"PatientName\", None) and d[\"image_meta_dict\"][\"PatientName\"].startswith(\"NI\"):\n",
        "                    file_path = d[\"label_meta_dict\"][\"filename_or_obj\"]\n",
        "                    data_path = d[\"image_meta_dict\"][\"filename_or_obj\"]\n",
        "                    d[key] = self.__adapt_overlay__(file_path, data_path, d[key])\n",
        "        return d\n",
        "\n",
        "    def inverse(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = deepcopy(dict(data))\n",
        "        for key in self.key_iterator(d):\n",
        "            transform = self.get_most_recent_transform(d, key)\n",
        "            orig_size = transform[TraceKeys.ORIG_SIZE]\n",
        "            mode = transform[TraceKeys.EXTRA_INFO][\"mode\"]\n",
        "            align_corners = transform[TraceKeys.EXTRA_INFO][\"align_corners\"]\n",
        "            # Create inverse transform\n",
        "            inverse_transform = Resize(\n",
        "                spatial_size=orig_size,\n",
        "                mode=mode,\n",
        "                align_corners=None\n",
        "                if align_corners == TraceKeys.NONE\n",
        "                else align_corners,\n",
        "            )\n",
        "            # Apply inverse transform\n",
        "            d[key] = inverse_transform(d[key])\n",
        "            # Remove the applied transform\n",
        "            self.pop_transform(d, key)\n",
        "\n",
        "        return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RemoveDicts(MapTransform, InvertibleTransform):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        keys: KeysCollection,\n",
        "        allow_missing_keys: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(keys, allow_missing_keys)\n",
        "\n",
        "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = dict(data)\n",
        "        for key in self.key_iterator(d):\n",
        "            self.push_transform(d, key)\n",
        "        a = {\"image\": d[\"image\"], \"label\": d[\"label\"], \"path\": d[\"image_meta_dict\"][\"filename_or_obj\"]}\n",
        "        d = a\n",
        "        return d\n",
        "\n",
        "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
        "        d = deepcopy(dict(data))\n",
        "        for key in self.key_iterator(d):\n",
        "            d[key] = d[key]\n",
        "            # Remove the applied transform\n",
        "            self.pop_transform(d, key)\n",
        "        return d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "PRETRAINED = True\n",
        "TRANSFER_LEARNING = True\n",
        "N_WORKERS_LOADER = 0\n",
        "N_WORKERS_CACHE = 4\n",
        "CACHE_RATE = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WzCJRUGGr8a"
      },
      "source": [
        "#### Define the LightningModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spleen_data\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = \"spleen_data\"\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.6667, 0.6667])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dl.aggregate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(pytorch_lightning.LightningModule):\n",
        "    def __init__(self, train_img_size, val_img_size):\n",
        "        super().__init__()\n",
        "        self.train_img_size = train_img_size\n",
        "        self.val_img_size = val_img_size\n",
        "        if PRETRAINED:\n",
        "            print(\"using a pretrained model.\")\n",
        "            unet_model = load_from_mmar(\n",
        "                item=mmar[RemoteMMARKeys.NAME],\n",
        "                mmar_dir=root_dir,\n",
        "                # map_location=device,\n",
        "                pretrained=True,\n",
        "            )\n",
        "            self._model = unet_model\n",
        "            # copy all the pretrained weights except for variables whose name matches \"model.0.conv.unit0\"\n",
        "            if TRANSFER_LEARNING:\n",
        "                pretrained_dict, updated_keys, unchanged_keys = copy_model_state(\n",
        "                    self._model, unet_model,#  exclude_vars=\"model.[0-2].conv.unit[0-3]\"\n",
        "                )\n",
        "                print(\n",
        "                    \"num. var. using the pretrained\",\n",
        "                    len(updated_keys),\n",
        "                    \", random init\",\n",
        "                    len(unchanged_keys),\n",
        "                    \"variables.\",\n",
        "                )\n",
        "                self._model.load_state_dict(pretrained_dict)\n",
        "                # stop gradients for the pretrained weights\n",
        "                for x in self._model.named_parameters():\n",
        "                    if x[0] in updated_keys:\n",
        "                        x[1].requires_grad = True\n",
        "                params = generate_param_groups(\n",
        "                    network=self._model,\n",
        "                    layer_matches=[lambda x: x[0] in updated_keys],\n",
        "                    match_types=[\"filter\"],\n",
        "                    lr_values=[1e-4],\n",
        "                    include_others=False,\n",
        "                )\n",
        "                self.params = params\n",
        "\n",
        "        else:\n",
        "            self._model = UNet(\n",
        "                spatial_dims=3,\n",
        "                in_channels=1,\n",
        "                out_channels=3,\n",
        "                channels=(16, 32, 64, 128, 256),\n",
        "                strides=(2, 2, 2, 2),\n",
        "                num_res_units=2,\n",
        "                norm=Norm.BATCH,\n",
        "            )\n",
        "        self.loss_function = DiceCELoss(softmax=True, to_onehot_y=True)\n",
        "        self.post_pred = Compose(\n",
        "            [EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)]\n",
        "        )\n",
        "        self.post_label = Compose(\n",
        "            [EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)]\n",
        "        )\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
        "        self.best_val_dice = 0\n",
        "        self.best_val_epoch = 0\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # set up the correct data path\n",
        "        train_images = sorted(\n",
        "            glob.glob(\n",
        "                os.path.join(\n",
        "                    \"/mnt/chansey/\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\", \"imagesTr\", \"*.nii.gz\"\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        train_labels = [img.replace('imagesTr', 'labelsTr') for img in train_images]\n",
        "        \n",
        "        data_dicts = [\n",
        "            {\"image\": image_name, \"label\": label_name}\n",
        "            for image_name, label_name in zip(train_images, train_labels)\n",
        "        ]\n",
        "\n",
        "        random.shuffle(data_dicts)\n",
        "        train_files, val_files = data_dicts[:-5], data_dicts[-5:]\n",
        "        print(\"validation files\", val_files)\n",
        "        # print(\"training files\", train_files)\n",
        "        print(\"len(train_files)\", len(train_files))\n",
        "        print(\"len(validation files)\", len(val_files))\n",
        "\n",
        "        # set deterministic training for reproducibility\n",
        "        set_determinism(seed=0)\n",
        "\n",
        "        # define the data transforms\n",
        "\n",
        "        train_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                AddChanneld(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                Resized(keys=[\"image\", \"label\"], spatial_size=self.train_img_size),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                RemoveDicts(keys=[\"image\", \"label\"]),\n",
        "                ToTensord(keys=[\"image\", \"label\"]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # define the data transforms\n",
        "        val_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"], reader=ITKReader),\n",
        "                AddChanneld(keys=[\"image\", \"label\"]),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                Resized(keys=[\"image\", \"label\"], spatial_size=self.val_img_size),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                RemoveDicts(keys=[\"image\", \"label\"]),\n",
        "                ToTensord(keys=[\"image\", \"label\"]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # self.train_ds = CacheDataset(\n",
        "        #     data=train_files,\n",
        "        #     transform=train_transforms,\n",
        "        #     cache_rate=CACHE_RATE,\n",
        "        #     num_workers=N_WORKERS_CACHE,\n",
        "        # )\n",
        "\n",
        "        self.train_ds = LMDBDataset(\n",
        "            data=train_files,\n",
        "            transform=train_transforms,\n",
        "            cache_dir=os.path.join(\n",
        "                    \"/mnt/chansey/\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\"\n",
        "                )\n",
        "        )\n",
        "\n",
        "        self.val_ds = LMDBDataset(\n",
        "            data=val_files,\n",
        "            transform=val_transforms,\n",
        "            cache_dir=os.path.join(\n",
        "                    \"/mnt/chansey/\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\"\n",
        "                )\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n",
        "        if PRETRAINED:\n",
        "            optimizer = torch.optim.Adam(self.params, lr=5e-4, weight_decay=1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            self.train_ds, batch_size=2, shuffle=True, num_workers=N_WORKERS_LOADER\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            self.val_ds, batch_size=1, shuffle=False, num_workers=N_WORKERS_LOADER\n",
        "        )\n",
        "        return val_loader\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        predict_dataloader = torch.utils.data.DataLoader(\n",
        "            self.val_ds, batch_size=1, shuffle=False, num_workers=N_WORKERS_LOADER\n",
        "        )\n",
        "        return predict_dataloader\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        output = self.forward(images)\n",
        "        loss = self.loss_function(output, labels)\n",
        "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        #  the function is called after every epoch is completed\n",
        "\n",
        "        # calculating average loss\n",
        "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "\n",
        "        # logging using tensorboard logger\n",
        "        self.log(\"dice loss\", avg_loss)\n",
        "        lnp.lnp(f\"Dice Loss: {avg_loss}\")\n",
        "\n",
        "        self.logger.experiment.log({\"dice loss\": avg_loss})\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        filenames = batch[\"path\"]\n",
        "        post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3)])\n",
        "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 2\n",
        "        outputs = sliding_window_inference(\n",
        "            images, roi_size, sw_batch_size, self.forward\n",
        "        )\n",
        "        predicition ={\"output\": outputs, \"image\": images, \"label\": labels, \"filename\": filenames}\n",
        "        outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
        "\n",
        "        # if batch_idx % 2 == 0:\n",
        "        # self.logger.experiment.log({\"DICOM gif\": wandb.Image(img_path), \"epoch\": self.current_epoch})\n",
        "\n",
        "        labels = [\n",
        "            post_label(torch.unsqueeze(i, 0)).squeeze() for i in decollate_batch(labels)\n",
        "        ]\n",
        "        dice_metric = self.dice_metric(y_pred=outputs, y=labels)\n",
        "        return {\"dice_metric\": dice_metric, \"val_number\": len(outputs), \"prediction\": predicition}\n",
        "    \n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        print('predicting...')\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3)])\n",
        "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 2\n",
        "        outputs = sliding_window_inference(\n",
        "            images, roi_size, sw_batch_size, self.forward\n",
        "        )\n",
        "        predicition ={\"output\": outputs, \"image\": images, \"label\": labels}\n",
        "        outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
        "\n",
        "        labels = [\n",
        "            post_label(torch.unsqueeze(i, 0)).squeeze() for i in decollate_batch(labels)\n",
        "        ]\n",
        "        dice_metric = self.dice_metric(y_pred=outputs, y=labels)\n",
        "        return {\"prediction\": predicition, \"dice_metric\": dice_metric}\n",
        "\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        dice_liver, dice_injure = self.dice_metric.aggregate()\n",
        "\n",
        "        self.dice_metric.reset()\n",
        "        tensorboard_logs = {\n",
        "            \"dice_metric\": dice_injure,\n",
        "        }\n",
        "\n",
        "        predictions = [x[\"prediction\"] for x in outputs]\n",
        "\n",
        "        test_dt = wandb.Table(columns = ['filename', 'combined output', 'ground_truth', 'class predicted'])\n",
        "        figure = computeROC(predictions)\n",
        "        self.logger.experiment.log({\"ROC\": figure, \"epoch\": self.current_epoch})\n",
        "        \n",
        "        for i, prediction in enumerate(predictions):\n",
        "            filename = os.path.basename(prediction[\"filename\"][0])\n",
        "            blended = make_gif(prediction, filename=i)\n",
        "            class_predicted, _, ground_truth = get_classification_info(prediction)\n",
        "            row = [filename, wandb.Image(blended), int(ground_truth[0]), class_predicted]\n",
        "            test_dt.add_data(*row)\n",
        "        \n",
        "        self.logger.experiment.log({\"SUMMARY_EPOCH\" : test_dt})\n",
        "\n",
        "        if dice_injure > self.best_val_dice:\n",
        "            self.best_val_dice = dice_injure\n",
        "            self.best_val_epoch = self.current_epoch\n",
        "        print(\n",
        "            f\"current epoch: {self.current_epoch} \"\n",
        "            f\"current liver dice: {dice_liver:.4f}\"\n",
        "            f\"current injure  dice: {dice_injure:.4f}\"\n",
        "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
        "            f\"at epoch: {self.best_val_epoch}\"\n",
        "        )\n",
        "        self.log(\"dice_metric_liver\", dice_liver)\n",
        "        self.log(\"dice_metric_injure\", dice_injure)\n",
        "        return {\"log\": tensorboard_logs, \"predictions\": predictions}\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "mmar = {\n",
        "    RemoteMMARKeys.ID: \"clara_pt_liver_and_tumor_ct_segmentation_1\",\n",
        "    RemoteMMARKeys.NAME: \"clara_pt_liver_and_tumor_ct_segmentation\",\n",
        "    RemoteMMARKeys.FILE_TYPE: \"zip\",\n",
        "    RemoteMMARKeys.HASH_TYPE: \"md5\",\n",
        "    RemoteMMARKeys.HASH_VAL: None,\n",
        "    RemoteMMARKeys.MODEL_FILE: os.path.join(\"models\", \"model.pt\"),\n",
        "    RemoteMMARKeys.CONFIG_FILE: os.path.join(\"config\", \"config_train.json\"),\n",
        "    RemoteMMARKeys.VERSION: 1,\n",
        "}\n",
        "\n",
        "def save_checkpoint(state, name):\n",
        "    file_path = \"checkpoints/\"\n",
        "    if not os.path.exists(file_path): \n",
        "        os.makedirs(file_path)\n",
        "    epoch = state[\"epoch\"]\n",
        "    save_dir = file_path + name + str(epoch)\n",
        "    torch.save(state, save_dir)\n",
        "    print(f\"Saving checkpoint for epoch {epoch} in: {save_dir}\")\n",
        "\n",
        "def save_state_dict(state, name):\n",
        "    file_path = \"checkpoints/\"\n",
        "    if not os.path.exists(file_path):\n",
        "        os.makedirs(file_path)\n",
        "\n",
        "    save_dir = file_path + f\"{name}_best\"\n",
        "    torch.save(state, save_dir)\n",
        "    print(f\"Best accuracy so far. Saving model to:{save_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Log_and_print:\n",
        "    def __init__(self, run_name, tb_logger=None):\n",
        "        self.tb_logger = tb_logger\n",
        "        self.run_name = run_name\n",
        "        self.str_log = \"run_name\" + \"\\n  \\n\"\n",
        "\n",
        "    def lnp(self, tag):\n",
        "        print(self.run_name, time.asctime(), tag)\n",
        "        self.str_log += str(time.asctime()) + \" \" + str(tag) + \"  \\n\"\n",
        "\n",
        "    def dump_to_tensorboard(self):\n",
        "        if not self.tb_logger:\n",
        "            print(\"No tensorboard logger\")\n",
        "        self.tb_logger.experiment.add_text(\"log\", self.str_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Gif Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_gif(prediction, filename):\n",
        "    def _save_gif(volume, filename):\n",
        "        volume = volume.astype(np.float64) / np.max(volume) # normalize the data to 0 - 1\n",
        "        volume = 255 * volume # Now scale by 255\n",
        "        volume = volume.astype(np.uint8)\n",
        "        path_to_gif = os.path.join(\"gifs\", f'{filename}.gif')\n",
        "        if not os.path.exists(\"gifs\"):\n",
        "            print(\"Creating gifs directory\")\n",
        "            os.mkdir(\"gifs\")\n",
        "        imageio.mimsave(path_to_gif, volume)\n",
        "        return path_to_gif\n",
        "\n",
        "    selected = prediction\n",
        "    # print('true label:', selected['label'].shape)\n",
        "    pred = torch.argmax(selected['output'], dim=1).detach().cpu().numpy()\n",
        "    true_label = selected['label'][0].detach().cpu().numpy()\n",
        "    image = selected['image'][0].cpu().numpy()\n",
        "    # print('true label:', true_label.shape)\n",
        "    \n",
        "    blended_true_label = blend_images(image, true_label)\n",
        "    blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
        "\n",
        "    blended_prediction = blend_images(image, pred)\n",
        "    blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
        "\n",
        "    volume_pred = blended_final_prediction[:,:,:,:]\n",
        "    volume_label = blended_final_true_label[:,:,:,:]\n",
        "    volume_pred = np.squeeze(volume_pred).permute(3,0,1,2).cpu()\n",
        "    volume_label = np.squeeze(volume_label).permute(3,0,1,2).cpu()\n",
        "    volume = torch.hstack((volume_pred, volume_label))\n",
        "\n",
        "    volume_path = _save_gif(volume.numpy(), f\"blended-{filename}\")\n",
        "       \n",
        "    \n",
        "    return volume_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_classification_info(prediction):\n",
        "    post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
        "    ground_truth = [\n",
        "            1 if (post_label(i)[2,:,:,:].cpu() == 1).any() else 0 for i in decollate_batch(prediction['label'])\n",
        "        ]\n",
        "    prediction = torch.mean(torch.nn.Sigmoid()(prediction['output'].cpu().squeeze()[2,:,:,:]))\n",
        "\n",
        "    predicted_class = 1 if prediction > 0.5 else 0\n",
        "    \n",
        "    return predicted_class, prediction, ground_truth\n",
        "\n",
        "def computeROC(predictions):\n",
        "    from sklearn.metrics import roc_curve, auc # roc curve tools\n",
        "    \n",
        "    g_truths = []\n",
        "    preds = []\n",
        "    for prediction in predictions:\n",
        "        _, predict, ground_truth = get_classification_info(prediction)\n",
        "        g_truths.extend(ground_truth)\n",
        "        preds.append(predict)\n",
        "\n",
        "    preds = np.asarray(preds)\n",
        "    ground_truth = np.asarray(g_truths)\n",
        "    fpr, tpr, _ = roc_curve(g_truths, preds)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    ax.plot([0, 1], [0, 1], 'k--')\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title('ROC Curve')\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhnD1E-uGr8c"
      },
      "source": [
        "## Run the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SEED = 0\n",
        "IMG_SIZE = (160,160,160)\n",
        "VAL_SIZE = (256,256,256)\n",
        "SAVE_PATH = \"lightning_logs/\"\n",
        "run_idx = len(os.listdir(\"wandb\"))\n",
        "RUN_NAME = f\"Predict_Segmentation_{run_idx+1}\"\n",
        "pytorch_lightning.seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    wandb.finish()\n",
        "except:\n",
        "    print(\"Wandb not initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predict_Segmentation_116 Thu May 26 15:53:59 2022 Loggers start\n",
            "Predict_Segmentation_116 Thu May 26 15:53:59 2022 ts_script: 1653573239.1390793\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.17 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/trauma/Documents/AITrauma-Segmentation/wandb/run-20220526_155359-3qi70zc6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/lalvarez/traumaIA/runs/3qi70zc6\" target=\"_blank\">Predict_Segmentation_116</a></strong> to <a href=\"https://wandb.ai/lalvarez/traumaIA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "lnp = Log_and_print(RUN_NAME)\n",
        "lnp.lnp(\"Loggers start\")\n",
        "lnp.lnp(\"ts_script: \" + str(time.time()))\n",
        "\n",
        "# Start Wandb\n",
        "wandb_logger = pytorch_lightning.loggers.WandbLogger(\n",
        "    project=\"traumaIA\",\n",
        "    # save_dir=SAVE_DIR + \"log/\",\n",
        "    name=RUN_NAME,\n",
        "    # version=\"fixed_version\",\n",
        "    # sync_tensorboard=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CALLBACKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predict_Segmentation_116 Thu May 26 16:00:40 2022 MAIN callbacks\n",
            "Predict_Segmentation_116 Thu May 26 16:00:40 2022 checkpoint_dirpath: lightning_logs/checkpoints/\n",
            "Predict_Segmentation_116 Thu May 26 16:00:40 2022 checkpoint_filename: lightning_logs_Predict_Segmentation_116\n"
          ]
        }
      ],
      "source": [
        "lnp.lnp(\"MAIN callbacks\")\n",
        "l_callbacks = []\n",
        "cbEarlyStopping = pytorch_lightning.callbacks.early_stopping.EarlyStopping(\n",
        "    monitor=\"dice_metric_injure\", patience=30, mode=\"max\"\n",
        ")\n",
        "l_callbacks.append(cbEarlyStopping)\n",
        "\n",
        "\n",
        "checkpoint_dirpath = SAVE_PATH + \"checkpoints/\"\n",
        "checkpoint_filename = SAVE_PATH[:-1] + \"_\" + RUN_NAME\n",
        "lnp.lnp(\"checkpoint_dirpath: \" + checkpoint_dirpath)\n",
        "lnp.lnp(\"checkpoint_filename: \" + checkpoint_filename)\n",
        "cbModelCheckpoint = pytorch_lightning.callbacks.ModelCheckpoint(\n",
        "    monitor=\"dice_metric_injure\", mode=\"max\", dirpath=checkpoint_dirpath, filename=checkpoint_filename, \n",
        ")\n",
        "l_callbacks.append(cbModelCheckpoint)\n",
        "\n",
        "l_callbacks.append(PrintTableMetricsCallback())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predict_Segmentation_116 Thu May 26 16:00:41 2022  Start Trainining process...\n",
            "using a pretrained model.\n",
            "2022-05-26 16:00:42,957 - INFO - Expected md5 is None, skip md5 check for file spleen_data/clara_pt_liver_and_tumor_ct_segmentation_4.1.zip.\n",
            "2022-05-26 16:00:42,959 - INFO - File exists: spleen_data/clara_pt_liver_and_tumor_ct_segmentation_4.1.zip, skipped downloading.\n",
            "2022-05-26 16:00:42,963 - INFO - Non-empty folder exists in spleen_data/clara_pt_liver_and_tumor_ct_segmentation, skipped extracting.\n",
            "2022-05-26 16:00:42,966 - INFO - \n",
            "*** \"clara_pt_liver_and_tumor_ct_segmentation\" available at spleen_data/clara_pt_liver_and_tumor_ct_segmentation.\n",
            "2022-05-26 16:00:43,105 - INFO - *** Model: <class 'monai.networks.nets.unet.UNet'>\n",
            "2022-05-26 16:00:43,141 - INFO - *** Model params: {'dimensions': 3, 'in_channels': 1, 'out_channels': 3, 'channels': [16, 32, 64, 128, 256], 'strides': [2, 2, 2, 2], 'num_res_units': 2, 'norm': 'batch'}\n",
            "2022-05-26 16:00:43,170 - INFO - \n",
            "---\n",
            "2022-05-26 16:00:43,172 - INFO - For more information, please visit https://ngc.nvidia.com/catalog/models/nvidia:med:clara_pt_liver_and_tumor_ct_segmentation\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning:\n",
            "\n",
            "The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'dst' model updated: 148 of 148 variables.\n",
            "num. var. using the pretrained 148 , random init 0 variables.\n",
            "validation files [{'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_066_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_066_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_036_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_036_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_005_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_005_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_057_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_057_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_053_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_053_0000.nii.gz'}]\n",
            "len(train_files) 64\n",
            "len(validation files) 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 64/64 [00:00<00:00, 41877.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accessing lmdb file: /mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/monai_cache.lmdb.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 5/5 [00:00<00:00, 16008.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accessing lmdb file: /mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/monai_cache.lmdb.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning:\n",
            "\n",
            "Checkpoint directory /home/trauma/Documents/AITrauma-Segmentation/lightning_logs/checkpoints exists and is not empty.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type       | Params\n",
            "---------------------------------------------\n",
            "0 | _model        | UNet       | 4.8 M \n",
            "1 | loss_function | DiceCELoss | 0     \n",
            "---------------------------------------------\n",
            "4.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.8 M     Total params\n",
            "19.240    Total estimated model params size (MB)\n",
            "/home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning:\n",
            "\n",
            "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 28 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "\n",
            "/home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning:\n",
            "\n",
            "The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 28 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8db0dabaf4534073b2efab2b76b3faab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73c0ced94ffc4d46be845f43fc9ec387",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n",
            "\n",
            "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
            "\n",
            "dice_metric_liverdice_metric_injure\n",
            "\n",
            "0.6057704687118530.2562547028064728\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 0 current liver dice: 0.6058current injure  dice: 0.2563\n",
            "best mean dice: 0.2563 at epoch: 0\n",
            "Predict_Segmentation_116 Thu May 26 16:01:22 2022 Dice Loss: 0.6461766362190247\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverdice_metric_injure\n",
            "\n",
            "0.6057704687118530.2562547028064728\n",
            "0.6057704687118530.2562547028064728\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06c2f48026424077a4b8e7e70165fd61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='47.854 MB of 47.854 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dice loss</td><td></td></tr><tr><td>dice_metric_injure</td><td></td></tr><tr><td>dice_metric_liver</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dice loss</td><td>0.64618</td></tr><tr><td>dice_metric_injure</td><td>0.25625</td></tr><tr><td>dice_metric_liver</td><td>0.60577</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>trainer/global_step</td><td>31</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">Predict_Segmentation_116</strong>: <a href=\"https://wandb.ai/lalvarez/traumaIA/runs/3qi70zc6\" target=\"_blank\">https://wandb.ai/lalvarez/traumaIA/runs/3qi70zc6</a><br/>Synced 6 W&B file(s), 6 media file(s), 6 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220526_155359-3qi70zc6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# initialise the LightningModule\n",
        "lnp.lnp(\" Start Trainining process...\")\n",
        "net = Net(train_img_size=IMG_SIZE, val_img_size=VAL_SIZE)#.load_from_checkpoint(\"lightning_logs\\checkpoints\\lightning_logs_Segmentation_14.ckpt\", train_img_size=IMG_SIZE, val_img_size=VAL_SIZE)\n",
        "wandb_logger.watch(net)\n",
        "\n",
        "# set up loggers and checkpoints\n",
        "log_dir = os.path.join(root_dir, \"logs\")\n",
        "\n",
        "# initialise Lightning's trainer.\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    gpus=[0],\n",
        "    max_epochs=1,\n",
        "    logger=wandb_logger,\n",
        "    enable_checkpointing=True,\n",
        "    num_sanity_val_steps=0,\n",
        "    log_every_n_steps=1,\n",
        "    callbacks=l_callbacks,\n",
        ")\n",
        "\n",
        "# train\n",
        "result_pred2 = trainer.fit(net)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Run learning rate finder\n",
        "lr_finder = trainer.tuner.lr_find(net)\n",
        "\n",
        "# Results can be found in\n",
        "lr_finder.results\n",
        "\n",
        "# Plot with\n",
        "fig = lr_finder.plot(suggest=True)\n",
        "fig.show()      \n",
        "\n",
        "# Pick point based on plot, or get suggestion\n",
        "new_lr = lr_finder.suggestion()\n",
        "\n",
        "# update hparams of the model\n",
        "net.hparams.lr = new_lr\n",
        "\n",
        "# Fit model\n",
        "trainer.fit(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['output', 'image', 'label'])\n",
            "torch.Size([1, 3, 256, 256, 256])\n",
            "torch.Size([1, 1, 256, 256, 256])\n",
            "torch.Size([1, 1, 256, 256, 256])\n",
            "tensor([[0.8482, 0.1969]])\n",
            "dict_keys(['output', 'image', 'label'])\n",
            "torch.Size([1, 3, 256, 256, 256])\n",
            "torch.Size([1, 1, 256, 256, 256])\n",
            "torch.Size([1, 1, 256, 256, 256])\n",
            "tensor([[0.9715,    nan]])\n",
            "dict_keys(['output', 'image', 'label'])\n",
            "torch.Size([1, 3, 256, 256, 256])\n",
            "torch.Size([1, 1, 256, 256, 256])\n",
            "torch.Size([1, 1, 256, 256, 256])\n",
            "tensor([[0.8766,    nan]])\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(result_pred2)):\n",
        "    print(result_pred2[i]['prediction'].keys())\n",
        "    print(result_pred2[i]['prediction']['output'].shape)\n",
        "    print(result_pred2[i]['prediction']['image'].shape)\n",
        "    print(result_pred2[i]['prediction']['label'].shape)\n",
        "    print(result_pred2[i]['dice_metric'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9205692ac387463594db2381ad5f88c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">Predict_Segmentation_23</strong>: <a href=\"https://wandb.ai/lalvarez/traumaIA/runs/2jptek2f\" target=\"_blank\">https://wandb.ai/lalvarez/traumaIA/runs/2jptek2f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220525_180636-2jptek2f/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG9mdV0IGr8d",
        "outputId": "e6ed0616-1270-44ae-b685-4e75b4088066",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train completed, best_metric: 0.0686 at epoch 2\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f\"train completed, best_metric: {net.best_val_dice:.4f} \"\n",
        "    f\"at epoch {net.best_val_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 256, 256, 256])"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "selected['label'][:,:,:,:,:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "true label: torch.Size([1, 1, 256, 256, 256])\n",
            "true label: (1, 256, 256, 256)\n",
            "true label: torch.Size([1, 1, 256, 256, 256])\n",
            "true label: (1, 256, 256, 256)\n",
            "true label: torch.Size([1, 1, 256, 256, 256])\n",
            "true label: (1, 256, 256, 256)\n"
          ]
        }
      ],
      "source": [
        "predictions = [x[\"prediction\"] for x in result_pred2]\n",
        "volumes = []\n",
        "for prediction in predictions:\n",
        "    selected = prediction\n",
        "    print('true label:', selected['label'].shape)\n",
        "    pred = torch.argmax(selected['output'], dim=1).detach().cpu().numpy()\n",
        "    true_label = selected['label'][0].detach().cpu().numpy()\n",
        "    image = selected['image'][0].cpu().numpy()\n",
        "    print('true label:', true_label.shape)\n",
        "    \n",
        "    blended_true_label = blend_images(image, true_label)\n",
        "    blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
        "\n",
        "    blended_prediction = blend_images(image, pred)\n",
        "    blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
        "\n",
        "    volume_pred = blended_final_prediction[:,:,:,:]\n",
        "    volume_label = blended_final_true_label[:,:,:,:]\n",
        "    volume_pred = np.squeeze(volume_pred).permute(3,0,1,2)\n",
        "    volume_label = np.squeeze(volume_label).permute(3,0,1,2)\n",
        "    volume = torch.hstack((volume_pred, volume_label)).numpy()\n",
        "    volumes.append(volume)\n",
        "volume = np.hstack((volumes))\n",
        "data = volume.astype(np.float64) / np.max(volume) # normalize the data to 0 - 1\n",
        "data = 255 * data # Now scale by 255\n",
        "volume = data.astype(np.uint8)\n",
        "# path_to_gif = f'gifs\\\\prediction.gif'\n",
        "# if not os.path.exists(\"gifs\\\\\"):\n",
        "#     os.mkdir(\"gifs\\\\\")\n",
        "# imageio.mimsave(path_to_gif, volume)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ce2b4bc42584f1db595356ab8e25fa5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=127, description='slice', max=255), Output()), _dom_classes=('widget-int"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<function __main__.dicom_animation(slice)>"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img_plot = volumes[2]\n",
        "# img_plot = np.transpose(img_plot, (0, 1,3, 2))\n",
        "def dicom_animation(slice):\n",
        "    # extent = np.min(x), np.max(x), np.min(y), np.max(y)\n",
        "    plt.figure(figsize=(18, 6))\n",
        "    plt.title(f\"liver no injured \")\n",
        "    plt.imshow(img_plot[slice, :, :,:], cmap=\"bone\")\n",
        "    plt.show()\n",
        "\n",
        "interact(dicom_animation, slice=(0, img_plot.shape[0]-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoGZ_9q3Gr8e"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoUeVHa5Gr8e"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "spleen_segmentation_3d_lightning.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f756ff16342a8157e6f46b879d688029dcc5bc6cd621b2b84934bbdd850a743a"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('env_trauma')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
