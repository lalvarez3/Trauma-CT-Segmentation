{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djzZ-bffGr8X"
      },
      "source": [
        "#### Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "biFwQV_yGr8X",
        "outputId": "caa6e216-4409-4de4-bc5a-ceec09bf6af7",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONAI version: 0.8.1\n",
            "Numpy version: 1.22.3\n",
            "Pytorch version: 1.11.0\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
            "MONAI rev id: 71ff399a3ea07aef667b23653620a290364095b1\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 3.2.2\n",
            "scikit-image version: 0.19.2\n",
            "Pillow version: 9.0.1\n",
            "Tensorboard version: 2.9.0\n",
            "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "TorchVision version: 0.12.0\n",
            "tqdm version: 4.64.0\n",
            "lmdb version: 1.3.0\n",
            "psutil version: 5.9.0\n",
            "pandas version: 1.4.2\n",
            "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from monai.data.image_reader import ImageReader, ITKReader\n",
        "from ipywidgets.widgets import *\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning\n",
        "from monai.utils import set_determinism\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    AddChanneld,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    EnsureType,\n",
        "    EnsureChannelFirstd,\n",
        "    RandFlipd,\n",
        "    RandRotated,\n",
        "    ToTensord,\n",
        "    Resized\n",
        ")\n",
        "import wandb\n",
        "from monai.networks.nets import UNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceFocalLoss, GeneralizedDiceLoss\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, list_data_collate, decollate_batch, Dataset, LMDBDataset\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from monai.data import DataLoader\n",
        "import os\n",
        "import glob\n",
        "from monai.transforms.spatial.array import Resize\n",
        "\n",
        "from copy import deepcopy\n",
        "from enum import Enum\n",
        "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
        "\n",
        "from monai.config import DtypeLike, KeysCollection\n",
        "from monai.config.type_definitions import NdarrayOrTensor\n",
        "from monai.networks.layers import AffineTransform\n",
        "from monai.networks.layers.simplelayers import GaussianFilter\n",
        "from monai.transforms.croppad.array import CenterSpatialCrop, SpatialPad\n",
        "from monai.transforms.inverse import InvertibleTransform\n",
        "from monai.transforms.spatial.array import (\n",
        "    Resize,\n",
        ")\n",
        "from monai.transforms.transform import MapTransform, RandomizableTransform\n",
        "from monai.transforms.utils import create_grid\n",
        "from monai.utils import (\n",
        "    InterpolateMode,\n",
        "    ensure_tuple_rep,\n",
        ")\n",
        "from monai.utils.deprecate_utils import deprecated_arg\n",
        "from monai.utils.enums import TraceKeys\n",
        "from monai.utils.module import optional_import\n",
        "from monai.utils.type_conversion import convert_data_type, convert_to_dst_type\n",
        "from monai.apps import load_from_mmar\n",
        "from monai.apps.mmars import RemoteMMARKeys\n",
        "from monai.networks.utils import copy_model_state\n",
        "from monai.optimizers import generate_param_groups\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from pl_bolts.callbacks import PrintTableMetricsCallback\n",
        "from monai.visualize import matshow3d, blend_images\n",
        "import imageio\n",
        "print_config()\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InterpolateMode(Enum):\n",
        "    NEAREST = \"nearest\"\n",
        "    LINEAR = \"linear\"\n",
        "    BILINEAR = \"bilinear\"\n",
        "    BICUBIC = \"bicubic\"\n",
        "    TRILINEAR = \"trilinear\"\n",
        "    AREA = \"area\"\n",
        "\n",
        "\n",
        "InterpolateModeSequence = Union[\n",
        "    Sequence[Union[InterpolateMode, str]], InterpolateMode, str\n",
        "]\n",
        "\n",
        "class ResizedC(MapTransform, InvertibleTransform):\n",
        "\n",
        "    backend = Resize.backend\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        keys: KeysCollection,\n",
        "        spatial_size: Union[Sequence[int], int],\n",
        "        size_mode: str = \"all\",\n",
        "        mode: InterpolateModeSequence = InterpolateMode.AREA,\n",
        "        align_corners: Union[Sequence[Optional[bool]], Optional[bool]] = None,\n",
        "        allow_missing_keys: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(keys, allow_missing_keys)\n",
        "        self.mode = ensure_tuple_rep(mode, len(self.keys))\n",
        "        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))\n",
        "        self.resizer = Resize(spatial_size=spatial_size, size_mode=size_mode)\n",
        "        self.spatial_size = spatial_size\n",
        "\n",
        "    def __call__(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = dict(data)\n",
        "        for key, mode, align_corners in self.key_iterator(\n",
        "            d, self.mode, self.align_corners\n",
        "        ):\n",
        "            self.push_transform(\n",
        "                d,\n",
        "                key,\n",
        "                extra_info={\n",
        "                    \"mode\": mode.value if isinstance(mode, Enum) else mode,\n",
        "                    \"align_corners\": align_corners\n",
        "                    if align_corners is not None\n",
        "                    else TraceKeys.NONE,\n",
        "                },\n",
        "            )\n",
        "            init_slice = int(d[key].shape[-1]*0.15)\n",
        "            end_slice = int(d[key].shape[-1]*0.1)\n",
        "            # Reduce Size in Memory\n",
        "            if key == \"label\":\n",
        "                d[key] = d[key].astype(np.int8)\n",
        "                if d[key].shape[-1] > 600: d[key] = d[key][:,:,:,init_slice:-end_slice] #\n",
        "\n",
        "                if d[\"image_meta_dict\"].get(\"PatientName\", None) and d[\"image_meta_dict\"][\"PatientName\"].startswith(\"NI\") and len(d[key].shape) != 4:\n",
        "                    # print(d[key].shape)\n",
        "                    liver_channel = np.where((d[key] != 6), 0, d[key])\n",
        "                    liver_channel = np.where((liver_channel == 6), 1, liver_channel)\n",
        "                    # liver_channel = np.expand_dims(liver_channel, 0)\n",
        "                    w, h, z = self.spatial_size\n",
        "                    liver_channel = self.resizer(liver_channel, align_corners=align_corners)\n",
        "                    background = np.ones((1, z, w, h), dtype=np.float16) - liver_channel\n",
        "                    empty_injures = np.zeros((1, z, w, h), dtype=np.float16)\n",
        "                    resized = [background, liver_channel, empty_injures]\n",
        "                    d[key] = np.stack(resized).astype(np.int8).squeeze()\n",
        "\n",
        "                else:\n",
        "                    label = d[key]\n",
        "                    w, h, z = self.spatial_size\n",
        "                    resized = list()\n",
        "                    background = np.ones((1, w, h, z), dtype=np.int8)\n",
        "                    for i, channel in enumerate([0, 2]):  # TODO: desharcodead\n",
        "                        resized.append(\n",
        "                            self.resizer(\n",
        "                                np.expand_dims(label[channel, :, :, :], 0),\n",
        "                                align_corners=align_corners,\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                    background -= resized[0] # + resized[1]\n",
        "                    resized = [background] + resized\n",
        "                    d[key] = np.stack(resized).astype(np.int8).squeeze()\n",
        "            else:\n",
        "                if d[key].shape[-1] > 600: d[key] = d[key][:,:,:,init_slice:-end_slice]\n",
        "                d[key] = self.resizer(d[key], align_corners=align_corners)\n",
        "\n",
        "        keys = ['spacing', 'original_affine', 'affine', 'spatial_shape', 'original_channel_dim', 'filename_or_obj']\n",
        "        new_label_metadata = dict()\n",
        "        for key in keys:\n",
        "            new_label_metadata[key] = d[\"label_meta_dict\"].get(key, 0)\n",
        "\n",
        "        d[\"label_meta_dict\"] = new_label_metadata\n",
        "\n",
        "        if \"PatientID\" not in d[\"image_meta_dict\"]:\n",
        "            d[\"image_meta_dict\"][\"PatientID\"] = \"0\"\n",
        "        if \"PatientName\" not in d[\"image_meta_dict\"]:\n",
        "            d[\"image_meta_dict\"][\"PatientName\"] = \"0\"\n",
        "        if \"SliceThickness\" not in d[\"image_meta_dict\"]:\n",
        "            d[\"image_meta_dict\"][\"SliceThickness\"] = \"0\"\n",
        "        return d\n",
        "\n",
        "    def inverse(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = deepcopy(dict(data))\n",
        "        for key in self.key_iterator(d):\n",
        "            transform = self.get_most_recent_transform(d, key)\n",
        "            orig_size = transform[TraceKeys.ORIG_SIZE]\n",
        "            mode = transform[TraceKeys.EXTRA_INFO][\"mode\"]\n",
        "            align_corners = transform[TraceKeys.EXTRA_INFO][\"align_corners\"]\n",
        "            # Create inverse transform\n",
        "            inverse_transform = Resize(\n",
        "                spatial_size=orig_size,\n",
        "                mode=mode,\n",
        "                align_corners=None\n",
        "                if align_corners == TraceKeys.NONE\n",
        "                else align_corners,\n",
        "            )\n",
        "            # Apply inverse transform\n",
        "            d[key] = inverse_transform(d[key])\n",
        "            # Remove the applied transform\n",
        "            self.pop_transform(d, key)\n",
        "\n",
        "        return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "class adaptOverlay(MapTransform, InvertibleTransform):\n",
        "\n",
        "    backend = Resize.backend\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        keys: KeysCollection,\n",
        "        size_mode: str = \"all\",\n",
        "        mode: InterpolateModeSequence = InterpolateMode.AREA,\n",
        "        align_corners: Union[Sequence[Optional[bool]], Optional[bool]] = None,\n",
        "        allow_missing_keys: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(keys, allow_missing_keys)\n",
        "        self.mode = ensure_tuple_rep(mode, len(self.keys))\n",
        "        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))\n",
        "\n",
        "    def __adapt_overlay__(self, overlay_path, mha_path, label):\n",
        "        import SimpleITK as sitk\n",
        "        if label.shape[-1] == 6:\n",
        "            return label\n",
        "        # Load the mha\n",
        "        mha_data = sitk.ReadImage(mha_path)\n",
        "        mha_org = mha_data.GetOrigin()[-1]\n",
        "        # Load the mha image\n",
        "        mha_img = sitk.GetArrayFromImage(mha_data)\n",
        "        original_z_size = mha_img.shape[0]\n",
        "\n",
        "        # Load the overlay\n",
        "        overlay_data = sitk.ReadImage(overlay_path)\n",
        "        overlay_org = overlay_data.GetOrigin()[-1]\n",
        "\n",
        "        overlay_init = np.abs(1/mha_data.GetSpacing()[-1]*(mha_org-overlay_org) )\n",
        "\n",
        "        lower_bound = int(overlay_init)\n",
        "        upper_bound = label.shape[-1]\n",
        "        zeros_up = lower_bound\n",
        "        zeros_down = original_z_size - (upper_bound + lower_bound)\n",
        "        new = list()\n",
        "\n",
        "        if zeros_up > 0:\n",
        "            new.append(np.zeros((label.shape[0], label.shape[1], zeros_up), dtype=label.dtype))\n",
        "\n",
        "        new.append(label)\n",
        "\n",
        "        if zeros_down > 0:\n",
        "            new.append(np.zeros((label.shape[0], label.shape[1], zeros_down), dtype=label.dtype))\n",
        "\n",
        "        label = np.concatenate(new, axis=2)\n",
        "\n",
        "        return label\n",
        "\n",
        "\n",
        "    def __call__(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = dict(data)\n",
        "        for key, mode, align_corners in self.key_iterator(\n",
        "            d, self.mode, self.align_corners\n",
        "        ):\n",
        "            self.push_transform(\n",
        "                d,\n",
        "                key,\n",
        "                extra_info={\n",
        "                    \"mode\": mode.value if isinstance(mode, Enum) else mode,\n",
        "                    \"align_corners\": align_corners\n",
        "                    if align_corners is not None\n",
        "                    else TraceKeys.NONE,\n",
        "                },\n",
        "            )\n",
        "            # Reduce Size in Memory\n",
        "            if key == \"label\":\n",
        "                d[key] = d[key].astype(np.int8)\n",
        "                if d[\"image_meta_dict\"].get(\"PatientName\", None) and d[\"image_meta_dict\"][\"PatientName\"].startswith(\"NI\"):\n",
        "                    file_path = d[\"label_meta_dict\"][\"filename_or_obj\"]\n",
        "                    data_path = d[\"image_meta_dict\"][\"filename_or_obj\"]\n",
        "                    d[key] = self.__adapt_overlay__(file_path, data_path, d[key])\n",
        "        return d\n",
        "\n",
        "    def inverse(\n",
        "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
        "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = deepcopy(dict(data))\n",
        "        for key in self.key_iterator(d):\n",
        "            transform = self.get_most_recent_transform(d, key)\n",
        "            orig_size = transform[TraceKeys.ORIG_SIZE]\n",
        "            mode = transform[TraceKeys.EXTRA_INFO][\"mode\"]\n",
        "            align_corners = transform[TraceKeys.EXTRA_INFO][\"align_corners\"]\n",
        "            # Create inverse transform\n",
        "            inverse_transform = Resize(\n",
        "                spatial_size=orig_size,\n",
        "                mode=mode,\n",
        "                align_corners=None\n",
        "                if align_corners == TraceKeys.NONE\n",
        "                else align_corners,\n",
        "            )\n",
        "            # Apply inverse transform\n",
        "            d[key] = inverse_transform(d[key])\n",
        "            # Remove the applied transform\n",
        "            self.pop_transform(d, key)\n",
        "\n",
        "        return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RemoveDicts(MapTransform, InvertibleTransform):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        keys: KeysCollection,\n",
        "        allow_missing_keys: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(keys, allow_missing_keys)\n",
        "\n",
        "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
        "        d = dict(data)\n",
        "        for key in self.key_iterator(d):\n",
        "            self.push_transform(d, key)\n",
        "        a = {\"image\": d[\"image\"], \"label\": d[\"label\"], \"path\": d[\"image_meta_dict\"][\"filename_or_obj\"]}\n",
        "        d = a\n",
        "        return d\n",
        "\n",
        "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
        "        d = deepcopy(dict(data))\n",
        "        for key in self.key_iterator(d):\n",
        "            d[key] = d[key]\n",
        "            # Remove the applied transform\n",
        "            self.pop_transform(d, key)\n",
        "        return d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "PRETRAINED = True\n",
        "TRANSFER_LEARNING = True\n",
        "N_WORKERS_LOADER = 4\n",
        "N_WORKERS_CACHE = 4\n",
        "CACHE_RATE = 0\n",
        "SEED=42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WzCJRUGGr8a"
      },
      "source": [
        "#### Define the LightningModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spleen_data\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = \"spleen_data\"\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(pytorch_lightning.LightningModule):\n",
        "    def __init__(self, train_img_size, val_img_size):\n",
        "        super().__init__()\n",
        "        self.train_img_size = train_img_size\n",
        "        self.val_img_size = val_img_size\n",
        "        if PRETRAINED:\n",
        "            print(\"using a pretrained model.\")\n",
        "            unet_model = load_from_mmar(\n",
        "                item=mmar[RemoteMMARKeys.NAME],\n",
        "                mmar_dir=root_dir,\n",
        "                # map_location=device,\n",
        "                pretrained=True,\n",
        "            )\n",
        "            self._model = unet_model\n",
        "            # copy all the pretrained weights except for variables whose name matches \"model.0.conv.unit0\"\n",
        "            if TRANSFER_LEARNING:\n",
        "                pretrained_dict, updated_keys, unchanged_keys = copy_model_state(\n",
        "                    self._model, unet_model,#  exclude_vars=\"model.[0-2].conv.unit[0-3]\"\n",
        "                )\n",
        "                print(\n",
        "                    \"num. var. using the pretrained\",\n",
        "                    len(updated_keys),\n",
        "                    \", random init\",\n",
        "                    len(unchanged_keys),\n",
        "                    \"variables.\",\n",
        "                )\n",
        "                self._model.load_state_dict(pretrained_dict)\n",
        "                # stop gradients for the pretrained weights\n",
        "                for x in self._model.named_parameters():\n",
        "                    if x[0] in updated_keys:\n",
        "                        x[1].requires_grad = True\n",
        "                params = generate_param_groups(\n",
        "                    network=self._model,\n",
        "                    layer_matches=[lambda x: x[0] in updated_keys],\n",
        "                    match_types=[\"filter\"],\n",
        "                    lr_values=[1e-4],\n",
        "                    include_others=False,\n",
        "                )\n",
        "                self.params = params\n",
        "\n",
        "        else:\n",
        "            self._model = UNet(\n",
        "                spatial_dims=3,\n",
        "                in_channels=1,\n",
        "                out_channels=3,\n",
        "                channels=(16, 32, 64, 128, 256),\n",
        "                strides=(2, 2, 2, 2),\n",
        "                num_res_units=2,\n",
        "                norm=Norm.BATCH,\n",
        "            )\n",
        "        self.loss_function = DiceFocalLoss(to_onehot_y=True, focal_weight=[0.25, 0.25, 0.75], softmax=True)\n",
        "        # self.loss_function = DiceCELoss(softmax=True, to_onehot_y=True)\n",
        "        self.post_pred = Compose(\n",
        "            [EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)]\n",
        "        )\n",
        "        self.post_label = Compose(\n",
        "            [EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)]\n",
        "        )\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
        "        self.best_val_dice = 0\n",
        "        self.best_val_epoch = 0\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # set up the correct data path\n",
        "        train_images = sorted(\n",
        "            glob.glob(\n",
        "                os.path.join(\n",
        "                    \"/mnt/chansey/\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\", \"imagesTr\", \"*.nii.gz\"\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        train_labels = [img.replace('imagesTr', 'labelsTr') for img in train_images]\n",
        "        \n",
        "        data_dicts = [\n",
        "            {\"image\": image_name, \"label\": label_name}\n",
        "            for image_name, label_name in zip(train_images, train_labels)\n",
        "        ]\n",
        "\n",
        "        random.shuffle(data_dicts)\n",
        "        train_files, val_files = data_dicts[:-5], data_dicts[-5:]\n",
        "        print(\"validation files\", val_files)\n",
        "        # print(\"training files\", train_files)\n",
        "        print(\"len(train_files)\", len(train_files))\n",
        "        print(\"len(validation files)\", len(val_files))\n",
        "\n",
        "        # set deterministic training for reproducibility\n",
        "        set_determinism(seed=SEED)\n",
        "\n",
        "        # define the data transforms\n",
        "\n",
        "        train_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                AddChanneld(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                Resized(keys=[\"image\", \"label\"], spatial_size=self.train_img_size),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                RemoveDicts(keys=[\"image\", \"label\"]),\n",
        "                ToTensord(keys=[\"image\", \"label\"]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # define the data transforms\n",
        "        val_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"], reader=ITKReader),\n",
        "                AddChanneld(keys=[\"image\", \"label\"]),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                Resized(keys=[\"image\", \"label\"], spatial_size=self.val_img_size),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                RemoveDicts(keys=[\"image\", \"label\"]),\n",
        "                ToTensord(keys=[\"image\", \"label\"]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # self.train_ds = CacheDataset(\n",
        "        #     data=train_files,\n",
        "        #     transform=train_transforms,\n",
        "        #     cache_rate=CACHE_RATE,\n",
        "        #     num_workers=N_WORKERS_CACHE,\n",
        "        # )\n",
        "\n",
        "        self.train_ds = LMDBDataset(\n",
        "            data=train_files,\n",
        "            transform=train_transforms,\n",
        "            cache_dir=os.path.join(\n",
        "                    \"/mnt/chansey/\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\"\n",
        "                )\n",
        "        )\n",
        "\n",
        "        self.val_ds = LMDBDataset(\n",
        "            data=val_files,\n",
        "            transform=val_transforms,\n",
        "            cache_dir=os.path.join(\n",
        "                    \"/mnt/chansey/\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\"\n",
        "                )\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-3)\n",
        "        if PRETRAINED:\n",
        "            optimizer = torch.optim.Adam(self.params, lr=1e-3, weight_decay=1e-4)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True)\n",
        "        return {'optimizer':optimizer, 'scheduler':scheduler, 'monitor':\"loss\", \"interval\": \"epoch\"}\n",
        "    \n",
        "    def lr_scheduler_step(self, scheduler, optimizer_idx, metric):\n",
        "        scheduler.step(epoch=self.current_epoch)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            self.train_ds, batch_size=2, shuffle=True, num_workers=N_WORKERS_LOADER\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            self.val_ds, batch_size=1, shuffle=False, num_workers=N_WORKERS_LOADER\n",
        "        )\n",
        "        return val_loader\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        predict_dataloader = torch.utils.data.DataLoader(\n",
        "            self.val_ds, batch_size=1, shuffle=False, num_workers=N_WORKERS_LOADER\n",
        "        )\n",
        "        return predict_dataloader\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        output = self.forward(images)\n",
        "        loss = self.loss_function(output, labels)\n",
        "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        #  the function is called after every epoch is completed\n",
        "\n",
        "        # calculating average loss\n",
        "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "\n",
        "        # logging using tensorboard logger\n",
        "        self.log(\"dice loss\", avg_loss)\n",
        "        lnp.lnp(f\"Dice Loss: {avg_loss}\")\n",
        "\n",
        "        self.logger.experiment.log({\"dice loss\": avg_loss})\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        filenames = batch[\"path\"]\n",
        "        post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3)])\n",
        "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 2\n",
        "        outputs = sliding_window_inference(\n",
        "            images, roi_size, sw_batch_size, self.forward\n",
        "        )\n",
        "        predicition ={\"output\": outputs, \"image\": images, \"label\": labels, \"filename\": filenames}\n",
        "        outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
        "\n",
        "        labels = [\n",
        "            post_label(torch.unsqueeze(i, 0)).squeeze() for i in decollate_batch(labels)\n",
        "        ]\n",
        "        dice_metric = self.dice_metric(y_pred=outputs, y=labels)\n",
        "        return {\"dice_metric\": dice_metric, \"val_number\": len(outputs), \"prediction\": predicition}\n",
        "    \n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        print('predicting...')\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3)])\n",
        "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 2\n",
        "        outputs = sliding_window_inference(\n",
        "            images, roi_size, sw_batch_size, self.forward\n",
        "        )\n",
        "        predicition ={\"output\": outputs, \"image\": images, \"label\": labels}\n",
        "        outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
        "\n",
        "        labels = [\n",
        "            post_label(torch.unsqueeze(i, 0)).squeeze() for i in decollate_batch(labels)\n",
        "        ]\n",
        "        dice_metric = self.dice_metric(y_pred=outputs, y=labels)\n",
        "        return {\"prediction\": predicition, \"dice_metric\": dice_metric}\n",
        "\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        dice_liver, dice_injure = self.dice_metric.aggregate()\n",
        "\n",
        "        self.dice_metric.reset()\n",
        "        tensorboard_logs = {\n",
        "            \"dice_metric\": dice_injure,\n",
        "        }\n",
        "\n",
        "        predictions = [x[\"prediction\"] for x in outputs]\n",
        "        \n",
        "        if self.current_epoch % 25 == 0 or dice_injure - self.best_val_dice > 0.1 or self.current_epoch == self.trainer.max_epochs - 1:\n",
        "            test_dt = wandb.Table(columns = ['epoch', 'filename', 'combined output', 'ground_truth', 'class predicted'])\n",
        "            figure = computeROC(predictions)\n",
        "            self.logger.experiment.log({\"ROC\": figure, \"epoch\": self.current_epoch})\n",
        "            \n",
        "            for i, prediction in enumerate(predictions):\n",
        "                filename = os.path.basename(prediction[\"filename\"][0])\n",
        "                blended = make_gif(prediction, filename=i)\n",
        "                class_predicted, _, ground_truth = get_classification_info(prediction)\n",
        "                row = [self.current_epoch, filename, wandb.Image(blended), int(ground_truth[0]), class_predicted]\n",
        "                test_dt.add_data(*row)\n",
        "            \n",
        "            self.logger.experiment.log({f\"SUMMARY_EPOCH_{self.current_epoch}\" : test_dt})\n",
        "\n",
        "        if dice_injure > self.best_val_dice:\n",
        "            self.best_val_dice = dice_injure\n",
        "            self.best_val_epoch = self.current_epoch\n",
        "        print(\n",
        "            f\"current epoch: {self.current_epoch} \"\n",
        "            f\"current liver dice: {dice_liver:.4f}\"\n",
        "            f\"current injure  dice: {dice_injure:.4f}\"\n",
        "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
        "            f\"at epoch: {self.best_val_epoch}\"\n",
        "        )\n",
        "        self.log(\"dice_metric_liver\", dice_liver)\n",
        "        self.log(\"dice_metric_injure\", dice_injure)\n",
        "        return {\"log\": tensorboard_logs}\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "mmar = {\n",
        "    RemoteMMARKeys.ID: \"clara_pt_liver_and_tumor_ct_segmentation_1\",\n",
        "    RemoteMMARKeys.NAME: \"clara_pt_liver_and_tumor_ct_segmentation\",\n",
        "    RemoteMMARKeys.FILE_TYPE: \"zip\",\n",
        "    RemoteMMARKeys.HASH_TYPE: \"md5\",\n",
        "    RemoteMMARKeys.HASH_VAL: None,\n",
        "    RemoteMMARKeys.MODEL_FILE: os.path.join(\"models\", \"model.pt\"),\n",
        "    RemoteMMARKeys.CONFIG_FILE: os.path.join(\"config\", \"config_train.json\"),\n",
        "    RemoteMMARKeys.VERSION: 1,\n",
        "}\n",
        "\n",
        "def save_checkpoint(state, name):\n",
        "    file_path = \"checkpoints/\"\n",
        "    if not os.path.exists(file_path): \n",
        "        os.makedirs(file_path)\n",
        "    epoch = state[\"epoch\"]\n",
        "    save_dir = file_path + name + str(epoch)\n",
        "    torch.save(state, save_dir)\n",
        "    print(f\"Saving checkpoint for epoch {epoch} in: {save_dir}\")\n",
        "\n",
        "def save_state_dict(state, name):\n",
        "    file_path = \"checkpoints/\"\n",
        "    if not os.path.exists(file_path):\n",
        "        os.makedirs(file_path)\n",
        "\n",
        "    save_dir = file_path + f\"{name}_best\"\n",
        "    torch.save(state, save_dir)\n",
        "    print(f\"Best accuracy so far. Saving model to:{save_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Log_and_print:\n",
        "    def __init__(self, run_name, tb_logger=None):\n",
        "        self.tb_logger = tb_logger\n",
        "        self.run_name = run_name\n",
        "        self.str_log = \"run_name\" + \"\\n  \\n\"\n",
        "\n",
        "    def lnp(self, tag):\n",
        "        print(self.run_name, time.asctime(), tag)\n",
        "        self.str_log += str(time.asctime()) + \" \" + str(tag) + \"  \\n\"\n",
        "\n",
        "    def dump_to_tensorboard(self):\n",
        "        if not self.tb_logger:\n",
        "            print(\"No tensorboard logger\")\n",
        "        self.tb_logger.experiment.add_text(\"log\", self.str_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Gif Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_gif(prediction, filename):\n",
        "    def _save_gif(volume, filename):\n",
        "        volume = volume.astype(np.float64) / np.max(volume) # normalize the data to 0 - 1\n",
        "        volume = 255 * volume # Now scale by 255\n",
        "        volume = volume.astype(np.uint8)\n",
        "        path_to_gif = os.path.join(\"gifs\", f'{filename}.gif')\n",
        "        if not os.path.exists(\"gifs\"):\n",
        "            print(\"Creating gifs directory\")\n",
        "            os.mkdir(\"gifs\")\n",
        "        imageio.mimsave(path_to_gif, volume)\n",
        "        return path_to_gif\n",
        "\n",
        "    selected = prediction\n",
        "    # print('true label:', selected['label'].shape)\n",
        "    pred = torch.argmax(selected['output'], dim=1).detach().cpu().numpy()\n",
        "    true_label = selected['label'][0].detach().cpu().numpy()\n",
        "    image = selected['image'][0].cpu().numpy()\n",
        "    # print('true label:', true_label.shape)\n",
        "    \n",
        "    blended_true_label = blend_images(image, true_label)\n",
        "    blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
        "\n",
        "    blended_prediction = blend_images(image, pred)\n",
        "    blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
        "\n",
        "    volume_pred = blended_final_prediction[:,:,:,:]\n",
        "    volume_label = blended_final_true_label[:,:,:,:]\n",
        "    volume_pred = np.squeeze(volume_pred).permute(3,0,1,2).cpu()\n",
        "    volume_label = np.squeeze(volume_label).permute(3,0,1,2).cpu()\n",
        "    volume = torch.hstack((volume_pred, volume_label))\n",
        "\n",
        "    volume_path = _save_gif(volume.numpy(), f\"blended-{filename}\")\n",
        "       \n",
        "    \n",
        "    return volume_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_classification_info(prediction):\n",
        "    post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
        "    ground_truth = [\n",
        "            1 if (post_label(i)[2,:,:,:].cpu() == 1).any() else 0 for i in decollate_batch(prediction['label'])\n",
        "        ]\n",
        "    prediction = torch.max(torch.nn.Sigmoid()(prediction['output'].cpu().squeeze()[2,:,:,:]))\n",
        "\n",
        "    predicted_class = 1 if prediction > 0.5 else 0\n",
        "    \n",
        "    return predicted_class, prediction, ground_truth\n",
        "\n",
        "def computeROC(predictions):\n",
        "    from sklearn.metrics import roc_curve, auc # roc curve tools\n",
        "    \n",
        "    g_truths = []\n",
        "    preds = []\n",
        "    for prediction in predictions:\n",
        "        _, predict, ground_truth = get_classification_info(prediction)\n",
        "        g_truths.extend(ground_truth)\n",
        "        preds.append(predict)\n",
        "\n",
        "    preds = np.asarray(preds)\n",
        "    ground_truth = np.asarray(g_truths)\n",
        "    fpr, tpr, _ = roc_curve(g_truths, preds)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    ax.plot([0, 1], [0, 1], 'k--')\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title('ROC Curve')\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhnD1E-uGr8c"
      },
      "source": [
        "## Run the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SEED = 0\n",
        "IMG_SIZE = (160,160,160)\n",
        "VAL_SIZE = (256,256,256)\n",
        "SAVE_PATH = \"lightning_logs/\"\n",
        "run_idx = len(os.listdir(\"wandb\"))\n",
        "RUN_NAME = f\"Predict_Segmentation_{run_idx+1}\"\n",
        "pytorch_lightning.seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f850d7c63b9468a8159aa1e484c5211",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">Predict_Segmentation_77</strong>: <a href=\"https://wandb.ai/lalvarez/traumaIA/runs/2qwwe7uw\" target=\"_blank\">https://wandb.ai/lalvarez/traumaIA/runs/2qwwe7uw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220526_181850-2qwwe7uw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    wandb.finish()\n",
        "except:\n",
        "    print(\"Wandb not initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predict_Segmentation_78 Thu May 26 18:23:20 2022 Loggers start\n",
            "Predict_Segmentation_78 Thu May 26 18:23:20 2022 ts_script: 1653582200.200363\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.17 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/wandb/run-20220526_182320-3f1hfcvi</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/lalvarez/traumaIA/runs/3f1hfcvi\" target=\"_blank\">Predict_Segmentation_78</a></strong> to <a href=\"https://wandb.ai/lalvarez/traumaIA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "lnp = Log_and_print(RUN_NAME)\n",
        "lnp.lnp(\"Loggers start\")\n",
        "lnp.lnp(\"ts_script: \" + str(time.time()))\n",
        "\n",
        "wandb_logger = pytorch_lightning.loggers.WandbLogger(\n",
        "    project=\"traumaIA\",\n",
        "    name=RUN_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CALLBACKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predict_Segmentation_78 Thu May 26 18:23:26 2022 MAIN callbacks\n",
            "Predict_Segmentation_78 Thu May 26 18:23:26 2022 checkpoint_dirpath: lightning_logs/checkpoints/\n",
            "Predict_Segmentation_78 Thu May 26 18:23:26 2022 checkpoint_filename: lightning_logs_Predict_Segmentation_78\n"
          ]
        }
      ],
      "source": [
        "lnp.lnp(\"MAIN callbacks\")\n",
        "l_callbacks = []\n",
        "cbEarlyStopping = pytorch_lightning.callbacks.early_stopping.EarlyStopping(\n",
        "    monitor=\"dice_metric_injure\", patience=60, mode=\"max\"\n",
        ")\n",
        "l_callbacks.append(cbEarlyStopping)\n",
        "\n",
        "\n",
        "checkpoint_dirpath = SAVE_PATH + \"checkpoints/\"\n",
        "checkpoint_filename = SAVE_PATH[:-1] + \"_\" + RUN_NAME\n",
        "lnp.lnp(\"checkpoint_dirpath: \" + checkpoint_dirpath)\n",
        "lnp.lnp(\"checkpoint_filename: \" + checkpoint_filename)\n",
        "cbModelCheckpoint = pytorch_lightning.callbacks.ModelCheckpoint(\n",
        "    monitor=\"dice_metric_injure\", mode=\"max\", dirpath=checkpoint_dirpath, filename=checkpoint_filename, \n",
        ")\n",
        "l_callbacks.append(cbModelCheckpoint)\n",
        "\n",
        "l_callbacks.append(PrintTableMetricsCallback())\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
        "l_callbacks.append(lr_monitor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predict_Segmentation_78 Thu May 26 18:23:27 2022  Start Trainining process...\n",
            "using a pretrained model.\n",
            "2022-05-26 18:23:28,706 - INFO - Expected md5 is None, skip md5 check for file spleen_data/clara_pt_liver_and_tumor_ct_segmentation_4.1.zip.\n",
            "2022-05-26 18:23:28,708 - INFO - File exists: spleen_data/clara_pt_liver_and_tumor_ct_segmentation_4.1.zip, skipped downloading.\n",
            "2022-05-26 18:23:28,715 - INFO - Non-empty folder exists in spleen_data/clara_pt_liver_and_tumor_ct_segmentation, skipped extracting.\n",
            "2022-05-26 18:23:28,718 - INFO - \n",
            "*** \"clara_pt_liver_and_tumor_ct_segmentation\" available at spleen_data/clara_pt_liver_and_tumor_ct_segmentation.\n",
            "2022-05-26 18:23:28,999 - INFO - *** Model: <class 'monai.networks.nets.unet.UNet'>\n",
            "2022-05-26 18:23:29,068 - INFO - *** Model params: {'dimensions': 3, 'in_channels': 1, 'out_channels': 3, 'channels': [16, 32, 64, 128, 256], 'strides': [2, 2, 2, 2], 'num_res_units': 2, 'norm': 'batch'}\n",
            "2022-05-26 18:23:29,097 - INFO - \n",
            "---\n",
            "2022-05-26 18:23:29,099 - INFO - For more information, please visit https://ngc.nvidia.com/catalog/models/nvidia:med:clara_pt_liver_and_tumor_ct_segmentation\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'dst' model updated: 148 of 148 variables.\n",
            "num. var. using the pretrained 148 , random init 0 variables.\n",
            "validation files [{'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_066_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_066_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_036_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_036_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_005_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_005_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_057_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_057_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_053_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_053_0000.nii.gz'}]\n",
            "len(train_files) 64\n",
            "len(validation files) 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 42120.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accessing lmdb file: /mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/monai_cache.lmdb.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 8927.85it/s]\n",
            "Checkpoint directory /mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/lightning_logs/checkpoints exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Found unsupported keys in the optimizer configuration: {'interval', 'scheduler'}\n",
            "\n",
            "  | Name          | Type          | Params\n",
            "------------------------------------------------\n",
            "0 | _model        | UNet          | 4.8 M \n",
            "1 | loss_function | DiceFocalLoss | 0     \n",
            "------------------------------------------------\n",
            "4.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.8 M     Total params\n",
            "19.240    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accessing lmdb file: /mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/monai_cache.lmdb.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c648604a5c64a14a169d6c3da22f518",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "339213dcf5374e80a477c18909764426",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n",
            "\n",
            "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
            "\n",
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 0 current liver dice: 0.6769current injure  dice: 0.2312\n",
            "best mean dice: 0.2312 at epoch: 0\n",
            "Predict_Segmentation_78 Thu May 26 18:24:14 2022 Dice Loss: 0.5002999305725098\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83870e79f83e4f9d894f73fe50a29c18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 1 current liver dice: 0.7342current injure  dice: 0.2593\n",
            "best mean dice: 0.2593 at epoch: 1\n",
            "Predict_Segmentation_78 Thu May 26 18:24:26 2022 Dice Loss: 0.43286561965942383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14f4721b73294a45972e9661c440db64",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 2 current liver dice: 0.7045current injure  dice: 0.2774\n",
            "best mean dice: 0.2774 at epoch: 2\n",
            "Predict_Segmentation_78 Thu May 26 18:24:37 2022 Dice Loss: 0.3960399627685547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81150a78bc774d85bec3ea78e994f0c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 3 current liver dice: 0.7028current injure  dice: 0.2778\n",
            "best mean dice: 0.2778 at epoch: 3\n",
            "Predict_Segmentation_78 Thu May 26 18:24:49 2022 Dice Loss: 0.3832627534866333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.7028151750564575â”‚0.2777789831161499\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64db3d834bef4c6987fc11c1211ab721",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.6530170440673828â”‚0.2565494477748871\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 4 current liver dice: 0.6530current injure  dice: 0.2565\n",
            "best mean dice: 0.2778 at epoch: 3\n",
            "Predict_Segmentation_78 Thu May 26 18:25:00 2022 Dice Loss: 0.37558937072753906\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "944292076e764cef8f212296758b5fa8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.7463485598564148â”‚0.3058048188686371\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 5 current liver dice: 0.7463current injure  dice: 0.3058\n",
            "best mean dice: 0.3058 at epoch: 5\n",
            "Predict_Segmentation_78 Thu May 26 18:25:11 2022 Dice Loss: 0.3611050248146057\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7463485598564148â”‚0.3058048188686371\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc9839034de44165addee067ef558aa0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7149161100387573â”‚0.28446805477142334\n",
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7149161100387573â”‚0.28446805477142334\n",
            "0.7149161100387573â”‚0.28446805477142334\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 6 current liver dice: 0.7149current injure  dice: 0.2845\n",
            "best mean dice: 0.3058 at epoch: 5\n",
            "Predict_Segmentation_78 Thu May 26 18:25:23 2022 Dice Loss: 0.3544299900531769\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5aa89ebc48234adeb3600be19d625671",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7149161100387573â”‚0.28446805477142334\n",
            "0.7149161100387573â”‚0.28446805477142334\n",
            "0.8691362142562866â”‚0.36323145031929016\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 7 current liver dice: 0.8691current injure  dice: 0.3632\n",
            "best mean dice: 0.3632 at epoch: 7\n",
            "Predict_Segmentation_78 Thu May 26 18:25:34 2022 Dice Loss: 0.347711980342865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7149161100387573â”‚0.28446805477142334\n",
            "0.7149161100387573â”‚0.28446805477142334\n",
            "0.8691362142562866â”‚0.36323145031929016\n",
            "0.8691362142562866â”‚0.36323145031929016\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f076d4da7df40f9bb40fb7a85ed9600",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7149161100387573â”‚0.28446805477142334\n",
            "0.7149161100387573â”‚0.28446805477142334\n",
            "0.8691362142562866â”‚0.36323145031929016\n",
            "0.8691362142562866â”‚0.36323145031929016\n",
            "0.8215296864509583â”‚0.3480059802532196\n",
            "dice_metric_liverâ”‚dice_metric_injure\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.6769118309020996â”‚0.2312137484550476\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.734175980091095â”‚0.2593151330947876\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7044748067855835â”‚0.2774018347263336\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.7028151750564575â”‚0.2777789831161499\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.6530170440673828â”‚0.2565494477748871\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7463485598564148â”‚0.3058048188686371\n",
            "0.7149161100387573â”‚0.28446805477142334\n",
            "0.7149161100387573â”‚0.28446805477142334\n",
            "0.8691362142562866â”‚0.36323145031929016\n",
            "0.8691362142562866â”‚0.36323145031929016\n",
            "0.8215296864509583â”‚0.3480059802532196\n",
            "0.8215296864509583â”‚0.3480059802532196\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 8 current liver dice: 0.8215current injure  dice: 0.3480\n",
            "best mean dice: 0.3632 at epoch: 7\n",
            "Predict_Segmentation_78 Thu May 26 18:25:46 2022 Dice Loss: 0.34429240226745605\n"
          ]
        }
      ],
      "source": [
        "# initialise the LightningModule\n",
        "lnp.lnp(\" Start Trainining process...\")\n",
        "net = Net(train_img_size=IMG_SIZE, val_img_size=VAL_SIZE)#.load_from_checkpoint(\"lightning_logs\\checkpoints\\lightning_logs_Segmentation_14.ckpt\", train_img_size=IMG_SIZE, val_img_size=VAL_SIZE)\n",
        "wandb_logger.watch(net)\n",
        "\n",
        "# set up loggers and checkpoints\n",
        "log_dir = os.path.join(root_dir, \"logs\")\n",
        "\n",
        "# initialise Lightning's trainer.\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    gpus=[0],\n",
        "    max_epochs=250,\n",
        "    auto_lr_find=False,\n",
        "    logger=wandb_logger,\n",
        "    enable_checkpointing=True,\n",
        "    num_sanity_val_steps=0,\n",
        "    log_every_n_steps=1,\n",
        "    callbacks=l_callbacks,\n",
        ")\n",
        "\n",
        "# train\n",
        "result_pred2 = trainer.fit(net)\n",
        "wandb.alert(\n",
        "    title=\"Train finished\", \n",
        "    text=\"The train has finished\"\n",
        ")\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using a pretrained model.\n",
            "2022-05-26 17:38:10,190 - INFO - Expected md5 is None, skip md5 check for file spleen_data/clara_pt_liver_and_tumor_ct_segmentation_4.1.zip.\n",
            "2022-05-26 17:38:10,192 - INFO - File exists: spleen_data/clara_pt_liver_and_tumor_ct_segmentation_4.1.zip, skipped downloading.\n",
            "2022-05-26 17:38:10,199 - INFO - Non-empty folder exists in spleen_data/clara_pt_liver_and_tumor_ct_segmentation, skipped extracting.\n",
            "2022-05-26 17:38:10,202 - INFO - \n",
            "*** \"clara_pt_liver_and_tumor_ct_segmentation\" available at spleen_data/clara_pt_liver_and_tumor_ct_segmentation.\n",
            "2022-05-26 17:38:10,503 - INFO - *** Model: <class 'monai.networks.nets.unet.UNet'>\n",
            "2022-05-26 17:38:10,557 - INFO - *** Model params: {'dimensions': 3, 'in_channels': 1, 'out_channels': 3, 'channels': [16, 32, 64, 128, 256], 'strides': [2, 2, 2, 2], 'num_res_units': 2, 'norm': 'batch'}\n",
            "2022-05-26 17:38:10,583 - INFO - \n",
            "---\n",
            "2022-05-26 17:38:10,585 - INFO - For more information, please visit https://ngc.nvidia.com/catalog/models/nvidia:med:clara_pt_liver_and_tumor_ct_segmentation\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning:\n",
            "\n",
            "`max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
            "\n",
            "GPU available: True, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'dst' model updated: 148 of 148 variables.\n",
            "num. var. using the pretrained 148 , random init 0 variables.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1812: PossibleUserWarning:\n",
            "\n",
            "GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation files [{'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_030_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_030_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_034_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_034_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_038_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_038_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_003_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_003_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTr/TRMLIV_015_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTr/TRMLIV_015_0000.nii.gz'}]\n",
            "len(train_files) 64\n",
            "len(validation files) 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 31502.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accessing lmdb file: /mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/monai_cache.lmdb.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 2786.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accessing lmdb file: /mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/monai_cache.lmdb.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1007: UndefinedMetricWarning:\n",
            "\n",
            "No negative samples in y_true, false positive value should be meaningless\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 0 current liver dice: 0.6386current injure  dice: 0.0000\n",
            "best mean dice: 0.0000 at epoch: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning:\n",
            "\n",
            "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 28 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "\n",
            "/home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning:\n",
            "\n",
            "The number of training batches (32) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [1, 160, 160, 160] at entry 0 and [1, 256, 256, 256] at entry 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/liver-segmentation-lightning-predict.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/liver-segmentation-lightning-predict.ipynb#ch0000026vscode-remote?line=3'>4</a>\u001b[0m trainer \u001b[39m=\u001b[39m pytorch_lightning\u001b[39m.\u001b[39mTrainer()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/liver-segmentation-lightning-predict.ipynb#ch0000026vscode-remote?line=5'>6</a>\u001b[0m \u001b[39m#  Run learning rate finder\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/liver-segmentation-lightning-predict.ipynb#ch0000026vscode-remote?line=6'>7</a>\u001b[0m lr_finder \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49mlr_find(net)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/liver-segmentation-lightning-predict.ipynb#ch0000026vscode-remote?line=8'>9</a>\u001b[0m \u001b[39m# Results can be found in\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/liver-segmentation-lightning-predict.ipynb#ch0000026vscode-remote?line=9'>10</a>\u001b[0m lr_finder\u001b[39m.\u001b[39mresults\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py:192\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=154'>155</a>\u001b[0m \u001b[39m\"\"\"Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=155'>156</a>\u001b[0m \u001b[39mpicking a good starting learning rate.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=156'>157</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=188'>189</a>\u001b[0m \u001b[39m        or if you are using more than one optimizer.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=189'>190</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=190'>191</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_lr_find \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=191'>192</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtune(\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=192'>193</a>\u001b[0m     model,\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=193'>194</a>\u001b[0m     train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloaders,\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=194'>195</a>\u001b[0m     val_dataloaders\u001b[39m=\u001b[39;49mval_dataloaders,\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=195'>196</a>\u001b[0m     datamodule\u001b[39m=\u001b[39;49mdatamodule,\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=196'>197</a>\u001b[0m     lr_find_kwargs\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=197'>198</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmin_lr\u001b[39;49m\u001b[39m\"\u001b[39;49m: min_lr,\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=198'>199</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmax_lr\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_lr,\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=199'>200</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mnum_training\u001b[39;49m\u001b[39m\"\u001b[39;49m: num_training,\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=200'>201</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmode\u001b[39;49m\u001b[39m\"\u001b[39;49m: mode,\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=201'>202</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mearly_stop_threshold\u001b[39;49m\u001b[39m\"\u001b[39;49m: early_stop_threshold,\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=202'>203</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mupdate_attr\u001b[39;49m\u001b[39m\"\u001b[39;49m: update_attr,\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=203'>204</a>\u001b[0m     },\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=204'>205</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=205'>206</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_lr_find \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=206'>207</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result[\u001b[39m\"\u001b[39m\u001b[39mlr_find\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1126\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1120'>1121</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1121'>1122</a>\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1122'>1123</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1124'>1125</a>\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1125'>1126</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_tune(\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1126'>1127</a>\u001b[0m         model, scale_batch_size_kwargs\u001b[39m=\u001b[39;49mscale_batch_size_kwargs, lr_find_kwargs\u001b[39m=\u001b[39;49mlr_find_kwargs\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1127'>1128</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1129'>1130</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1130'>1131</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtuning \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py:63\u001b[0m, in \u001b[0;36mTuner._tune\u001b[0;34m(self, model, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=60'>61</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_lr_find:\n\u001b[1;32m     <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=61'>62</a>\u001b[0m     lr_find_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mupdate_attr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=62'>63</a>\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39mlr_find\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m lr_find(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer, model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mlr_find_kwargs)\n\u001b[1;32m     <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=64'>65</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mFINISHED\n\u001b[1;32m     <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=66'>67</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/lr_finder.py:224\u001b[0m, in \u001b[0;36mlr_find\u001b[0;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/lr_finder.py?line=220'>221</a>\u001b[0m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39msetup_optimizers \u001b[39m=\u001b[39m lr_finder\u001b[39m.\u001b[39m_exchange_scheduler(trainer, model)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/lr_finder.py?line=222'>223</a>\u001b[0m \u001b[39m# Fit, lr & loss logged in callback\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/lr_finder.py?line=223'>224</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_run(model)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/lr_finder.py?line=225'>226</a>\u001b[0m \u001b[39m# Prompt if we stopped early\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/lr_finder.py?line=226'>227</a>\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mglobal_step \u001b[39m!=\u001b[39m num_training:\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py:73\u001b[0m, in \u001b[0;36mTuner._run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=70'>71</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING  \u001b[39m# last `_run` call might have set it to `FINISHED`\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=71'>72</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=72'>73</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py?line=73'>74</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtuning \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1229'>1230</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1231'>1232</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1233'>1234</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1235'>1236</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1236'>1237</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1318'>1319</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1319'>1320</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1320'>1321</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1351\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1348'>1349</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1349'>1350</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1350'>1351</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:268\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=263'>264</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=264'>265</a>\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=265'>266</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=266'>267</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=267'>268</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:171\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=168'>169</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=169'>170</a>\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=170'>171</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=171'>172</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=172'>173</a>\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=182'>183</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=183'>184</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:259\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=255'>256</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=256'>257</a>\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=257'>258</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=258'>259</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=259'>260</a>\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=260'>261</a>\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:273\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=270'>271</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetch_next_batch\u001b[39m(\u001b[39mself\u001b[39m, iterator: Iterator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=271'>272</a>\u001b[0m     start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=272'>273</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=273'>274</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=274'>275</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_batches \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_len:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py?line=275'>276</a>\u001b[0m         \u001b[39m# when we don't prefetch but the dataloader is sized, we use the length for `done`\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py:553\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=546'>547</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=547'>548</a>\u001b[0m     \u001b[39m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=548'>549</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=549'>550</a>\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=550'>551</a>\u001b[0m \u001b[39m        a collections of batch data\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=551'>552</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=552'>553</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_iters)\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py:565\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[0;34m(loader_iters)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=554'>555</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=555'>556</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=556'>557</a>\u001b[0m     \u001b[39m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=557'>558</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=558'>559</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=562'>563</a>\u001b[0m \u001b[39m        Any: a collections of batch data\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=563'>564</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py?line=564'>565</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_to_collection(loader_iters, Iterator, \u001b[39mnext\u001b[39;49m)\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/apply_func.py:99\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/apply_func.py?line=96'>97</a>\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/apply_func.py?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/apply_func.py?line=98'>99</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/apply_func.py?line=100'>101</a>\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/pytorch_lightning/utilities/apply_func.py?line=102'>103</a>\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:157\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=154'>155</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=155'>156</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=156'>157</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: default_collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=157'>158</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=158'>159</a>\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=159'>160</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: default_collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:157\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=154'>155</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=155'>156</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=156'>157</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: default_collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=157'>158</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=158'>159</a>\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=159'>160</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: default_collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n",
            "File \u001b[0;32m~/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:138\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=135'>136</a>\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=136'>137</a>\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=137'>138</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=138'>139</a>\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=139'>140</a>\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=140'>141</a>\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///home/trauma/miniconda3/envs/env_trauma/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py?line=141'>142</a>\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 160, 160, 160] at entry 0 and [1, 256, 256, 256] at entry 1"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "net = Net(train_img_size=IMG_SIZE, val_img_size=VAL_SIZE)#.load_from_checkpoint(\"lightning_logs\\checkpoints\\lightning_logs_Segmentation_14.ckpt\", train_img_size=IMG_SIZE, val_img_size=VAL_SIZE)\n",
        "\n",
        "# initialise Lightning's trainer.\n",
        "trainer = pytorch_lightning.Trainer()\n",
        "\n",
        "#  Run learning rate finder\n",
        "lr_finder = trainer.tuner.lr_find(net)\n",
        "\n",
        "# Results can be found in\n",
        "lr_finder.results\n",
        "\n",
        "# Plot with\n",
        "fig = lr_finder.plot(suggest=True)\n",
        "fig.show()      \n",
        "\n",
        "# Pick point based on plot, or get suggestion\n",
        "new_lr = lr_finder.suggestion()\n",
        "\n",
        "# update hparams of the model\n",
        "net.hparams.lr = new_lr\n",
        "\n",
        "# Fit model\n",
        "# trainer.fit(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9205692ac387463594db2381ad5f88c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">Predict_Segmentation_23</strong>: <a href=\"https://wandb.ai/lalvarez/traumaIA/runs/2jptek2f\" target=\"_blank\">https://wandb.ai/lalvarez/traumaIA/runs/2jptek2f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220525_180636-2jptek2f/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoGZ_9q3Gr8e"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoUeVHa5Gr8e"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "spleen_segmentation_3d_lightning.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "16d90db93701a4e14595aea1a6791a3dd0d33758ed6b394279d759beaff9b73f"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('env_trauma')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
