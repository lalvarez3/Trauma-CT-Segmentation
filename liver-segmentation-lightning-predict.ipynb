{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djzZ-bffGr8X"
   },
   "source": [
    "#### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "biFwQV_yGr8X",
    "outputId": "caa6e216-4409-4de4-bc5a-ceec09bf6af7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.8.1\n",
      "Numpy version: 1.22.3\n",
      "Pytorch version: 1.11.0\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 71ff399a3ea07aef667b23653620a290364095b1\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 3.2.2\n",
      "scikit-image version: 0.19.2\n",
      "Pillow version: 9.0.1\n",
      "Tensorboard version: 2.9.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.12.0\n",
      "tqdm version: 4.64.0\n",
      "lmdb version: 1.3.0\n",
      "psutil version: 5.9.0\n",
      "pandas version: 1.4.2\n",
      "einops version: 0.4.1\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from monai.data.image_reader import ImageReader, ITKReader\n",
    "from ipywidgets.widgets import *\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning\n",
    "from monai.utils import set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureType,\n",
    "    EnsureChannelFirstd,\n",
    "    RandFlipd,\n",
    "    RandRotated,\n",
    "    ToTensord,\n",
    "    Resized,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandRotate90d,\n",
    "    RandShiftIntensityd,\n",
    "    KeepLargestConnectedComponent,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandCropByLabelClassesd\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceFocalLoss, GeneralizedDiceLoss\n",
    "from monai.losses import DiceLoss, DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, list_data_collate, decollate_batch, Dataset, LMDBDataset\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from monai.data import DataLoader\n",
    "import os\n",
    "import glob\n",
    "from monai.transforms.spatial.array import Resize\n",
    "\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from monai.config import DtypeLike, KeysCollection\n",
    "from monai.config.type_definitions import NdarrayOrTensor\n",
    "from monai.networks.layers import AffineTransform\n",
    "from monai.networks.layers.simplelayers import GaussianFilter\n",
    "from monai.transforms.croppad.array import CenterSpatialCrop, SpatialPad\n",
    "from monai.transforms.inverse import InvertibleTransform\n",
    "from monai.transforms.spatial.array import (\n",
    "    Resize,\n",
    ")\n",
    "from monai.transforms.transform import MapTransform, RandomizableTransform\n",
    "from monai.transforms.utils import create_grid\n",
    "from monai.utils import (\n",
    "    InterpolateMode,\n",
    "    ensure_tuple_rep,\n",
    ")\n",
    "from monai.utils.deprecate_utils import deprecated_arg\n",
    "from monai.utils.enums import TraceKeys\n",
    "from monai.utils.module import optional_import\n",
    "from monai.utils.type_conversion import convert_data_type, convert_to_dst_type\n",
    "from monai.apps import load_from_mmar\n",
    "from monai.apps.mmars import RemoteMMARKeys\n",
    "from monai.networks.utils import copy_model_state\n",
    "from monai.optimizers import generate_param_groups\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from pl_bolts.callbacks import PrintTableMetricsCallback\n",
    "from monai.visualize import matshow3d, blend_images\n",
    "import imageio\n",
    "print_config()\n",
    "from monai.losses import GeneralizedWassersteinDiceLoss\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolateMode(Enum):\n",
    "    NEAREST = \"nearest\"\n",
    "    LINEAR = \"linear\"\n",
    "    BILINEAR = \"bilinear\"\n",
    "    BICUBIC = \"bicubic\"\n",
    "    TRILINEAR = \"trilinear\"\n",
    "    AREA = \"area\"\n",
    "\n",
    "\n",
    "InterpolateModeSequence = Union[\n",
    "    Sequence[Union[InterpolateMode, str]], InterpolateMode, str\n",
    "]\n",
    "\n",
    "class ResizedC(MapTransform, InvertibleTransform):\n",
    "\n",
    "    backend = Resize.backend\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        spatial_size: Union[Sequence[int], int],\n",
    "        size_mode: str = \"all\",\n",
    "        mode: InterpolateModeSequence = InterpolateMode.AREA,\n",
    "        align_corners: Union[Sequence[Optional[bool]], Optional[bool]] = None,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.mode = ensure_tuple_rep(mode, len(self.keys))\n",
    "        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))\n",
    "        self.resizer = Resize(spatial_size=spatial_size, size_mode=size_mode)\n",
    "        self.spatial_size = spatial_size\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key, mode, align_corners in self.key_iterator(\n",
    "            d, self.mode, self.align_corners\n",
    "        ):\n",
    "            self.push_transform(\n",
    "                d,\n",
    "                key,\n",
    "                extra_info={\n",
    "                    \"mode\": mode.value if isinstance(mode, Enum) else mode,\n",
    "                    \"align_corners\": align_corners\n",
    "                    if align_corners is not None\n",
    "                    else TraceKeys.NONE,\n",
    "                },\n",
    "            )\n",
    "            init_slice = int(d[key].shape[-1]*0.15)\n",
    "            end_slice = int(d[key].shape[-1]*0.1)\n",
    "            # Reduce Size in Memory\n",
    "            if key == \"label\":\n",
    "                d[key] = d[key].astype(np.int8)\n",
    "                if d[key].shape[-1] > 600: d[key] = d[key][:,:,:,init_slice:-end_slice] #\n",
    "\n",
    "                if d[\"image_meta_dict\"].get(\"PatientName\", None) and d[\"image_meta_dict\"][\"PatientName\"].startswith(\"NI\") and len(d[key].shape) != 4:\n",
    "                    # print(d[key].shape)\n",
    "                    liver_channel = np.where((d[key] != 6), 0, d[key])\n",
    "                    liver_channel = np.where((liver_channel == 6), 1, liver_channel)\n",
    "                    # liver_channel = np.expand_dims(liver_channel, 0)\n",
    "                    w, h, z = self.spatial_size\n",
    "                    liver_channel = self.resizer(liver_channel, align_corners=align_corners)\n",
    "                    background = np.ones((1, z, w, h), dtype=np.float16) - liver_channel\n",
    "                    empty_injures = np.zeros((1, z, w, h), dtype=np.float16)\n",
    "                    resized = [background, liver_channel, empty_injures]\n",
    "                    d[key] = np.stack(resized).astype(np.int8).squeeze()\n",
    "\n",
    "                else:\n",
    "                    label = d[key]\n",
    "                    w, h, z = self.spatial_size\n",
    "                    resized = list()\n",
    "                    background = np.ones((1, w, h, z), dtype=np.int8)\n",
    "                    for i, channel in enumerate([0, 2]):  # TODO: desharcodead\n",
    "                        resized.append(\n",
    "                            self.resizer(\n",
    "                                np.expand_dims(label[channel, :, :, :], 0),\n",
    "                                align_corners=align_corners,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                    background -= resized[0] # + resized[1]\n",
    "                    resized = [background] + resized\n",
    "                    d[key] = np.stack(resized).astype(np.int8).squeeze()\n",
    "            else:\n",
    "                if d[key].shape[-1] > 600: d[key] = d[key][:,:,:,init_slice:-end_slice]\n",
    "                d[key] = self.resizer(d[key], align_corners=align_corners)\n",
    "\n",
    "        keys = ['spacing', 'original_affine', 'affine', 'spatial_shape', 'original_channel_dim', 'filename_or_obj']\n",
    "        new_label_metadata = dict()\n",
    "        for key in keys:\n",
    "            new_label_metadata[key] = d[\"label_meta_dict\"].get(key, 0)\n",
    "\n",
    "        d[\"label_meta_dict\"] = new_label_metadata\n",
    "\n",
    "        if \"PatientID\" not in d[\"image_meta_dict\"]:\n",
    "            d[\"image_meta_dict\"][\"PatientID\"] = \"0\"\n",
    "        if \"PatientName\" not in d[\"image_meta_dict\"]:\n",
    "            d[\"image_meta_dict\"][\"PatientName\"] = \"0\"\n",
    "        if \"SliceThickness\" not in d[\"image_meta_dict\"]:\n",
    "            d[\"image_meta_dict\"][\"SliceThickness\"] = \"0\"\n",
    "        return d\n",
    "\n",
    "    def inverse(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            transform = self.get_most_recent_transform(d, key)\n",
    "            orig_size = transform[TraceKeys.ORIG_SIZE]\n",
    "            mode = transform[TraceKeys.EXTRA_INFO][\"mode\"]\n",
    "            align_corners = transform[TraceKeys.EXTRA_INFO][\"align_corners\"]\n",
    "            # Create inverse transform\n",
    "            inverse_transform = Resize(\n",
    "                spatial_size=orig_size,\n",
    "                mode=mode,\n",
    "                align_corners=None\n",
    "                if align_corners == TraceKeys.NONE\n",
    "                else align_corners,\n",
    "            )\n",
    "            # Apply inverse transform\n",
    "            d[key] = inverse_transform(d[key])\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adaptOverlay(MapTransform, InvertibleTransform):\n",
    "\n",
    "    backend = Resize.backend\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        size_mode: str = \"all\",\n",
    "        mode: InterpolateModeSequence = InterpolateMode.AREA,\n",
    "        align_corners: Union[Sequence[Optional[bool]], Optional[bool]] = None,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.mode = ensure_tuple_rep(mode, len(self.keys))\n",
    "        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))\n",
    "\n",
    "    def __adapt_overlay__(self, overlay_path, mha_path, label):\n",
    "        import SimpleITK as sitk\n",
    "        if label.shape[-1] == 6:\n",
    "            return label\n",
    "        # Load the mha\n",
    "        mha_data = sitk.ReadImage(mha_path)\n",
    "        mha_org = mha_data.GetOrigin()[-1]\n",
    "        # Load the mha image\n",
    "        mha_img = sitk.GetArrayFromImage(mha_data)\n",
    "        original_z_size = mha_img.shape[0]\n",
    "\n",
    "        # Load the overlay\n",
    "        overlay_data = sitk.ReadImage(overlay_path)\n",
    "        overlay_org = overlay_data.GetOrigin()[-1]\n",
    "\n",
    "        overlay_init = np.abs(1/mha_data.GetSpacing()[-1]*(mha_org-overlay_org) )\n",
    "\n",
    "        lower_bound = int(overlay_init)\n",
    "        upper_bound = label.shape[-1]\n",
    "        zeros_up = lower_bound\n",
    "        zeros_down = original_z_size - (upper_bound + lower_bound)\n",
    "        new = list()\n",
    "\n",
    "        if zeros_up > 0:\n",
    "            new.append(np.zeros((label.shape[0], label.shape[1], zeros_up), dtype=label.dtype))\n",
    "\n",
    "        new.append(label)\n",
    "\n",
    "        if zeros_down > 0:\n",
    "            new.append(np.zeros((label.shape[0], label.shape[1], zeros_down), dtype=label.dtype))\n",
    "\n",
    "        label = np.concatenate(new, axis=2)\n",
    "\n",
    "        return label\n",
    "\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key, mode, align_corners in self.key_iterator(\n",
    "            d, self.mode, self.align_corners\n",
    "        ):\n",
    "            self.push_transform(\n",
    "                d,\n",
    "                key,\n",
    "                extra_info={\n",
    "                    \"mode\": mode.value if isinstance(mode, Enum) else mode,\n",
    "                    \"align_corners\": align_corners\n",
    "                    if align_corners is not None\n",
    "                    else TraceKeys.NONE,\n",
    "                },\n",
    "            )\n",
    "            # Reduce Size in Memory\n",
    "            if key == \"label\":\n",
    "                d[key] = d[key].astype(np.int8)\n",
    "                if d[\"image_meta_dict\"].get(\"PatientName\", None) and d[\"image_meta_dict\"][\"PatientName\"].startswith(\"NI\"):\n",
    "                    file_path = d[\"label_meta_dict\"][\"filename_or_obj\"]\n",
    "                    data_path = d[\"image_meta_dict\"][\"filename_or_obj\"]\n",
    "                    d[key] = self.__adapt_overlay__(file_path, data_path, d[key])\n",
    "        return d\n",
    "\n",
    "    def inverse(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            transform = self.get_most_recent_transform(d, key)\n",
    "            orig_size = transform[TraceKeys.ORIG_SIZE]\n",
    "            mode = transform[TraceKeys.EXTRA_INFO][\"mode\"]\n",
    "            align_corners = transform[TraceKeys.EXTRA_INFO][\"align_corners\"]\n",
    "            # Create inverse transform\n",
    "            inverse_transform = Resize(\n",
    "                spatial_size=orig_size,\n",
    "                mode=mode,\n",
    "                align_corners=None\n",
    "                if align_corners == TraceKeys.NONE\n",
    "                else align_corners,\n",
    "            )\n",
    "            # Apply inverse transform\n",
    "            d[key] = inverse_transform(d[key])\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeepOnlyClass(MapTransform, InvertibleTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        class_to_keep: int,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.class_to_keep = class_to_keep\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        print(d[\"label\"].shape)\n",
    "        for key in self.key_iterator(d):\n",
    "            self.push_transform(d, key)\n",
    "            d[key] = np.where((d[key] != self.class_to_keep), 0, d[key])\n",
    "            d[key] = np.where((d[key] == self.class_to_keep), 1, d[key])\n",
    "            d[key] = d[key][d[key] == self.class_to_keep]\n",
    "        return d\n",
    "\n",
    "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = d[key]\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveDicts(MapTransform, InvertibleTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        allow_missing_keys: bool = False,\n",
    "        verbose:bool = False\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            self.push_transform(d, key)\n",
    "        # print(d[\"image_meta_dict\"][\"filename_or_obj\"])\n",
    "        a = {\"image\": d[\"image\"], \"label\": d[\"label\"], \"path\": d[\"image_meta_dict\"][\"filename_or_obj\"]}\n",
    "        if self.verbose:\n",
    "            print(a[\"path\"])\n",
    "        d = a\n",
    "        return d\n",
    "\n",
    "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = d[key]\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintInfo(MapTransform, InvertibleTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            self.push_transform(d, key)\n",
    "        # print(d[\"image_meta_dict\"][\"filename_or_obj\"])\n",
    "        # a = {\"image\": d[\"image\"], \"label\": d[\"label\"], \"path\": d[\"image_meta_dict\"][\"filename_or_obj\"]}\n",
    "        print(d[\"path\"])\n",
    "        # d = a\n",
    "        return d\n",
    "\n",
    "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = d[key]\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED = False\n",
    "TRANSFER_LEARNING = False\n",
    "N_WORKERS_LOADER = 4\n",
    "N_WORKERS_CACHE = 4\n",
    "CACHE_RATE = 0\n",
    "SEED = 42\n",
    "BS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WzCJRUGGr8a"
   },
   "source": [
    "#### Define the LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spleen_data\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = \"spleen_data\"\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, train_img_size, val_img_size, n_output):\n",
    "        super().__init__()\n",
    "        self.train_img_size = train_img_size\n",
    "        self.val_img_size = val_img_size\n",
    "        self.n_output = n_output\n",
    "        if PRETRAINED:\n",
    "            print(\"using a pretrained model.\")\n",
    "            unet_model = load_from_mmar(\n",
    "                item=mmar[RemoteMMARKeys.NAME],\n",
    "                mmar_dir=root_dir,\n",
    "                # map_location=device,\n",
    "                pretrained=True,\n",
    "            )\n",
    "            self._model = unet_model\n",
    "            # copy all the pretrained weights except for variables whose name matches \"model.0.conv.unit0\"\n",
    "            if TRANSFER_LEARNING:\n",
    "                pretrained_dict, updated_keys, unchanged_keys = copy_model_state(\n",
    "                    self._model, unet_model,  #exclude_vars=\"model.[0-2].conv.unit[0-3]\"\n",
    "                )\n",
    "                print(\n",
    "                    \"num. var. using the pretrained\",\n",
    "                    len(updated_keys),\n",
    "                    \", random init\",\n",
    "                    len(unchanged_keys),\n",
    "                    \"variables.\",\n",
    "                )\n",
    "                self._model.load_state_dict(pretrained_dict)\n",
    "                # stop gradients for the pretrained weights\n",
    "                for x in self._model.named_parameters():\n",
    "                    if x[0] in updated_keys:\n",
    "                        x[1].requires_grad = True\n",
    "                params = generate_param_groups(\n",
    "                    network=self._model,\n",
    "                    layer_matches=[lambda x: x[0] in updated_keys],\n",
    "                    match_types=[\"filter\"],\n",
    "                    lr_values=[1e-4],\n",
    "                    include_others=False,\n",
    "                )\n",
    "                self.params = params\n",
    "\n",
    "        else:\n",
    "            self._model = UNet(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=self.n_output,\n",
    "                channels=(16, 32, 64, 128, 256),\n",
    "                strides=(2, 2, 2, 2),\n",
    "                num_res_units=2,\n",
    "                norm=Norm.BATCH,\n",
    "            )\n",
    "        self.loss_function = DiceFocalLoss(to_onehot_y=True, softmax=True, jaccard=True)\n",
    "        # weights = [1.0, 1.1, 10]\n",
    "        # class_weights = torch.FloatTensor(weights)\n",
    "        # self.loss_function = DiceCELoss(softmax=True, to_onehot_y=True, jaccard =True, ce_weight=class_weights)\n",
    "        self.post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3)])\n",
    "        self.post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
    "        self.train_dice_metric = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "\n",
    "        # dist_mat = np.array([[0.0, 1.0, 1.0], [1.0, 0.0, 0.5], [1.0, 0.5, 0.0]], dtype=np.float32)\n",
    "        # self.loss_function = GeneralizedWassersteinDiceLoss(dist_matrix=dist_mat)\n",
    "\n",
    "\n",
    "        self.save_hyperparameters() # save hyperparameters\n",
    "\n",
    "        # self.logger.expe.init(self.hparams)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # set up the correct data path\n",
    "        train_images = sorted(\n",
    "            glob.glob(\n",
    "                os.path.join(\n",
    "                    \"/mnt/chansey/\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\", \"imagesTr\", \"*.nii.gz\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        train_labels = [img.replace('imagesTr', 'labelsTr') for img in train_images]\n",
    "        \n",
    "        data_dicts = [\n",
    "            {\"image\": image_name, \"label\": label_name}\n",
    "            for image_name, label_name in zip(train_images, train_labels)\n",
    "        ]\n",
    "\n",
    "        test_images = sorted(\n",
    "            glob.glob(\n",
    "                os.path.join(\n",
    "                    \"/mnt/chansey/\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\", \"imagesTs\", \"*.nii.gz\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        test_labels = [img.replace('imagesTs', 'labelsTs') for img in test_images]\n",
    "        \n",
    "        data_dicts_test = [\n",
    "            {\"image\": image_name, \"label\": label_name}\n",
    "            for image_name, label_name in zip(test_images, test_labels)\n",
    "        ]\n",
    "\n",
    "        random.shuffle(data_dicts)\n",
    "        train_files, val_files = data_dicts, data_dicts_test\n",
    "        print(\"validation files\", val_files)\n",
    "        # print(\"training files\", train_files)\n",
    "        print(\"len(train_files)\", len(train_files))\n",
    "        print(\"len(validation files)\", len(val_files))\n",
    "\n",
    "        # set deterministic training for reproducibility\n",
    "        set_determinism(seed=SEED)\n",
    "\n",
    "        # define the data transforms\n",
    "\n",
    "        # Computed for the randomCropByLabel transformation based on outputs\n",
    "        if self.n_output == 3:\n",
    "            ratios = [1, 1, 2]\n",
    "        else:\n",
    "            ratios = [1, 1]\n",
    "\n",
    "        train_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                RemoveDicts(keys=[\"image\", \"label\"]),\n",
    "                # KeepOnlyClass(keys=[\"label\"], class_ids=[2]),\n",
    "                # RemoveDicts(keys=[\"image\", \"label\"]),\n",
    "                AddChanneld(keys=[\"image\", \"label\"]),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.5, 1.5, 2.0),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                # Resized(keys=[\"image\", \"label\"], spatial_size=self.train_img_size),\n",
    "                ScaleIntensityRanged(\n",
    "                    keys=[\"image\"],\n",
    "                    a_min=-175,\n",
    "                    a_max=250,\n",
    "                    b_min=0.0,\n",
    "                    b_max=1.0,\n",
    "                    clip=True,\n",
    "                ),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                # PrintInfo(keys=[\"image\", \"label\"]),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "                RandCropByLabelClassesd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    label_key=\"label\",\n",
    "                    spatial_size=[96, 96, 96],\n",
    "                    ratios=ratios,\n",
    "                    num_classes=self.n_output,\n",
    "                    num_samples=4,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[0],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[1],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[2],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandRotate90d(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    prob=0.10,\n",
    "                    max_k=3,\n",
    "                ),\n",
    "                RandShiftIntensityd(\n",
    "                    keys=[\"image\"],\n",
    "                    offsets=0.10,\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # define the data transforms\n",
    "        val_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                RemoveDicts(keys=[\"image\", \"label\"]),\n",
    "                AddChanneld(keys=[\"image\", \"label\"]),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.5, 1.5, 2.0),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                ScaleIntensityRanged(\n",
    "                    keys=[\"image\"],\n",
    "                    a_min=-175,\n",
    "                    a_max=250,\n",
    "                    b_min=0.0,\n",
    "                    b_max=1.0,\n",
    "                    clip=True,\n",
    "                ),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "            ]\n",
    "        )\n",
    "        # self.train_ds = CacheDataset(\n",
    "        #     data=train_files,\n",
    "        #     transform=train_transforms,\n",
    "        #     cache_rate=CACHE_RATE,\n",
    "        #     num_workers=N_WORKERS_CACHE,\n",
    "        # )\n",
    "\n",
    "        self.train_ds = LMDBDataset(\n",
    "            data=train_files,\n",
    "            transform=train_transforms,\n",
    "            cache_dir=os.path.join(\n",
    "                    \"/mnt/chansey/\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\"\n",
    "                )\n",
    "        )\n",
    "\n",
    "        self.val_ds = LMDBDataset(\n",
    "            data=val_files,\n",
    "            transform=val_transforms,\n",
    "            cache_dir=os.path.join(\n",
    "                    \"/mnt/chansey/\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data_base\", \"nnUNet_raw_data\", \"Task501_LiverTrauma\"\n",
    "                )\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self._model.parameters(), 1e-3)\n",
    "        if PRETRAINED:\n",
    "            optimizer = torch.optim.AdamW(self.params, lr=5e-4, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.05, patience=100, min_lr=1e-7)\n",
    "        return {'optimizer':optimizer, 'lr_scheduler':scheduler, 'monitor':\"val_loss\", \"interval\": \"epoch\"}\n",
    "    \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_ds, batch_size=BS, shuffle=True, num_workers=N_WORKERS_LOADER,\n",
    "            collate_fn=list_data_collate,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            self.val_ds, batch_size=1, shuffle=False, num_workers=N_WORKERS_LOADER, \n",
    "        )\n",
    "        return val_loader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        predict_dataloader = torch.utils.data.DataLoader(\n",
    "            self.val_ds, batch_size=1, shuffle=False, num_workers=N_WORKERS_LOADER\n",
    "        )\n",
    "        return predict_dataloader\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        output = self.forward(images)\n",
    "\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(output)]\n",
    "\n",
    "        labels_1 = [\n",
    "            self.post_label(i) for i in decollate_batch(labels)\n",
    "        ]\n",
    "\n",
    "        self.train_dice_metric(y_pred=outputs, y=labels_1)\n",
    "\n",
    "        loss = self.loss_function(output, labels)\n",
    "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        #  the function is called after every epoch is completed\n",
    "\n",
    "        dice_liver, dice_injure = self.train_dice_metric.aggregate()\n",
    "        self.train_dice_metric.reset()\n",
    "        # calculating average loss\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "\n",
    "        # logging using tensorboard logger\n",
    "        self.log(\"dice loss\", avg_loss)\n",
    "        # lnp.lnp(f\"Dice Loss: {avg_loss}\")\n",
    "        # self.log(\"train background dice\", dice_background)\n",
    "        self.log(\"train liver dice\", dice_liver)\n",
    "        self.log(\"train injure dice\", dice_injure)\n",
    "\n",
    "        self.logger.experiment.log({\"dice loss\": avg_loss})\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        # filenames = batch[\"image_meta_dict\"][\"filename_or_obj\"]\n",
    "        filenames = batch[\"path\"]\n",
    "        post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3)])\n",
    "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "        roi_size = (96, 96, 96)\n",
    "        sw_batch_size = 4\n",
    "        outputs = sliding_window_inference(\n",
    "            images, roi_size, sw_batch_size, self.forward\n",
    "        )\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        predicition ={\"output\": torch.nn.Softmax()(outputs), \"image\": images, \"label\": labels, \"filename\": filenames}\n",
    "        outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
    "\n",
    "        labels = [\n",
    "            post_label(i) for i in decollate_batch(labels)\n",
    "        ]\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        return {\"dice_metric\": self.dice_metric, \"val_number\": len(outputs), \"prediction\": predicition, \"val_loss\": loss}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        val_loss, num_items = 0, 0\n",
    "        for output in outputs:\n",
    "            val_loss += output[\"val_loss\"].sum().item()\n",
    "            num_items += output[\"val_number\"]\n",
    "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "\n",
    "        post_pred_dice = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3), KeepLargestConnectedComponent([1,2], is_onehot=True, independent=False)])\n",
    "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "        dice_liver, dice_injure = self.dice_metric.aggregate()\n",
    "\n",
    "        self.dice_metric.reset()\n",
    "        tensorboard_logs = {\n",
    "            \"dice_metric\": dice_injure,\n",
    "        }\n",
    "\n",
    "        predictions = [x[\"prediction\"] for x in outputs]\n",
    "        \n",
    "        if self.current_epoch % 25 == 0 or dice_injure - self.best_val_dice > 0.1 or self.current_epoch == self.trainer.max_epochs - 1:\n",
    "            test_dt = wandb.Table(columns = ['epoch', 'filename', 'combined output','dice_value_liver', 'dice_value_injure', 'ground_truth', 'class predicted'])\n",
    "            figure = computeROC(predictions)\n",
    "            self.logger.experiment.log({\"ROC\": figure, \"epoch\": self.current_epoch})\n",
    "            \n",
    "            for i, prediction in enumerate(predictions):\n",
    "                filename = os.path.basename(prediction[\"filename\"][0])\n",
    "                output_one = [post_pred_dice(i) for i in decollate_batch(prediction[\"output\"])]\n",
    "                label_one = [post_label(i) for i in decollate_batch(prediction[\"label\"])]\n",
    "                self.dice_metric(y_pred=output_one, y=label_one)\n",
    "                dice_value_liver, dice_value_injure = self.dice_metric.aggregate()\n",
    "                self.dice_metric.reset()\n",
    "                class_predicted, _, ground_truth = get_classification_info(prediction)\n",
    "                blended = make_gif(prediction, filename=i)\n",
    "                row = [self.current_epoch, filename, wandb.Image(blended), dice_value_liver, dice_value_injure, int(ground_truth[0]), class_predicted]\n",
    "                test_dt.add_data(*row)\n",
    "            \n",
    "            self.logger.experiment.log({f\"SUMMARY_EPOCH_{self.current_epoch}\" : test_dt})\n",
    "\n",
    "        if dice_injure > self.best_val_dice:\n",
    "            self.best_val_dice = dice_injure.item()\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "        print(\n",
    "            f\"current epoch: {self.current_epoch} \"\n",
    "            f\"current liver dice: {dice_liver:.4f}\"\n",
    "            f\"current injure  dice: {dice_injure:.4f}\"\n",
    "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
    "            f\"at epoch: {self.best_val_epoch}\"\n",
    "        )\n",
    "        self.log(\"dice_metric_liver\", dice_liver.item(), prog_bar=True)\n",
    "        self.log(\"dice_metric_injure\", dice_injure.item(), prog_bar=True)\n",
    "        self.log(\"val_loss\", mean_val_loss, prog_bar=True)\n",
    "        return {\"log\": tensorboard_logs}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        print('predicting...')\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3)])\n",
    "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 2\n",
    "        outputs = sliding_window_inference(\n",
    "            images, roi_size, sw_batch_size, self.forward\n",
    "        )\n",
    "        predicition ={\"output\": outputs, \"image\": images, \"label\": labels}\n",
    "        outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
    "\n",
    "        labels = [\n",
    "            post_label(torch.unsqueeze(i, 0)).squeeze() for i in decollate_batch(labels)\n",
    "        ]\n",
    "        dice_metric = self.dice_metric(y_pred=outputs, y=labels)\n",
    "        return {\"prediction\": predicition, \"dice_metric\": dice_metric}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gif Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(prediction, filename):\n",
    "    def _save_gif(volume, filename):\n",
    "        volume = volume.astype(np.float64) / np.max(volume) # normalize the data to 0 - 1\n",
    "        volume = 255 * volume # Now scale by 255\n",
    "        volume = volume.astype(np.uint8)\n",
    "        path_to_gif = os.path.join(\"gifs\", f'{filename}.gif')\n",
    "        if not os.path.exists(\"gifs\"):\n",
    "            print(\"Creating gifs directory\")\n",
    "            os.mkdir(\"gifs\")\n",
    "        imageio.mimsave(path_to_gif, volume)\n",
    "        return path_to_gif\n",
    "    post_pred_blending = Compose([EnsureType(), AsDiscrete(argmax=False),KeepLargestConnectedComponent([1,2], is_onehot=False, independent=False)])\n",
    "    prediction[\"output\"] = [post_pred_blending(i) for i in decollate_batch(prediction[\"output\"])]\n",
    "    selected = {\"output\": prediction[\"output\"][0], \"image\": prediction[\"image\"][0], \"label\": prediction[\"label\"][0]}\n",
    "\n",
    "    selected = Resized(keys=[\"image\", \"label\"], spatial_size=(160, 160, 160))(selected)\n",
    "    selected = Resized(keys=[\"output\"], spatial_size=(160, 160, 160))(selected)\n",
    "\n",
    "    selected = {\"output\": selected[\"output\"].unsqueeze(0), \"image\": selected[\"image\"].unsqueeze(0), \"label\": selected[\"label\"].unsqueeze(0)}\n",
    "\n",
    "    # print('true label:', selected['label'].shape)\n",
    "    pred = torch.argmax(selected['output'], dim=1).detach().cpu().numpy()\n",
    "    true_label = selected['label'][0].detach().cpu().numpy()\n",
    "    image = selected['image'][0].cpu().numpy()\n",
    "    # print('true label:', true_label.shape)\n",
    "    \n",
    "    blended_true_label = blend_images(image, true_label, alpha=0.7)\n",
    "    blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
    "\n",
    "    blended_prediction = blend_images(image, pred, alpha=0.7)\n",
    "    blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
    "\n",
    "    volume_pred = blended_final_prediction[:,:,:,:]\n",
    "    volume_label = blended_final_true_label[:,:,:,:]\n",
    "    volume_pred = np.squeeze(volume_pred).permute(3,0,1,2).cpu()\n",
    "    volume_label = np.squeeze(volume_label).permute(3,0,1,2).cpu()\n",
    "    volume_img = torch.tensor(image).permute(3,1,2,0).repeat(1,1,1,3).cpu()\n",
    "\n",
    "    volume = torch.hstack((volume_img, volume_pred, volume_label))\n",
    "\n",
    "    volume_path = _save_gif(volume.numpy(), f\"blended-{filename}\")\n",
    "       \n",
    "    \n",
    "    return volume_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_info(prediction):\n",
    "    post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "\n",
    "    ground_truth = [\n",
    "            1 if (post_label(i)[2,:,:,:].cpu() == 1).any() else 0 for i in decollate_batch(prediction['label'])\n",
    "        ]\n",
    "\n",
    "    test = prediction['output'].cpu()\n",
    "    prediction_1 = torch.argmax(test, dim=1)\n",
    "\n",
    "    class_2_mask = (prediction_1 == 2).cpu()\n",
    "    if class_2_mask.any():\n",
    "        prediction = torch.max(test[:,2,:,:,:]).item()\n",
    "    else:\n",
    "        prediction = np.max(np.ma.masked_array(test[:,2,:,:,:], mask=class_2_mask))\n",
    "    \n",
    "    unique_values = torch.unique(prediction_1)\n",
    "    predicted_class = 1 if 2 in unique_values else 0\n",
    "    \n",
    "    return predicted_class, prediction, ground_truth\n",
    "\n",
    "def computeROC(predictions):\n",
    "    from sklearn.metrics import roc_curve, auc # roc curve tools\n",
    "    \n",
    "    g_truths = []\n",
    "    preds = []\n",
    "    for prediction in predictions:\n",
    "        _, predict, ground_truth = get_classification_info(prediction)\n",
    "        g_truths.extend(ground_truth)\n",
    "        preds.append(predict)\n",
    "\n",
    "    preds = np.asarray(preds)\n",
    "    ground_truth = np.asarray(g_truths)\n",
    "    fpr, tpr, _ = roc_curve(g_truths, preds)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhnD1E-uGr8c"
   },
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 0\n",
    "IMG_SIZE = (96,96,96)\n",
    "VAL_SIZE = (256,256,256)\n",
    "SAVE_PATH = \"lightning_logs/\"\n",
    "run_idx = len(os.listdir(\"wandb\"))\n",
    "RUN_NAME = f\"Predict_Segmentation_{run_idx+1}\"\n",
    "pytorch_lightning.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmar = {\n",
    "    RemoteMMARKeys.ID: \"clara_pt_liver_and_tumor_ct_segmentation_1\",\n",
    "    RemoteMMARKeys.NAME: \"clara_pt_liver_and_tumor_ct_segmentation\",\n",
    "    RemoteMMARKeys.FILE_TYPE: \"zip\",\n",
    "    RemoteMMARKeys.HASH_TYPE: \"md5\",\n",
    "    RemoteMMARKeys.HASH_VAL: None,\n",
    "    RemoteMMARKeys.MODEL_FILE: os.path.join(\"models\", \"model.pt\"),\n",
    "    RemoteMMARKeys.CONFIG_FILE: os.path.join(\"config\", \"config_train.json\"),\n",
    "    RemoteMMARKeys.VERSION: 1,\n",
    "}\n",
    "\n",
    "def save_checkpoint(state, name):\n",
    "    file_path = \"checkpoints/\"\n",
    "    if not os.path.exists(file_path): \n",
    "        os.makedirs(file_path)\n",
    "    epoch = state[\"epoch\"]\n",
    "    save_dir = file_path + name + str(epoch)\n",
    "    torch.save(state, save_dir)\n",
    "    print(f\"Saving checkpoint for epoch {epoch} in: {save_dir}\")\n",
    "\n",
    "def save_state_dict(state, name):\n",
    "    file_path = \"checkpoints/\"\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    save_dir = file_path + f\"{name}_best\"\n",
    "    torch.save(state, save_dir)\n",
    "    print(f\"Best accuracy so far. Saving model to:{save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_and_print:\n",
    "    def __init__(self, run_name, tb_logger=None):\n",
    "        self.tb_logger = tb_logger\n",
    "        self.run_name = run_name\n",
    "        self.str_log = \"run_name\" + \"\\n  \\n\"\n",
    "\n",
    "    def lnp(self, tag):\n",
    "        print(self.run_name, time.asctime(), tag)\n",
    "        self.str_log += str(time.asctime()) + \" \" + str(tag) + \"  \\n\"\n",
    "\n",
    "    def dump_to_tensorboard(self):\n",
    "        if not self.tb_logger:\n",
    "            print(\"No tensorboard logger\")\n",
    "        self.tb_logger.experiment.add_text(\"log\", self.str_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wandb.finish()\n",
    "except:\n",
    "    print(\"Wandb not initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Segmentation_228 Mon Jun  6 17:28:57 2022 Loggers start\n",
      "Predict_Segmentation_228 Mon Jun  6 17:28:57 2022 ts_script: 1654529337.024067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlalvarez\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.17 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/wandb/run-20220606_172858-38x0fneh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/lalvarez/traumaIA/runs/38x0fneh\" target=\"_blank\">Predict_Segmentation_228</a></strong> to <a href=\"https://wandb.ai/lalvarez/traumaIA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lnp = Log_and_print(RUN_NAME)\n",
    "lnp.lnp(\"Loggers start\")\n",
    "lnp.lnp(\"ts_script: \" + str(time.time()))\n",
    "\n",
    "wandb_logger = pytorch_lightning.loggers.WandbLogger(\n",
    "    project=\"traumaIA\",\n",
    "    name=RUN_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALLBACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Segmentation_228 Mon Jun  6 17:29:04 2022 MAIN callbacks\n",
      "Predict_Segmentation_228 Mon Jun  6 17:29:04 2022 checkpoint_dirpath: lightning_logs/checkpoints/\n",
      "Predict_Segmentation_228 Mon Jun  6 17:29:04 2022 checkpoint_filename: lightning_logs_Predict_Segmentation_228_Best\n",
      "Predict_Segmentation_228 Mon Jun  6 17:29:04 2022 checkpoint_dirpath: lightning_logs/checkpoints/\n",
      "Predict_Segmentation_228 Mon Jun  6 17:29:04 2022 checkpoint_filename: lightning_logs_Predict_Segmentation_228_Last\n"
     ]
    }
   ],
   "source": [
    "lnp.lnp(\"MAIN callbacks\")\n",
    "l_callbacks = []\n",
    "cbEarlyStopping = pytorch_lightning.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=500, mode=\"max\"\n",
    ")\n",
    "l_callbacks.append(cbEarlyStopping)\n",
    "\n",
    "\n",
    "checkpoint_dirpath = SAVE_PATH + \"checkpoints/\"\n",
    "checkpoint_filename = SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"_Best\"\n",
    "lnp.lnp(\"checkpoint_dirpath: \" + checkpoint_dirpath)\n",
    "lnp.lnp(\"checkpoint_filename: \" + checkpoint_filename)\n",
    "cbModelCheckpoint = pytorch_lightning.callbacks.ModelCheckpoint(\n",
    "    monitor=\"dice_metric_injure\", mode=\"max\", dirpath=checkpoint_dirpath, filename=checkpoint_filename, \n",
    ")\n",
    "l_callbacks.append(cbModelCheckpoint)\n",
    "\n",
    "\n",
    "checkpoint_dirpath = SAVE_PATH + \"checkpoints/\"\n",
    "checkpoint_filename = SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"_Last\"\n",
    "lnp.lnp(\"checkpoint_dirpath: \" + checkpoint_dirpath)\n",
    "lnp.lnp(\"checkpoint_filename: \" + checkpoint_filename)\n",
    "cbModelCheckpointLast = pytorch_lightning.callbacks.ModelCheckpoint(\n",
    "   every_n_epochs = 1, dirpath=checkpoint_dirpath, filename=checkpoint_filename, \n",
    ")\n",
    "l_callbacks.append(cbModelCheckpointLast)\n",
    "\n",
    "l_callbacks.append(PrintTableMetricsCallback())\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "l_callbacks.append(lr_monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Segmentation_228 Mon Jun  6 17:29:04 2022  Start Trainining process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation files [{'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_000_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_000_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_001_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_001_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_002_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_002_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_003_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_003_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_004_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_004_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_005_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_005_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_006_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_006_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_007_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_007_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_008_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_008_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_009_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_009_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_010_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_010_0000.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/imagesTs/TRMLIV_011_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/labelsTs/TRMLIV_011_0000.nii.gz'}]\n",
      "len(train_files) 42\n",
      "len(validation files) 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 42/42 [00:00<00:00, 787.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing lmdb file: /mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/monai_cache.lmdb.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 12/12 [00:00<00:00, 301.25it/s]\n",
      "Checkpoint directory /mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/lightning_logs/checkpoints exists and is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing lmdb file: /mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task501_LiverTrauma/monai_cache.lmdb.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Found unsupported keys in the optimizer configuration: {'interval'}\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | _model        | UNet          | 4.8 M \n",
      "1 | loss_function | DiceFocalLoss | 0     \n",
      "------------------------------------------------\n",
      "4.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 M     Total params\n",
      "19.240    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b3f71e55d14d96b4606abbe9209b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd932eac6c0742d4b7bfb88f02f1a205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr-AdamW</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr-AdamW</td><td>0.001</td></tr><tr><td>trainer/global_step</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">Predict_Segmentation_228</strong>: <a href=\"https://wandb.ai/lalvarez/traumaIA/runs/38x0fneh\" target=\"_blank\">https://wandb.ai/lalvarez/traumaIA/runs/38x0fneh</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220606_172858-38x0fneh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialise the LightningModule\n",
    "lnp.lnp(\" Start Trainining process...\")\n",
    "net = Net(train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=3).load_from_checkpoint(\"lightning_logs/checkpoints/lightning_logs_Predict_Segmentation_222_Last.ckpt\", train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=3)\n",
    "wandb_logger.watch(net)\n",
    "\n",
    "# set up loggers and checkpoints\n",
    "log_dir = os.path.join(root_dir, \"logs\")\n",
    "\n",
    "# initialise Lightning's trainer.\n",
    "trainer = pytorch_lightning.Trainer(\n",
    "    default_root_dir=\"lightning_logs/checkpoints\",\n",
    "    gpus=[0],\n",
    "    max_epochs=1000,\n",
    "    auto_lr_find=False,\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=l_callbacks,\n",
    ")\n",
    "\n",
    "# train\n",
    "result_pred2 = trainer.fit(net)\n",
    "wandb.alert(\n",
    "    title=\"Train finished\", \n",
    "    text=\"The train has finished\"\n",
    ")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=3).load_from_checkpoint(\"lightning_logs/checkpoints/lightning_logs_Predict_Segmentation_168_Best.ckpt\", train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pytorch_lightning.Trainer(\n",
    "    default_root_dir=\"lightning_logs/checkpoints\",\n",
    "    gpus=[0],\n",
    "    max_epochs=1000,\n",
    "    auto_lr_find=False,\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=l_callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True),KeepLargestConnectedComponent([1,2], is_onehot=False, independent=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_one = [post_pred(i) for i in decollate_batch(predictions[1][\"prediction\"][\"output\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_gif(volume, filename):\n",
    "    volume = volume.astype(np.float64) / np.max(volume) # normalize the data to 0 - 1\n",
    "    volume = 255 * volume # Now scale by 255\n",
    "    volume = volume.astype(np.uint8)\n",
    "    path_to_gif = os.path.join(\"gifs\", f'{filename}.gif')\n",
    "    if not os.path.exists(\"gifs\"):\n",
    "        print(\"Creating gifs directory\")\n",
    "        os.mkdir(\"gifs\")\n",
    "    imageio.mimsave(path_to_gif, volume)\n",
    "    return path_to_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_save_gif(output_one[0].permute(3,1,2,0).cpu().numpy(), \"test-after\")\n",
    "_save_gif(predictions[1][\"prediction\"][\"output\"][0].permute(3,1,2,0).cpu().numpy(), \"test-before\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests Gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictions[1][\"prediction\"]\n",
    "selected = {\"output\":  output_one[0], \"image\": prediction[\"image\"][0], \"label\": prediction[\"label\"][0]}\n",
    "selected = Resized(keys=[\"image\", \"label\"], spatial_size=(160, 160, 160))(selected)\n",
    "selected = Resized(keys=[\"output\"], spatial_size=(160, 160, 160))(selected)\n",
    "selected = {\"output\": selected[\"output\"].unsqueeze(0), \"image\": selected[\"image\"].unsqueeze(0), \"label\": selected[\"label\"].unsqueeze(0)}\n",
    "# print('true label:', selected['label'].shape)\n",
    "pred = selected[\"output\"][0].detach().cpu().numpy()\n",
    "true_label = selected['label'][0].detach().cpu().numpy()\n",
    "image = selected['image'][0].cpu().numpy()\n",
    "# print('true label:', true_label.shape)\n",
    "\n",
    "blended_true_label = blend_images(image, true_label, alpha=0.3)\n",
    "blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
    "\n",
    "blended_prediction = blend_images(image, pred, alpha=0.3)\n",
    "blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
    "\n",
    "volume_pred = blended_final_prediction[:,:,:,:]\n",
    "volume_label = blended_final_true_label[:,:,:,:]\n",
    "volume_pred = np.squeeze(volume_pred).permute(3,0,1,2).cpu()\n",
    "volume_label = np.squeeze(volume_label).permute(3,0,1,2).cpu()\n",
    "volume_img = torch.tensor(image).permute(3,1,2,0).repeat(1,1,1,3).cpu()\n",
    "\n",
    "volume = torch.hstack((volume_img, volume_pred, volume_label))\n",
    "\n",
    "volume_path = _save_gif(volume.numpy(), f\"blended-test-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictions[1][\"prediction\"]\n",
    "\n",
    "selected = {\"output\": prediction[\"output\"][0], \"image\": prediction[\"image\"][0], \"label\": prediction[\"label\"][0]}\n",
    "selected = Resized(keys=[\"image\", \"label\"], spatial_size=(160, 160, 160))(selected)\n",
    "selected = Resized(keys=[\"output\"], spatial_size=(160, 160, 160))(selected)\n",
    "selected = {\"output\": torch.argmax(selected[\"output\"], 0).unsqueeze(0), \"image\": selected[\"image\"].unsqueeze(0), \"label\": selected[\"label\"].unsqueeze(0)}\n",
    "\n",
    "pred = selected[\"output\"].detach().cpu().numpy()\n",
    "true_label = selected['label'][0].detach().cpu().numpy()\n",
    "image = selected['image'][0].cpu().numpy()\n",
    "\n",
    "blended_true_label = blend_images(image, true_label, alpha=0.3)\n",
    "blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
    "\n",
    "blended_prediction = blend_images(image, pred, alpha=0.3)\n",
    "blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
    "\n",
    "volume_pred = blended_final_prediction[:,:,:,:]\n",
    "volume_label = blended_final_true_label[:,:,:,:]\n",
    "volume_pred = np.squeeze(volume_pred).permute(3,0,1,2).cpu()\n",
    "volume_label = np.squeeze(volume_label).permute(3,0,1,2).cpu()\n",
    "volume_img = torch.tensor(image).permute(3,1,2,0).repeat(1,1,1,3).cpu()\n",
    "\n",
    "volume = torch.hstack((volume_img, volume_pred, volume_label))\n",
    "\n",
    "volume_path = _save_gif(volume.numpy(), f\"blended-test-1-org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"_Last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"Last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spleen_segmentation_3d_lightning.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "16d90db93701a4e14595aea1a6791a3dd0d33758ed6b394279d759beaff9b73f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
