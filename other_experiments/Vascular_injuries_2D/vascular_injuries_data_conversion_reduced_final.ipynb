{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    ToTensord,\n",
    "    SaveImaged,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    AsChannelLastd,\n",
    "    AsChannelFirstd,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityRanged,\n",
    "    FillHolesd,\n",
    "    RandCropByLabelClassesd,\n",
    "    Resized, RandFlipd, RandRotate90d,\n",
    "    CropForeground,\n",
    ")\n",
    "from monai.transforms.transform import MapTransform\n",
    "from monai.transforms.inverse import InvertibleTransform\n",
    "from monai.config import DtypeLike, KeysCollection\n",
    "from monai.config.type_definitions import NdarrayOrTensor\n",
    "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "from monai.transforms.intensity.array import (\n",
    "    ScaleIntensityRangePercentiles,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets.widgets import * \n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import glob \n",
    "import torch\n",
    "import os\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveDicts(MapTransform, InvertibleTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            self.push_transform(d, key)\n",
    "        # print(d[\"image_meta_dict\"][\"filename_or_obj\"])\n",
    "        a = {\"image\": d[\"image\"], \"label\": d[\"label\"], \"path\": d[\"image_meta_dict\"][\"filename_or_obj\"]}\n",
    "        # print(a[\"path\"])\n",
    "        d = a\n",
    "        return d\n",
    "\n",
    "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = d[key]\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNUnetScaleIntensity(MapTransform):\n",
    "    \"\"\"\n",
    "    Dictionary-based wrapper of :py:class:`monai.transforms.ScaleIntensityRange`.\n",
    "\n",
    "    Args:\n",
    "        keys: keys of the corresponding items to be transformed.\n",
    "            See also: monai.transforms.MapTransform\n",
    "        a_min: intensity original range min.\n",
    "        a_max: intensity original range max.\n",
    "        b_min: intensity target range min.\n",
    "        b_max: intensity target range max.\n",
    "        clip: whether to perform clip after scaling.\n",
    "        dtype: output data type, if None, same as input image. defaults to float32.\n",
    "        allow_missing_keys: don't raise exception if key is missing.\n",
    "    \"\"\"\n",
    "    def _compute_stats(self, volume, mask):\n",
    "        volume = volume.copy()\n",
    "        mask = np.greater(mask, 0) # get only non-zero positive pixels/labels\n",
    "        volume = volume * mask\n",
    "        volume = np.ma.masked_equal(volume,0).compressed()\n",
    "        if len(volume) == 0:\n",
    "            return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "        median = np.median(volume)\n",
    "        mean = np.mean(volume)\n",
    "        std = np.std(volume)\n",
    "        mn = np.min(volume)\n",
    "        mx = np.max(volume)\n",
    "        percentile_99_5 = np.percentile(volume, 99.5)\n",
    "        percentile_00_5 = np.percentile(volume, 00.5)\n",
    "        print(median, mean, std, mn, mx, percentile_99_5, percentile_00_5)\n",
    "        return median, mean, std, mn, mx, percentile_99_5, percentile_00_5\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        dtype: DtypeLike = np.float32,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            stats = self._compute_stats(d[key], d['label'])\n",
    "            d[key] = np.clip(d[key], stats[6], stats[5])\n",
    "            d[key] = (d[key] - stats[1]) / stats[2]\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCropForegroundd(MapTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        source_key: str,\n",
    "        allow_missing_keys: bool = False,\n",
    "        **np_kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.source_key = source_key\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        original_label = d['label']\n",
    "        unique_labels = np.unique(original_label) \n",
    "        # Check if there is one VI or two\n",
    "        if len(unique_labels > 4) == 1:\n",
    "            d = CropForegroundd(keys=self.keys, source_key=self.source_key)(d) # crop ROI\n",
    "        else:\n",
    "            spleen_labels = np.where((original_label%2 != 0), 0, original_label)\n",
    "            vi_spleen_labels = np.where((original_label != 6), 0, original_label)\n",
    "            liver_labels = np.where((original_label%2 == 0), 0, original_label)\n",
    "            vi_liver_labels = np.where((original_label != 5), 0, original_label)\n",
    "            new_d = {'image': d['image'], 'label_spleen': spleen_labels, 'label_liver': liver_labels, 'label_vi_spleen': vi_spleen_labels, 'label_vi_liver': vi_liver_labels}\n",
    "            spleen_d = CropForegroundd(keys=['image', 'label_spleen'], source_key=['label_spleen'])(new_d).pop('label_liver') # crop ROI\n",
    "            start, end = spleen_d[\"boundg_box\"]\n",
    "            box_start_spleen, box_end_spleen = CropForeground().compute_bounding_box(img=new_d['label_vi_spleen'])\n",
    "            box_start_liver, box_end_liver = CropForeground().compute_bounding_box(img=new_d['label_vi_liver'])\n",
    "            # spleen_d[\"label\"][s:e, :,:]\n",
    "            # debug to check if the bounding box is correct dimension wise, should have 3 dimensions\n",
    "            \n",
    "\n",
    "            liver_d = CropForegroundd(keys=['image', 'label_liver'], source_key=['label_liver'])(new_d).pop('label_spleen') # crop ROI\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class WriteToPNG(MapTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        output_dir: str,\n",
    "        mode:str,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.output_dir = output_dir\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            if isinstance(d[key], torch.Tensor):\n",
    "                d[key] = d[key].detach().cpu().numpy()\n",
    "            for slice in range(d[key].shape[-1]):\n",
    "                # print(\"type: \" + d[key].dtype)\n",
    "                filename = os.path.basename(d[\"image_meta_dict\"][\"filename_or_obj\"]).split(\".\")[0] + f\"_{slice}.png\"\n",
    "                if key == \"image\":\n",
    "                    if self.mode == \"train\":\n",
    "                        save_dir = os.path.join(self.output_dir, 'imagesTr', filename)\n",
    "                    else:\n",
    "                        save_dir = os.path.join(self.output_dir, 'imagesTs', filename)\n",
    "                else:\n",
    "                    if self.mode == \"train\":\n",
    "                        save_dir = os.path.join(self.output_dir, 'labelsTr', filename)\n",
    "                    else:\n",
    "                        save_dir = os.path.join(self.output_dir, 'labelsTs', filename)\n",
    "                if not os.path.exists(os.path.dirname(save_dir)):\n",
    "                    print(f\"Creating directory: {os.path.dirname(save_dir)}\")\n",
    "                    os.makedirs(os.path.dirname(save_dir))\n",
    "                print(f\"Saving to {save_dir}\")\n",
    "                plt.imsave(save_dir, d[key][0, :, :, slice], cmap=\"gray\")\n",
    "                # img = Image.fromarray(d[key][0, :, :, slice].astype(np.float32))\n",
    "                # img.save(save_dir)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = sorted( glob.glob( os.path.join( \"/mnt/chansey/lauraalvarez/\",\"data\", \"vascular_injuries\", \"nii\", \"imagesTr\", \"*.nii.gz\") ) )\n",
    "train_labels = sorted( glob.glob( os.path.join( \"/mnt/chansey/lauraalvarez/\",\"data\", \"vascular_injuries\", \"nii\", \"labelsTr\", \"*.nii.gz\") ) )\n",
    "data_dicts = [ {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels) ]\n",
    "test_images = sorted( glob.glob( os.path.join( \"/mnt/chansey/lauraalvarez/\",\"data\", \"vascular_injuries\", \"nii\", \"imagesTs\", \"*.nii.gz\" ) ) )\n",
    "test_labels = sorted( glob.glob( os.path.join( \"/mnt/chansey/lauraalvarez/\",\"data\", \"vascular_injuries\", \"nii\", \"labelsTs\", \"*.nii.gz\") ) )\n",
    "data_dicts_test = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(test_images, test_labels) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 error cases\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# transforms_bsl = Compose([ LoadImaged(keys=[\"image\", \"label\"]), AddChanneld(keys=[\"image\"]), ToTensord(keys=[\"image\", \"label\"]),])\n",
    "transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # RemoveDicts(keys=[\"image\", \"label\"]),\n",
    "        AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        # AsChannelFirstd(keys=[\"label\"]),\n",
    "        # AsDiscreted(keys=[\"label\"], argmax=True),\n",
    "        # Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        # Spacingd( keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 1), mode=(\"bilinear\", \"nearest\"),),\n",
    "        # CropForegroundd(keys=[\"image\", \"label\"], source_key=\"label\"),\n",
    "        CustomCropForegroundd(keys=[\"image\", \"label\"], source_key=\"label\"),\n",
    "        NNUnetScaleIntensity(keys=[\"image\"]),\n",
    "        # ClosePreprocessing(keys=[\"label\"]),\n",
    "        WriteToPNG(keys=[\"image\", \"label\"], output_dir=\"/mnt/chansey/lauraalvarez/data/vascular_injuries/png/\", mode=\"test\"),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# injure_org = transforms_bsl(data_dicts)\n",
    "error_cases = list()\n",
    "for data_dict in data_dicts_test:\n",
    "    try:\n",
    "        data_dict = transforms(data_dict)\n",
    "    except Exception as e:\n",
    "            error_cases.append(data_dict)\n",
    "print(f\"{len(error_cases)} error cases\")\n",
    "print(error_cases)\n",
    "# injure_crop = transforms(data_dicts)\n",
    "# print(injure_crop[\"image\"].shape, injure_crop[\"label\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'injure_crop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mu:\\lauraalvarez\\traumaAI\\Liver_Segmentation\\vascular_injuries_data_conversion.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular_injuries_data_conversion.ipynb#ch0000012?line=0'>1</a>\u001b[0m blended_true_label \u001b[39m=\u001b[39m blend_images(injure_crop[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m], injure_crop[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m], alpha\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular_injuries_data_conversion.ipynb#ch0000012?line=1'>2</a>\u001b[0m blended_final_true_label_closed \u001b[39m=\u001b[39m blended_true_label\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular_injuries_data_conversion.ipynb#ch0000012?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(blended_final_true_label_closed\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'injure_crop' is not defined"
     ]
    }
   ],
   "source": [
    "blended_true_label = blend_images(injure_crop[\"image\"], injure_crop[\"label\"], alpha=0.9)\n",
    "blended_final_true_label_closed = blended_true_label.permute(1,2,0,3)\n",
    "print(blended_final_true_label_closed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3217b50e700449f985463f429c52265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=88, description='slice', max=177), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.dicom_animation(slice)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.visualize import matshow3d, blend_images\n",
    "import torch \n",
    "\n",
    "def dicom_animation(slice):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.title(f\"liver no injured \")\n",
    "    plt.imshow(blended_final_true_label_closed[:, :, :, slice], cmap=\"bone\")\n",
    "    plt.show()\n",
    "\n",
    "interact(dicom_animation, slice=(0, blended_final_true_label_closed.shape[-1]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a270b461b7f747bfa62901acd69aa588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=88, description='slice', max=177), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.dicom_animation(slice)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.visualize import matshow3d, blend_images\n",
    "import torch \n",
    "\n",
    "def dicom_animation(slice):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.title(f\"liver no injured \")\n",
    "    plt.imshow(blended_final_true_label[:, :, :, slice], cmap=\"bone\")\n",
    "    plt.show()\n",
    "\n",
    "interact(dicom_animation, slice=(0, blended_final_true_label.shape[-1]-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env_trauma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f756ff16342a8157e6f46b879d688029dcc5bc6cd621b2b84934bbdd850a743a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
