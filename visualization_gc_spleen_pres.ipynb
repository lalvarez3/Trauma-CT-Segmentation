{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    ToTensord,\n",
    "    Resized,\n",
    "    AsChannelLastd,\n",
    "    AsChannelFirstd,\n",
    "    AsDiscrete,\n",
    "    CropForeground,\n",
    "    SpatialCropd,\n",
    "    AsDiscreted,\n",
    ")\n",
    "from monai.transforms.transform import MapTransform\n",
    "from monai.transforms.inverse import InvertibleTransform\n",
    "from monai.data import decollate_batch\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from monai.config import DtypeLike, KeysCollection\n",
    "from monai.config.type_definitions import NdarrayOrTensor\n",
    "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "\n",
    "from monai.visualize import matshow3d, blend_images\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveDicts(MapTransform, InvertibleTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            self.push_transform(d, key)\n",
    "\n",
    "        a = {}\n",
    "        for key in self.key_iterator(d):\n",
    "            a[key] = d[key]\n",
    "            if key == \"image\":\n",
    "                a[\"path\"] = d[\"image_meta_dict\"][\"filename_or_obj\"]\n",
    "        if a.get(\"path\", None) is None:\n",
    "            a[\"path\"] = d[\"label_meta_dict\"][\"filename_or_obj\"]\n",
    "\n",
    "        # a = {\"image\": d[\"image\"], \"label\": d[\"label\"], \"label-gc\":  d[\"label-gc\"] , \"path\": d[\"image_meta_dict\"][\"filename_or_obj\"]}\n",
    "\n",
    "        # print(a[\"path\"])\n",
    "        d = a\n",
    "        return d\n",
    "\n",
    "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = d[key]\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms.intensity.array import ScaleIntensityRangePercentiles\n",
    "\n",
    "\n",
    "class ScaleIntensityRangePercentilesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Dictionary-based wrapper of :py:class:`monai.transforms.ScaleIntensityRangePercentiles`.\n",
    "\n",
    "    Args:\n",
    "        keys: keys of the corresponding items to be transformed.\n",
    "            See also: monai.transforms.MapTransform\n",
    "        lower: lower percentile.\n",
    "        upper: upper percentile.\n",
    "        b_min: intensity target range min.\n",
    "        b_max: intensity target range max.\n",
    "        clip: whether to perform clip after scaling.\n",
    "        relative: whether to scale to the corresponding percentiles of [b_min, b_max]\n",
    "        channel_wise: if True, compute intensity percentile and normalize every channel separately.\n",
    "            default to False.\n",
    "        dtype: output data type, if None, same as input image. defaults to float32.\n",
    "        allow_missing_keys: don't raise exception if key is missing.\n",
    "    \"\"\"\n",
    "\n",
    "    backend = ScaleIntensityRangePercentiles.backend\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        lower: float,\n",
    "        upper: float,\n",
    "        b_min: Optional[float],\n",
    "        b_max: Optional[float],\n",
    "        clip: bool = False,\n",
    "        relative: bool = False,\n",
    "        channel_wise: bool = False,\n",
    "        dtype: DtypeLike = np.float32,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.scaler = ScaleIntensityRangePercentiles(\n",
    "            lower, upper, b_min, b_max, clip, relative, channel_wise, dtype\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = self.scaler(d[key])\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNUnetScaleIntensity(MapTransform):\n",
    "    \"\"\"\n",
    "    Dictionary-based wrapper of :py:class:`monai.transforms.ScaleIntensityRange`.\n",
    "\n",
    "    Args:\n",
    "        keys: keys of the corresponding items to be transformed.\n",
    "            See also: monai.transforms.MapTransform\n",
    "        a_min: intensity original range min.\n",
    "        a_max: intensity original range max.\n",
    "        b_min: intensity target range min.\n",
    "        b_max: intensity target range max.\n",
    "        clip: whether to perform clip after scaling.\n",
    "        dtype: output data type, if None, same as input image. defaults to float32.\n",
    "        allow_missing_keys: don't raise exception if key is missing.\n",
    "    \"\"\"\n",
    "\n",
    "    def _compute_stats(self, volume, mask):\n",
    "        volume = np.ma.masked_equal(volume.copy().astype(np.int16) * np.greater(mask, 0), 0).compressed()\n",
    "        median = np.median(volume)\n",
    "        mean = np.mean(volume)\n",
    "        std = np.std(volume)\n",
    "        mn = np.min(volume)\n",
    "        mx = np.max(volume)\n",
    "        percentile_99_5 = np.percentile(volume, 99.5)\n",
    "        percentile_00_5 = np.percentile(volume, 00.5)\n",
    "        print(median, mean, std, mn, mx, percentile_99_5, percentile_00_5)\n",
    "        return median, mean, std, mn, mx, percentile_99_5, percentile_00_5\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        dtype: DtypeLike = np.float32,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            stats = self._compute_stats(d[key], d[\"label\"])\n",
    "            d[key] = np.clip(d[key], stats[6], stats[5])\n",
    "            d[key] = (d[key] - stats[1]) / stats[2]\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriteToMHA(MapTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        output_dir: str,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            if isinstance(d[key], torch.Tensor):\n",
    "                d[key] = d[key].detach().cpu().numpy().astype(np.int8)\n",
    "            original = sitk.ReadImage(d[\"path\"])\n",
    "            filename = os.path.basename(d[\"path\"]).split(\".\")[0] + \".mha\"\n",
    "            save_dir = os.path.join(self.output_dir, filename)\n",
    "            if not os.path.exists(os.path.dirname(save_dir)):\n",
    "                print(f\"Creating directory: {os.path.dirname(save_dir)}\")\n",
    "                os.makedirs(os.path.dirname(save_dir))\n",
    "            print(f\"Saving to {save_dir}\")\n",
    "            img = sitk.GetImageFromArray(d[key])\n",
    "            img.SetSpacing(original.GetSpacing())\n",
    "            img.SetOrigin(original.GetOrigin())\n",
    "            sitk.WriteImage(img, save_dir)\n",
    "\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "class CreateSyntheticLabel(MapTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        threshold: float = 0.5,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        print(d[\"label\"].shape)\n",
    "        old_mask = d[\"label\"][3, :, :, :].unsqueeze(0).numpy().copy()\n",
    "        new_mask = np.zeros_like(old_mask)\n",
    "        new_img = d[\"image\"][:, :, :, :].numpy().copy()\n",
    "        idx_label = np.where(old_mask.flatten() == 1)[0]\n",
    "        idx_img = np.where(new_img.flatten() > self.threshold)[0]\n",
    "        idx_to_change = np.intersect1d(idx_img, idx_label)\n",
    "        np.put(new_mask, idx_to_change, 1)\n",
    "        old_mask -= new_mask\n",
    "\n",
    "        kernel = np.ones((4, 4), np.uint8)\n",
    "        closed_slices = list()\n",
    "        for slice in range(new_mask.shape[-1]):\n",
    "            result = cv2.morphologyEx(\n",
    "                new_mask[0, :, :, slice], cv2.MORPH_CLOSE, kernel, iterations=2\n",
    "            )\n",
    "            result = cv2.medianBlur(result, 3)\n",
    "            closed_slices.append(result)\n",
    "\n",
    "        new_mask = np.stack(closed_slices)\n",
    "\n",
    "        d[\"label\"][1, :, :, :] = torch.Tensor(new_mask).permute(1, 2, 0)\n",
    "        d[\"label\"][3, :, :, :] = torch.Tensor(old_mask)\n",
    "\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGCLabel(MapTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        threshold: float = 0.5,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "\n",
    "        d = dict(data)\n",
    "        print(d[\"label\"].shape, d[\"label-gc\"].shape)\n",
    "        spleen_channel = np.where((d[\"label-gc\"] != 1), 0, d[\"label-gc\"])\n",
    "        spleen_channel = np.where((spleen_channel == 1), 1, spleen_channel)\n",
    "        print(d[\"label\"].shape, d[\"label-gc\"].shape)\n",
    "        d[\"label\"][1, :, :, :] = (\n",
    "            torch.Tensor(spleen_channel).unsqueeze(0) - d[\"label\"][3, :, :, :]\n",
    "        )\n",
    "\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsDiscrete1sd(MapTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "\n",
    "        d = data\n",
    "        d[\"label\"] = d[\"label\"].astype(np.int8)\n",
    "        back = np.expand_dims(np.zeros(d[\"label\"][0, :, :, :].shape, dtype=np.int8), 0)\n",
    "        d[\"label\"] = torch.Tensor(np.append(back, d[\"label\"], axis=0))\n",
    "        print(d[\"label\"].shape)\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveBackgroundd(MapTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "\n",
    "        d = data\n",
    "        print(\"before: \", d[\"label\"].shape)\n",
    "        d[\"label\"] = d[\"label\"][1:, :, :, :]\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets.widgets import *\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "injure_liver = os.path.join(\n",
    "    \"/mnt/chansey/\",\n",
    "    \"lauraalvarez\",\n",
    "    \"nnunet\",\n",
    "    \"nnUNet_raw_data_base\",\n",
    "    \"nnUNet_raw_data\",\n",
    "    \"Task504_LiverTrauma\",\n",
    "    \"imagesTr\",\n",
    "    \"TRMLIV_000_0000.nii.gz\",\n",
    ")\n",
    "injure_liver_label = os.path.join(\n",
    "    \"/mnt/chansey/\",\n",
    "    \"lauraalvarez\",\n",
    "    \"nnunet\",\n",
    "    \"nnUNet_raw_data_base\",\n",
    "    \"nnUNet_raw_data\",\n",
    "    \"Task504_LiverTrauma\",\n",
    "    \"labelsTr\",\n",
    "    \"TRMLIV_000.nii.gz\",\n",
    ")\n",
    "\n",
    "spleen_error_img = os.path.join(\n",
    "    \"U:\\\\\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data\", \"Task504_LiverTrauma\", \"imagesTs\", \"TRMLIV_000_0000.nii.gz\"\n",
    ")\n",
    "spleen_error_msk = os.path.join(  \"U:\\\\\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data\", \"Task504_LiverTrauma\", \"out\", \"TRMLIV_000.nii.gz\")\n",
    "spleen_error_true = os.path.join(\n",
    "    \"U:\\\\\", \"lauraalvarez\", \"nnunet\", \"nnUNet_raw_data\", \"Task504_LiverTrauma\", \"labelsTs\", \"TRMLIV_000.nii.gz\"\n",
    ")\n",
    "\n",
    "# spleen_error_img = os.path.join(\n",
    "#     \"/mnt/chansey/\", \"lauraalvarez\", \"data\", \"liver\", \"train\", \"data\", \"L110086.mha\"\n",
    "# )\n",
    "# spleen_error_msk = os.path.join(  \"/mnt/chansey/\", \"lauraalvarez\", \"data\", \"liver\", \"train\", \"mask\", \"L110086.mha\")\n",
    "# spleen_error_msk = os.path.join(\n",
    "#     \"/mnt/chansey/\",\n",
    "#     \"lauraalvarez\",\n",
    "#     \"data\",\n",
    "#     \"_overlays_from_alessa\",\n",
    "#     \"overlays\",\n",
    "#     \"overlay_B2\",\n",
    "#     \"L110027.mha\",\n",
    "# )\n",
    "# gc_msk = os.path.join(\"/mnt/chansey/\", \"lauraalvarez\", \"data\", \"liver\")\n",
    "\n",
    "selected_img = spleen_error_img\n",
    "selected_msk = spleen_error_msk\n",
    "selected_true = spleen_error_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126.0 125.40829693965654 32.8002518141156 -82 444 218.0 33.0\n",
      "(1, 1265, 512, 512)\n",
      "(1, 1265, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "paths = {\"label\": selected_msk, \"image\": selected_img, \"tLabel\": selected_true}\n",
    "normal_plot = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\",\"label\",\"tLabel\"]),\n",
    "        AsChannelFirstd(keys=[\"image\",\"label\",\"tLabel\"]),\n",
    "        AddChanneld(keys=[\"label\",\"image\", \"tLabel\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\", ], axcodes=\"RAS\"),\n",
    "        NNUnetScaleIntensity(keys=[\"image\"]),\n",
    "        # Resized(keys=[\"image\",\"label\"], spatial_size=(160,160,160)),\n",
    "    ]\n",
    ")\n",
    "# val_transforms_overlays_float_class = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"label\"]),\n",
    "#         RemoveDicts(keys=[\"label\"]),\n",
    "#         AsChannelFirstd(keys=[\"label\"]),\n",
    "#         AsDiscrete1sd(keys=[\"label\"]),\n",
    "#         AsDiscreted(keys=[\"label\"], argmax=True, to_onehot=7),\n",
    "#         RemoveBackgroundd(keys=[\"label\"]),\n",
    "#         WriteToMHA(\n",
    "#             keys=[\"label\"],\n",
    "#             output_dir=\"/mnt/chansey/lauraalvarez/data/spleen/synthetic_overlays\",\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# val_transforms_insert_gc_overlay = Compose( #igual no es asi\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"image\", \"label\", \"label-gc\"]),\n",
    "#         RemoveDicts(keys=[\"image\", \"label\", \"label-gc\"]),\n",
    "#         adaptOverlay(keys=[\"label-gc\"]),\n",
    "#         AsChannelFirstd(keys=[\"label\", \"label-gc\"]),\n",
    "#         AddGCLabel(keys=[\"label\"]),\n",
    "#         WriteToMHA(keys=[\"label\"], output_dir=\"/mnt/chansey/lauraalvarez/data/spleen/synthetic_overlays\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# val_transforms_synthetic_spleen = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"image\", \"label\"]),\n",
    "#         RemoveDicts(keys=[\"image\", \"label\"]),\n",
    "#         AddChanneld(keys=[\"image\"]),\n",
    "#         AsChannelFirstd(keys=[\"label\"]),\n",
    "#         NNUnetScaleIntensity(keys=[\"image\"]),\n",
    "#         ToTensord(keys=[\"image\", \"label\"]),\n",
    "#         CreateSyntheticLabel(keys=[\"label\"], threshold=0.5),\n",
    "#         # Resized(keys=[\"image\", \"label\"], spatial_size=(259, 259, 259))\n",
    "#         WriteToMHA(\n",
    "#             keys=[\"label\"],\n",
    "#             output_dir=\"/mnt/chansey/lauraalvarez/data/spleen/synthetic_overlays\",\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "injures = normal_plot(paths)\n",
    "\n",
    "print(injures[\"label\"].shape)\n",
    "print(injures[\"image\"].shape)\n",
    "# print(np.unique(injures[\"label\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 160, 160, 160)\n",
      "(1, 160, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "inj =  dict(injures)\n",
    "inj =Resized(keys=[\"image\", \"label\", \"tLabel\"], spatial_size=(160, 160, 160))(inj)\n",
    "# injures[\"label\"] = np.expand_dims(injures[\"label\"],0)\n",
    "# injures[\"image\"] = np.expand_dims(injures[\"image\"],0)\n",
    "print(inj[\"label\"].shape)\n",
    "print(inj[\"image\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pred_blending = Compose([AsDiscrete(argmax=True)])\n",
    "injures_one_channel = post_pred_blending(injures[\"label\"])\n",
    "blended_label_in = blend_images(\n",
    "    inj[\"image\"], inj[\"label\"], 0.5\n",
    ")\n",
    "blended_final = torch.from_numpy(blended_label_in).permute(1, 2, 0, 3)\n",
    "\n",
    "blended_true_label = blend_images(inj[\"image\"], inj[\"tLabel\"], 0.5)\n",
    "blended_true_label = torch.from_numpy(blended_true_label).permute(1, 2, 0, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 160, 3, 160])\n",
      "torch.Size([160, 160, 3, 160])\n"
     ]
    }
   ],
   "source": [
    "print(torch.from_numpy(inj[\"image\"]).permute(1,2,0,3).repeat(1,1,3,1).shape)\n",
    "print(blended_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 480, 3, 160])\n",
      "torch.Size([160, 480, 160, 3])\n"
     ]
    }
   ],
   "source": [
    "volume = torch.hstack((torch.from_numpy(inj[\"image\"]).permute(1,2,0,3).repeat(1,1,3,1), blended_final, blended_true_label))\n",
    "print(volume.shape)\n",
    "volume = volume.permute(0,1,3,2)\n",
    "print(volume.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "def _save_gif(volume, filename):\n",
    "    volume = volume.astype(np.float64) / np.max(volume) # normalize the data to 0 - 1\n",
    "    volume = volume *255 # Now scale by 255\n",
    "    volume = volume.astype(np.uint8)\n",
    "    path_to_gif = os.path.join(\"gifs\", f'{filename}.gif')\n",
    "    if not os.path.exists(\"gifs\"):\n",
    "        print(\"Creating gifs directory\")\n",
    "        os.mkdir(\"gifs\")\n",
    "    imageio.mimsave(path_to_gif, volume)\n",
    "    return path_to_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([160, 160, 3, 160])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blended_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_path = _save_gif(blended_final.numpy().transpose(0,1,3,2), f\"blended-test-11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09df06a12bbf4c3a8d7f16e8c32a6a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=79, description='slice', max=159), Output()), _dom_classes=('widget-inteâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.dicom_animation(slice)>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dicom_animation(slice):\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(20, 20))\n",
    "    plt.title(f\"liver no injured \")\n",
    "    axarr[0].imshow(blended_final[:, :, :, slice])\n",
    "    axarr[1].imshow(blended_true_label[:, :, :, slice])\n",
    "    axarr[2].imshow(inj[\"image\"][0, :, :, slice], cmap=\"bone\")\n",
    "    # axarr[2].imshow(new_mask[0, :, :, slice])\n",
    "    # axarr[1].imshow(injures[\"label\"][1, :, :, slice], cmap=\"bone\")\n",
    "\n",
    "\n",
    "interact(dicom_animation, slice=(0, inj[\"label\"].shape[-1] - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select pixels of injury mask\n",
    "injury_mask = injures[\"label\"][3, :, :, :]\n",
    "values = injures[\"image\"][0, :, :, :][injury_mask == 1]\n",
    "histogram, bin_edges = np.histogram(values, bins=256, range=(0, 1))\n",
    "\n",
    "# configure and draw the histogram figure\n",
    "plt.figure()\n",
    "plt.title(\"Grayscale Histogram\")\n",
    "plt.xlabel(\"grayscale value\")\n",
    "plt.ylabel(\"pixel count\")\n",
    "plt.xlim([0.0, 1.0])  # <- named arguments do not work here\n",
    "\n",
    "plt.plot(bin_edges[0:-1], histogram)  # <- or here\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.6025, -1.5511, -1.3500,  ..., -0.4651, -0.6260, -0.7467])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env_trauma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f756ff16342a8157e6f46b879d688029dcc5bc6cd621b2b84934bbdd850a743a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
