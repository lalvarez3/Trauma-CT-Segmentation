{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    ToTensord,\n",
    "    SaveImaged,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    AsChannelLastd,\n",
    "    AsChannelFirstd,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityRanged,\n",
    "    FillHolesd,\n",
    "    RandCropByLabelClassesd,\n",
    "    Resized, RandFlipd, RandRotate90d,\n",
    ")\n",
    "from monai.transforms.transform import MapTransform\n",
    "from monai.transforms.inverse import InvertibleTransform\n",
    "from monai.config import DtypeLike, KeysCollection\n",
    "from monai.config.type_definitions import NdarrayOrTensor\n",
    "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "from monai.transforms.intensity.array import (\n",
    "    ScaleIntensityRangePercentiles,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets.widgets import * \n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import glob \n",
    "import torch\n",
    "import os\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveDicts(MapTransform, InvertibleTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            self.push_transform(d, key)\n",
    "        # print(d[\"image_meta_dict\"][\"filename_or_obj\"])\n",
    "        a = {\"image\": d[\"image\"], \"label\": d[\"label\"], \"path\": d[\"image_meta_dict\"][\"filename_or_obj\"]}\n",
    "        # print(a[\"path\"])\n",
    "        d = a\n",
    "        return d\n",
    "\n",
    "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = d[key]\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleIntensityRangePercentilesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Dictionary-based wrapper of :py:class:`monai.transforms.ScaleIntensityRangePercentiles`.\n",
    "\n",
    "    Args:\n",
    "        keys: keys of the corresponding items to be transformed.\n",
    "            See also: monai.transforms.MapTransform\n",
    "        lower: lower percentile.\n",
    "        upper: upper percentile.\n",
    "        b_min: intensity target range min.\n",
    "        b_max: intensity target range max.\n",
    "        clip: whether to perform clip after scaling.\n",
    "        relative: whether to scale to the corresponding percentiles of [b_min, b_max]\n",
    "        channel_wise: if True, compute intensity percentile and normalize every channel separately.\n",
    "            default to False.\n",
    "        dtype: output data type, if None, same as input image. defaults to float32.\n",
    "        allow_missing_keys: don't raise exception if key is missing.\n",
    "    \"\"\"\n",
    "\n",
    "    backend = ScaleIntensityRangePercentiles.backend\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        lower: float,\n",
    "        upper: float,\n",
    "        b_min: Optional[float],\n",
    "        b_max: Optional[float],\n",
    "        clip: bool = False,\n",
    "        relative: bool = False,\n",
    "        channel_wise: bool = False,\n",
    "        dtype: DtypeLike = np.float32,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.scaler = ScaleIntensityRangePercentiles(lower, upper, b_min, b_max, clip, relative, channel_wise, dtype)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = self.scaler(d[key])\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNUnetScaleIntensity(MapTransform):\n",
    "    \"\"\"\n",
    "    Dictionary-based wrapper of :py:class:`monai.transforms.ScaleIntensityRange`.\n",
    "\n",
    "    Args:\n",
    "        keys: keys of the corresponding items to be transformed.\n",
    "            See also: monai.transforms.MapTransform\n",
    "        a_min: intensity original range min.\n",
    "        a_max: intensity original range max.\n",
    "        b_min: intensity target range min.\n",
    "        b_max: intensity target range max.\n",
    "        clip: whether to perform clip after scaling.\n",
    "        dtype: output data type, if None, same as input image. defaults to float32.\n",
    "        allow_missing_keys: don't raise exception if key is missing.\n",
    "    \"\"\"\n",
    "    def _compute_stats(self, volume, mask):\n",
    "        volume = volume.copy()\n",
    "        mask = np.greater(mask, 0) # get only non-zero positive pixels/labels\n",
    "        volume = volume * mask\n",
    "        volume = np.ma.masked_equal(volume,0).compressed()\n",
    "        if len(volume) == 0:\n",
    "            return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "        median = np.median(volume)\n",
    "        mean = np.mean(volume)\n",
    "        std = np.std(volume)\n",
    "        mn = np.min(volume)\n",
    "        mx = np.max(volume)\n",
    "        percentile_99_5 = np.percentile(volume, 99.5)\n",
    "        percentile_00_5 = np.percentile(volume, 00.5)\n",
    "        print(median, mean, std, mn, mx, percentile_99_5, percentile_00_5)\n",
    "        return median, mean, std, mn, mx, percentile_99_5, percentile_00_5\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        dtype: DtypeLike = np.float32,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            stats = self._compute_stats(d[key], d['label'])\n",
    "            d[key] = np.clip(d[key], stats[6], stats[5])\n",
    "            d[key] = (d[key] - stats[1]) / stats[2]\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClosePreprocessing(MapTransform):\n",
    "    \"\"\"\n",
    "    Dictionary-based wrapper of :py:class:`monai.transforms.ScaleIntensityRange`.\n",
    "\n",
    "    Args:\n",
    "        keys: keys of the corresponding items to be transformed.\n",
    "            See also: monai.transforms.MapTransform\n",
    "        a_min: intensity original range min.\n",
    "        a_max: intensity original range max.\n",
    "        b_min: intensity target range min.\n",
    "        b_max: intensity target range max.\n",
    "        clip: whether to perform clip after scaling.\n",
    "        dtype: output data type, if None, same as input image. defaults to float32.\n",
    "        allow_missing_keys: don't raise exception if key is missing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        kernel_size: int = 10,\n",
    "        dtype: DtypeLike = np.float32,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.kernel = np.ones((kernel_size,kernel_size),np.uint8)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        closed_slices = list()\n",
    "        for slice in range(d[\"label\"].shape[-1]):\n",
    "            result = cv2.morphologyEx(d[\"label\"][0, :, :, slice], cv2.MORPH_CLOSE, self.kernel)\n",
    "            closed_slices.append(result)\n",
    "\n",
    "        d[\"label\"] = torch.Tensor(np.stack(closed_slices)).permute(1, 2, 0).unsqueeze(0)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class WriteToPNG(MapTransform):\n",
    "    \"\"\"\n",
    "    Dictionary-based wrapper of :py:class:`monai.transforms.ScaleIntensityRange`.\n",
    "\n",
    "    Args:\n",
    "        keys: keys of the corresponding items to be transformed.\n",
    "            See also: monai.transforms.MapTransform\n",
    "        a_min: intensity original range min.\n",
    "        a_max: intensity original range max.\n",
    "        b_min: intensity target range min.\n",
    "        b_max: intensity target range max.\n",
    "        clip: whether to perform clip after scaling.\n",
    "        dtype: output data type, if None, same as input image. defaults to float32.\n",
    "        allow_missing_keys: don't raise exception if key is missing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        output_dir: str,\n",
    "        mode:str,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.output_dir = output_dir\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            if isinstance(d[key], torch.Tensor):\n",
    "                d[key] = d[key].detach().cpu().numpy()\n",
    "            for slice in range(d[key].shape[-1]):\n",
    "                # print(\"type: \" + d[key].dtype)\n",
    "                filename = os.path.basename(d[\"image_meta_dict\"][\"filename_or_obj\"]).split(\".\")[0] + f\"_{slice}.png\"\n",
    "                if key == \"image\":\n",
    "                    if self.mode == \"train\":\n",
    "                        save_dir = os.path.join(self.output_dir, 'imagesTr', filename)\n",
    "                    else:\n",
    "                        save_dir = os.path.join(self.output_dir, 'imagesTs', filename)\n",
    "                else:\n",
    "                    if self.mode == \"train\":\n",
    "                        save_dir = os.path.join(self.output_dir, 'labelsTr', filename)\n",
    "                    else:\n",
    "                        save_dir = os.path.join(self.output_dir, 'labelsTs', filename)\n",
    "                if not os.path.exists(os.path.dirname(save_dir)):\n",
    "                    print(f\"Creating directory: {os.path.dirname(save_dir)}\")\n",
    "                    os.makedirs(os.path.dirname(save_dir))\n",
    "                print(f\"Saving to {save_dir}\")\n",
    "                plt.imsave(save_dir, d[key][0, :, :, slice], cmap=\"gray\")\n",
    "                # img = Image.fromarray(d[key][0, :, :, slice].astype(np.float32))\n",
    "                # img.save(save_dir)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = sorted( glob.glob( os.path.join( \"/mnt/chansey/lauraalvarez/\",\"data\", \"vascular_injuries\", \"nii\", \"imagesTr\", \"*.nii.gz\") ) )\n",
    "train_labels = sorted( glob.glob( os.path.join( \"/mnt/chansey/lauraalvarez/\",\"data\", \"vascular_injuries\", \"nii\", \"labelsTr\", \"*.nii.gz\") ) )\n",
    "data_dicts = [ {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels) ]\n",
    "test_images = sorted( glob.glob( os.path.join( \"/mnt/chansey/lauraalvarez/\",\"data\", \"vascular_injuries\", \"nii\", \"imagesTs\", \"*.nii.gz\" ) ) )\n",
    "test_labels = sorted( glob.glob( os.path.join( \"/mnt/chansey/lauraalvarez/\",\"data\", \"vascular_injuries\", \"nii\", \"labelsTs\", \"*.nii.gz\") ) )\n",
    "data_dicts_test = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(test_images, test_labels) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 error cases\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# transforms_bsl = Compose([ LoadImaged(keys=[\"image\", \"label\"]), AddChanneld(keys=[\"image\"]), ToTensord(keys=[\"image\", \"label\"]),])\n",
    "transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # RemoveDicts(keys=[\"image\", \"label\"]),\n",
    "        AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        # AsChannelFirstd(keys=[\"label\"]),\n",
    "        # AsDiscreted(keys=[\"label\"], argmax=True),\n",
    "        # Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        # Spacingd( keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 1), mode=(\"bilinear\", \"nearest\"),),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"label\"),\n",
    "        NNUnetScaleIntensity(keys=[\"image\"]),\n",
    "        ClosePreprocessing(keys=[\"label\"]),\n",
    "        WriteToPNG(keys=[\"image\", \"label\"], output_dir=\"/mnt/chansey/lauraalvarez/data/vascular_injuries/png/\", mode=\"test\"),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# injure_org = transforms_bsl(data_dicts)\n",
    "error_cases = list()\n",
    "for data_dict in data_dicts_test:\n",
    "    try:\n",
    "        data_dict = transforms(data_dict)\n",
    "    except Exception as e:\n",
    "            error_cases.append(data_dict)\n",
    "print(f\"{len(error_cases)} error cases\")\n",
    "print(error_cases)\n",
    "# injure_crop = transforms(data_dicts)\n",
    "# print(injure_crop[\"image\"].shape, injure_crop[\"label\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'injure_crop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mu:\\lauraalvarez\\traumaAI\\Liver_Segmentation\\vascular_injuries_data_conversion.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular_injuries_data_conversion.ipynb#ch0000012?line=0'>1</a>\u001b[0m blended_true_label \u001b[39m=\u001b[39m blend_images(injure_crop[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m], injure_crop[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m], alpha\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular_injuries_data_conversion.ipynb#ch0000012?line=1'>2</a>\u001b[0m blended_final_true_label_closed \u001b[39m=\u001b[39m blended_true_label\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular_injuries_data_conversion.ipynb#ch0000012?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(blended_final_true_label_closed\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'injure_crop' is not defined"
     ]
    }
   ],
   "source": [
    "blended_true_label = blend_images(injure_crop[\"image\"], injure_crop[\"label\"], alpha=0.9)\n",
    "blended_final_true_label_closed = blended_true_label.permute(1,2,0,3)\n",
    "print(blended_final_true_label_closed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3217b50e700449f985463f429c52265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=88, description='slice', max=177), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.dicom_animation(slice)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.visualize import matshow3d, blend_images\n",
    "import torch \n",
    "\n",
    "def dicom_animation(slice):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.title(f\"liver no injured \")\n",
    "    plt.imshow(blended_final_true_label_closed[:, :, :, slice], cmap=\"bone\")\n",
    "    plt.show()\n",
    "\n",
    "interact(dicom_animation, slice=(0, blended_final_true_label_closed.shape[-1]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a270b461b7f747bfa62901acd69aa588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=88, description='slice', max=177), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.dicom_animation(slice)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.visualize import matshow3d, blend_images\n",
    "import torch \n",
    "\n",
    "def dicom_animation(slice):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.title(f\"liver no injured \")\n",
    "    plt.imshow(blended_final_true_label[:, :, :, slice], cmap=\"bone\")\n",
    "    plt.show()\n",
    "\n",
    "interact(dicom_animation, slice=(0, blended_final_true_label.shape[-1]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load png example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveAlpha(MapTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            if key == \"label\":\n",
    "                d[key] = d[key][...,:1]\n",
    "            else:\n",
    "                d[key] = d[key][...,:3]\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeepOnlyClass(MapTransform, InvertibleTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        class_to_keep: int,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.class_to_keep = class_to_keep\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            self.push_transform(d, key)\n",
    "            # d[key] = np.where((d[key] != self.class_to_keep), 0, d[key])\n",
    "            # d[key] = np.where((d[key] == self.class_to_keep), 1, d[key])\n",
    "            d[key] = np.where(d[key] == 255, 1, 0)\n",
    "            values = d[key]\n",
    "            n_values = np.max(values) + 1\n",
    "            d[key]= np.squeeze(np.eye(n_values)[values])\n",
    "            print(np.unique(d[key][:,:,0]))\n",
    "            print(np.unique(d[key][:,:,1]))\n",
    "            print(d[key].shape)\n",
    "            # print(np.unique(d[key][:,:,0]))\n",
    "            # print(np.unique(d[key][:,:,1]))\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToGrayScale(MapTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        normalize: bool = False,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = d[key][..., :1]\n",
    "            if self.normalize:\n",
    "                d[key] = d[key] / 255\n",
    "            print(d[key].shape)\n",
    "\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[0. 1.]\n",
      "(142, 106, 2)\n",
      "(142, 106, 1)\n"
     ]
    }
   ],
   "source": [
    "transforms =  Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"], reader='pilreader'),\n",
    "                RemoveAlpha(keys=[\"image\", \"label\"]),\n",
    "                KeepOnlyClass(keys=[\"label\"], class_to_keep=255),\n",
    "                ToGrayScale(keys=[\"image\"], normalize=True),\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "                Resized(keys=[\"image\", \"label\"], spatial_size=(256,256)),\n",
    "                # RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=self.train_img_size,random_size=True),\n",
    "                RandFlipd( \n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[0],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[0],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[1],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandRotate90d(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    prob=0.10,\n",
    "                    max_k=3,\n",
    "                ),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "train_images = sorted( glob.glob( os.path.join( \"U:\\\\lauraalvarez\",\"data\", \"vascular_injuries\", \"png\", \"imagesTr\", \"VI_L110016_157.png\") ) )\n",
    "train_labels = sorted( glob.glob( os.path.join( \"U:\\\\lauraalvarez\",\"data\", \"vascular_injuries\", \"png\", \"labelsTr\", \"VI_L110016_157.png\") ) )\n",
    "data_dicts = [ {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels) ]\n",
    "result = transforms(data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 96, 96)\n",
      "(2, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "print(result[0][0][\"image\"].shape)\n",
    "print(result[0][0][\"label\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8ab11887534ee2a39ed9153bdba637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='slice', max=1), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.dicom_animation(slice)>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.visualize import matshow3d, blend_images\n",
    "import torch \n",
    "\n",
    "\n",
    "def dicom_animation(slice):\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(15, 6))\n",
    "    axarr[0,0].imshow(result[0][0][\"image\"][0,:,:], cmap=\"bone\")\n",
    "    axarr[0,1].imshow(result[0][0][\"image\"][0,:,:], cmap=\"bone\")\n",
    "    axarr[0,2].imshow(result[0][0][\"image\"][0,:,:], cmap=\"bone\")\n",
    "    axarr[1,0].imshow(result[0][0][\"label\"][slice,:,:], cmap=\"bone\")\n",
    "    axarr[1,1].imshow(result[0][0][\"label\"][slice,:,:], cmap=\"bone\")\n",
    "    axarr[1,2].imshow(result[0][0][\"label\"][slice,:,:], cmap=\"bone\")\n",
    "\n",
    "\n",
    "interact(dicom_animation, slice=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd953d996cbefa82e0aa9bb4569a5a9d74ea146024a460b967761e6c3fe194fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
