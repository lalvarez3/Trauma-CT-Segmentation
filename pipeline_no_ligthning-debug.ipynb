{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.8.1\n",
      "Numpy version: 1.21.5\n",
      "Pytorch version: 1.11.0\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 71ff399a3ea07aef667b23653620a290364095b1\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 3.2.2\n",
      "scikit-image version: 0.19.2\n",
      "Pillow version: 9.0.1\n",
      "Tensorboard version: 2.8.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.12.0\n",
      "tqdm version: 4.64.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.0\n",
      "pandas version: 1.4.1\n",
      "einops version: 0.4.1\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from monai.data.image_reader import ImageReader, ITKReader\n",
    "from ipywidgets.widgets import *\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning\n",
    "from monai.utils import set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    EnsureChannelFirstd,\n",
    "    Resized,\n",
    ")\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, list_data_collate, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from monai.transforms.spatial.array import Resize\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOM TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from monai.config import DtypeLike, KeysCollection\n",
    "from monai.config.type_definitions import NdarrayOrTensor\n",
    "from monai.networks.layers import AffineTransform\n",
    "from monai.networks.layers.simplelayers import GaussianFilter\n",
    "from monai.transforms.croppad.array import CenterSpatialCrop, SpatialPad\n",
    "from monai.transforms.inverse import InvertibleTransform\n",
    "from monai.transforms.spatial.array import (\n",
    "    AddCoordinateChannels,\n",
    "    Affine,\n",
    "    AffineGrid,\n",
    "    Flip,\n",
    "    GridDistortion,\n",
    "    Orientation,\n",
    "    Rand2DElastic,\n",
    "    Rand3DElastic,\n",
    "    RandAffine,\n",
    "    RandAxisFlip,\n",
    "    RandFlip,\n",
    "    RandGridDistortion,\n",
    "    RandRotate,\n",
    "    RandZoom,\n",
    "    Resize,\n",
    "    Rotate,\n",
    "    Rotate90,\n",
    "    Spacing,\n",
    "    Zoom,\n",
    ")\n",
    "from monai.transforms.transform import MapTransform, RandomizableTransform\n",
    "from monai.transforms.utils import create_grid\n",
    "from monai.utils import (\n",
    "    GridSampleMode,\n",
    "    GridSamplePadMode,\n",
    "    InterpolateMode,\n",
    "    NumpyPadMode,\n",
    "    PytorchPadMode,\n",
    "    ensure_tuple,\n",
    "    ensure_tuple_rep,\n",
    "    fall_back_tuple,\n",
    ")\n",
    "from monai.utils.deprecate_utils import deprecated_arg\n",
    "from monai.utils.enums import TraceKeys\n",
    "from monai.utils.module import optional_import\n",
    "from monai.utils.type_conversion import convert_data_type, convert_to_dst_type\n",
    "from monai.apps import load_from_mmar\n",
    "from monai.apps.mmars import RemoteMMARKeys\n",
    "from monai.networks.utils import copy_model_state\n",
    "from monai.optimizers import generate_param_groups\n",
    "\n",
    "\n",
    "class InterpolateMode(Enum):\n",
    "    NEAREST = \"nearest\"\n",
    "    LINEAR = \"linear\"\n",
    "    BILINEAR = \"bilinear\"\n",
    "    BICUBIC = \"bicubic\"\n",
    "    TRILINEAR = \"trilinear\"\n",
    "    AREA = \"area\"\n",
    "\n",
    "\n",
    "InterpolateModeSequence = Union[\n",
    "    Sequence[Union[InterpolateMode, str]], InterpolateMode, str\n",
    "]\n",
    "\n",
    "\n",
    "class ResizedC(MapTransform, InvertibleTransform):\n",
    "\n",
    "    backend = Resize.backend\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        spatial_size: Union[Sequence[int], int],\n",
    "        size_mode: str = \"all\",\n",
    "        mode: InterpolateModeSequence = InterpolateMode.AREA,\n",
    "        align_corners: Union[Sequence[Optional[bool]], Optional[bool]] = None,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.mode = ensure_tuple_rep(mode, len(self.keys))\n",
    "        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))\n",
    "        self.resizer = Resize(spatial_size=spatial_size, size_mode=size_mode)\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key, mode, align_corners in self.key_iterator(\n",
    "            d, self.mode, self.align_corners\n",
    "        ):\n",
    "            self.push_transform(\n",
    "                d,\n",
    "                key,\n",
    "                extra_info={\n",
    "                    \"mode\": mode.value if isinstance(mode, Enum) else mode,\n",
    "                    \"align_corners\": align_corners\n",
    "                    if align_corners is not None\n",
    "                    else TraceKeys.NONE,\n",
    "                },\n",
    "            )\n",
    "            if key == \"label\":\n",
    "                label = d[key]\n",
    "                print(\"Old shape de la label\", np.shape(d[key]))\n",
    "                # img = dict(img)\n",
    "                for i, channel in enumerate([1, 2]):\n",
    "                    if i == 0:\n",
    "                        device = torch.device(\"cuda\")\n",
    "                        background = torch.zeros(\n",
    "                            1, label.shape[1], label.shape[2], label.shape[3]\n",
    "                        )\n",
    "                        new_image = np.expand_dims(label[channel, :, :, :], 0)\n",
    "                        # if len(new_image.shape) <6 :\n",
    "                        #     new_image = new_image.unsqueeze(0)\n",
    "                        print(background.shape)\n",
    "                        print(new_image.shape)\n",
    "                        new_image = np.concatenate((background, new_image), axis=0)\n",
    "                    else:\n",
    "                        new_image = np.concatenate(\n",
    "                            (new_image, np.expand_dims(label[channel, :, :, :], 0)),\n",
    "                            axis=0,\n",
    "                        )\n",
    "\n",
    "                print(\"Shape de la label\", new_image.shape)\n",
    "                print(\"labels\")\n",
    "                #                 print(np.shape(d[key]))\n",
    "                resized = list()\n",
    "                for channel in new_image:\n",
    "                    print(np.shape(channel))\n",
    "                    resized.append(\n",
    "                        self.resizer(\n",
    "                            np.expand_dims(channel, 0), align_corners=align_corners\n",
    "                        )\n",
    "                    )\n",
    "                d[key] = np.stack(resized).astype(np.float32).squeeze\n",
    "            else:\n",
    "                print(\"img\")\n",
    "                print(np.shape(d[key]))\n",
    "                d[key] = self.resizer(d[key], align_corners=align_corners)\n",
    "        return d\n",
    "\n",
    "    def inverse(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            transform = self.get_most_recent_transform(d, key)\n",
    "            orig_size = transform[TraceKeys.ORIG_SIZE]\n",
    "            mode = transform[TraceKeys.EXTRA_INFO][\"mode\"]\n",
    "            align_corners = transform[TraceKeys.EXTRA_INFO][\"align_corners\"]\n",
    "            # Create inverse transform\n",
    "            inverse_transform = Resize(\n",
    "                spatial_size=orig_size,\n",
    "                mode=mode,\n",
    "                align_corners=None\n",
    "                if align_corners == TraceKeys.NONE\n",
    "                else align_corners,\n",
    "            )\n",
    "            # Apply inverse transform\n",
    "            d[key] = inverse_transform(d[key])\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model\n",
    "Usa Pytorch Lightning pero no usa las funciones internas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED = True\n",
    "TRANSFER_LEARNING = True\n",
    "\n",
    "\n",
    "class Net(pytorch_lightning.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        if PRETRAINED:\n",
    "            print(\"using a pretrained model.\")\n",
    "            unet_model = load_from_mmar(\n",
    "                item=mmar[RemoteMMARKeys.NAME],\n",
    "                mmar_dir=root_dir,\n",
    "                # map_location=device,\n",
    "                pretrained=True,\n",
    "            )\n",
    "            self._model = unet_model\n",
    "            # copy all the pretrained weights except for variables whose name matches \"model.0.conv.unit0\"\n",
    "            if TRANSFER_LEARNING:\n",
    "                pretrained_dict, updated_keys, unchanged_keys = copy_model_state(\n",
    "                    self._model, unet_model, exclude_vars=\"model.0.conv.unit0\"\n",
    "                )\n",
    "                print(\n",
    "                    \"num. var. using the pretrained\",\n",
    "                    len(updated_keys),\n",
    "                    \", random init\",\n",
    "                    len(unchanged_keys),\n",
    "                    \"variables.\",\n",
    "                )\n",
    "                self._model.load_state_dict(pretrained_dict)\n",
    "                # stop gradients for the pretrained weights\n",
    "                for x in self._model.named_parameters():\n",
    "                    if x[0] in updated_keys:\n",
    "                        x[1].requires_grad = False\n",
    "                params = generate_param_groups(\n",
    "                    network=self._model,\n",
    "                    layer_matches=[lambda x: x[0] in updated_keys],\n",
    "                    match_types=[\"filter\"],\n",
    "                    lr_values=[1e-4],\n",
    "                    include_others=False,\n",
    "                )\n",
    "                self.params = params\n",
    "\n",
    "        else:\n",
    "            self._model = UNet(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=2,\n",
    "                channels=(16, 32, 64, 128, 256),\n",
    "                strides=(2, 2, 2, 2),\n",
    "                num_res_units=2,\n",
    "                norm=Norm.BATCH,\n",
    "            )\n",
    "        self.loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "        self.post_pred = Compose(\n",
    "            [EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)]\n",
    "        )\n",
    "        self.post_label = Compose(\n",
    "            [EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)]\n",
    "        )\n",
    "        self.dice_metric = DiceMetric(\n",
    "            include_background=False, reduction=\"mean\", get_not_nans=False\n",
    "        )\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # set up the correct data path\n",
    "        train_images = sorted(\n",
    "            glob.glob(\n",
    "                os.path.join(\n",
    "                    \"U:\", \"\\lauraalvarez\", \"data\", \"liver\", \"train\", \"scans\", \"*.mha\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        train_labels = sorted(\n",
    "            glob.glob(\n",
    "                os.path.join(\n",
    "                    \"U:\", \"\\lauraalvarez\", \"data\", \"liver\", \"train\", \"overlays\", \"*.mha\"\n",
    "                )\n",
    "            )\n",
    "        )  # U:\\lauraalvarez\\data\\overlays\\overlay_results\\overlay_results\\liver\\train\n",
    "        data_dicts = [\n",
    "            {\"image\": image_name, \"label\": label_name}\n",
    "            for image_name, label_name in zip(train_images, train_labels)\n",
    "        ]\n",
    "        train_files, val_files = [data_dicts[0]], [data_dicts[0]]\n",
    "        print(\"len(train_files)\", len(train_files))\n",
    "\n",
    "        # set deterministic training for reproducibility\n",
    "        set_determinism(seed=0)\n",
    "\n",
    "        # define the data transforms\n",
    "        train_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"], reader=ITKReader),\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "                ResizedC(keys=[\"image\"], spatial_size=(96, 96, 96)),\n",
    "                ResizedC(keys=[\"label\"], spatial_size=(96, 96, 96)),\n",
    "                # Orientationd(keys=[\"image\", \"label\"], axcodes=\"PLI\"),\n",
    "                # AddChanneld(keys=[\"label\"]),\n",
    "                ScaleIntensityRanged(\n",
    "                    keys=[\"image\"],\n",
    "                    a_min=-57,\n",
    "                    a_max=164,\n",
    "                    b_min=0.0,\n",
    "                    b_max=1.0,\n",
    "                    clip=True,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        # val_transforms = Compose(\n",
    "        #     [\n",
    "        #         LoadImaged(keys=[\"image\", \"label\"], reader=ITKReader),\n",
    "        #         EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        #         # SelectChannels(keys=[\"label\"], channel_dims=[1,2]),\n",
    "        #         Orientationd(keys=[\"image\", \"label\"], axcodes=\"PLI\"),\n",
    "        #         # AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        #         ScaleIntensityRanged(\n",
    "        #             keys=[\"image\"], a_min=-57, a_max=164,\n",
    "        #             b_min=0.0, b_max=1.0, clip=True,\n",
    "        #         ),\n",
    "        #     ]\n",
    "        # )\n",
    "\n",
    "        # we use cached datasets - these are 10x faster than regular datasets\n",
    "        self.train_ds = CacheDataset(\n",
    "            data=train_files,\n",
    "            transform=train_transforms,\n",
    "            cache_rate=1.0,\n",
    "            num_workers=20,\n",
    "        )\n",
    "        # self.val_ds = CacheDataset(\n",
    "        #     data=val_files, transform=val_transforms,\n",
    "        #     cache_rate=1.0, num_workers=20,\n",
    "        # )\n",
    "        # print('len(self.val_ds)', len(self.val_ds))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=1,\n",
    "            collate_fn=list_data_collate,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     val_loader = torch.utils.data.DataLoader(\n",
    "    #         self.val_ds, batch_size=1, num_workers=1)\n",
    "    #     return val_loader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        print(\"predicting dataload...\")\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            self.train_ds, batch_size=1, num_workers=1\n",
    "        )\n",
    "        print(\"len(val_loader)\", len(val_loader))\n",
    "        return val_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n",
    "        if TRANSFER_LEARNING:\n",
    "            optimizer = torch.optim.Adam(self.params, 1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        print(\"training started..\")\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        # labels = SelectChannels([1, 2], labels)\n",
    "        print(\"forwarding..\")\n",
    "        output = self.forward(images)\n",
    "        print(\"calculating loss\")\n",
    "        loss = self.loss_function(output, labels)\n",
    "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        print(\"predicting...\")\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        print(\"images.shape\", images.shape)\n",
    "        print(\"labels.shape\", labels.shape)\n",
    "        # labels = SelectChannels([1, 2], labels)\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        outputs = sliding_window_inference(\n",
    "            images, roi_size, sw_batch_size, self.forward\n",
    "        )\n",
    "        # outputs = self.forward(images)\n",
    "        return {\"output\": outputs, \"image\": images, \"label\": labels}\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     images, labels = batch[\"image\"], batch[\"label\"]\n",
    "    #     labels = SelectChannels([1,2], labels)\n",
    "    #     roi_size = (160, 160, 160)\n",
    "    #     sw_batch_size = 4\n",
    "    #     outputs = sliding_window_inference(\n",
    "    #         images, roi_size, sw_batch_size, self.forward)\n",
    "    #     loss = self.loss_function(outputs, labels)\n",
    "    #     outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "    #     labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "    #     self.dice_metric(y_pred=outputs, y=labels)\n",
    "    #     return {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
    "\n",
    "    # def validation_epoch_end(self, outputs):\n",
    "    #     val_loss, num_items = 0, 0\n",
    "    #     for output in outputs:\n",
    "    #         val_loss += output[\"val_loss\"].sum().item()\n",
    "    #         num_items += output[\"val_number\"]\n",
    "    #     mean_val_dice = self.dice_metric.aggregate().item()\n",
    "    #     self.dice_metric.reset()\n",
    "    #     mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "    #     tensorboard_logs = {\n",
    "    #         \"val_dice\": mean_val_dice,\n",
    "    #         \"val_loss\": mean_val_loss,\n",
    "    #     }\n",
    "    #     if mean_val_dice > self.best_val_dice:\n",
    "    #         self.best_val_dice = mean_val_dice\n",
    "    #         self.best_val_epoch = self.current_epoch\n",
    "    #     print(\n",
    "    #         f\"current epoch: {self.current_epoch} \"\n",
    "    #         f\"current mean dice: {mean_val_dice:.4f}\"\n",
    "    #         f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
    "    #         f\"at epoch: {self.best_val_epoch}\"\n",
    "    #     )\n",
    "    #     return {\"log\": tensorboard_logs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "import os \n",
    "import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data():\n",
    "    # set up the correct data path\n",
    "    train_images = sorted(\n",
    "        glob.glob(os.path.join(\"U:\",\"\\lauraalvarez\",\"data\",\"liver\", \"train\", \"scans\", \"*.mha\")))\n",
    "    train_labels = sorted(\n",
    "        glob.glob(os.path.join(\"U:\",\"\\lauraalvarez\",\"data\",\"liver\", \"train\", \"overlays\", \"*.mha\"))) # U:\\lauraalvarez\\data\\overlays\\overlay_results\\overlay_results\\liver\\train\n",
    "    data_dicts = [\n",
    "        {\"image\": image_name, \"label\": label_name}\n",
    "        for image_name, label_name in zip(train_images, train_labels)\n",
    "    ]\n",
    "    train_files, val_files = [data_dicts[0]], [data_dicts[0]]\n",
    "    print('len(train_files)', len(train_files))\n",
    "\n",
    "    # set deterministic training for reproducibility\n",
    "    set_determinism(seed=0)\n",
    "\n",
    "    # define the data transforms\n",
    "    train_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"label\"], reader=ITKReader),\n",
    "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "            ResizedC(keys=[\"image\"], spatial_size=(96, 96, 96)),\n",
    "            ResizedC(keys=[\"label\"], spatial_size=(96, 96, 96)),\n",
    "#                 Orientationd(keys=[\"image\", \"label\"], axcodes=\"PLI\"),\n",
    "#                 AddChanneld(keys=[\"label\"]),\n",
    "            ScaleIntensityRanged(\n",
    "                keys=[\"image\"], a_min=-57, a_max=164,\n",
    "                b_min=0.0, b_max=1.0, clip=True,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_ds = CacheDataset(\n",
    "        data=train_files, transform=train_transforms,\n",
    "        cache_rate=1.0, num_workers=20,\n",
    "    )\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train & val loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, name):\n",
    "    file_path = \"checkpoints/\"\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    epoch = state[\"epoch\"]\n",
    "    save_dir = file_path + name + str(epoch) + \"_\" + str(round(float(state[\"acc\"]), 4))\n",
    "    torch.save(state, save_dir)\n",
    "    print(f\"Saving checkpoint for epoch {epoch} in: {save_dir}\")\n",
    "\n",
    "\n",
    "def save_state_dict(state, name):\n",
    "    file_path = \"checkpoints/\"\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    save_dir = file_path + f\"{name}_best\"\n",
    "    torch.save(state, save_dir)\n",
    "    print(f\"Best accuracy so far. Saving model to:{save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "\n",
    "    # declare the bar for TQDM\n",
    "    pbar = tqdm(total=len(train_loader), position=0, unit=\"it\", leave=True)\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "        print(\"training started..\")\n",
    "        images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        print(\"forwarding..\")\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(images)\n",
    "        print(\"calculating loss\")\n",
    "        loss = model.loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"\\tLoss: {np.mean(losses):.4f}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx + 1}/{len(train_loader)}]\\tLoss: {np.mean(losses):.4f}\"\n",
    "            )\n",
    "\n",
    "    return losses, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_files) 1\n"
     ]
    },
    {
     "ename": "OptionalImportError",
     "evalue": "required package `itk` is not installed or the version doesn't match requirement.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOptionalImportError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vincent Stirler\\Documents\\laura_trauma_ai\\AITrauma-Segmentation\\pipeline_no_ligthning.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000014?line=0'>1</a>\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(prepare_data(), batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000014?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000014?line=3'>4</a>\u001b[0m     images, labels \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m], batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\Vincent Stirler\\Documents\\laura_trauma_ai\\AITrauma-Segmentation\\pipeline_no_ligthning.ipynb Cell 9'\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=14'>15</a>\u001b[0m     set_determinism(seed\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=16'>17</a>\u001b[0m     \u001b[39m# define the data transforms\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=17'>18</a>\u001b[0m     train_transforms \u001b[39m=\u001b[39m Compose(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=18'>19</a>\u001b[0m         [\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=19'>20</a>\u001b[0m             LoadImaged(keys\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m], reader\u001b[39m=\u001b[39;49mITKReader),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=20'>21</a>\u001b[0m             EnsureChannelFirstd(keys\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=21'>22</a>\u001b[0m             ResizedC(keys\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m], spatial_size\u001b[39m=\u001b[39m(\u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=22'>23</a>\u001b[0m             ResizedC(keys\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m], spatial_size\u001b[39m=\u001b[39m(\u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=23'>24</a>\u001b[0m \u001b[39m#                 Orientationd(keys=[\"image\", \"label\"], axcodes=\"PLI\"),\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=24'>25</a>\u001b[0m \u001b[39m#                 AddChanneld(keys=[\"label\"]),\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=25'>26</a>\u001b[0m             ScaleIntensityRanged(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=26'>27</a>\u001b[0m                 keys\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m], a_min\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m57\u001b[39m, a_max\u001b[39m=\u001b[39m\u001b[39m164\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=27'>28</a>\u001b[0m                 b_min\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, b_max\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, clip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=28'>29</a>\u001b[0m             ),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=29'>30</a>\u001b[0m         ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=30'>31</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=32'>33</a>\u001b[0m     train_ds \u001b[39m=\u001b[39m CacheDataset(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=33'>34</a>\u001b[0m         data\u001b[39m=\u001b[39mtrain_files, transform\u001b[39m=\u001b[39mtrain_transforms,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=34'>35</a>\u001b[0m         cache_rate\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=35'>36</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent%20Stirler/Documents/laura_trauma_ai/AITrauma-Segmentation/pipeline_no_ligthning.ipynb#ch0000008?line=36'>37</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m train_ds\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_pytorch\\lib\\site-packages\\monai\\transforms\\io\\dictionary.py:101\u001b[0m, in \u001b[0;36mLoadImaged.__init__\u001b[1;34m(self, keys, reader, dtype, meta_keys, meta_key_postfix, overwriting, image_only, allow_missing_keys, *args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/dictionary.py?line=75'>76</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/dictionary.py?line=76'>77</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/dictionary.py?line=77'>78</a>\u001b[0m \u001b[39m    keys: keys of the corresponding items to be transformed.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/dictionary.py?line=97'>98</a>\u001b[0m \u001b[39m    kwargs: additional parameters for reader if providing a reader name.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/dictionary.py?line=98'>99</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/dictionary.py?line=99'>100</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(keys, allow_missing_keys)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/dictionary.py?line=100'>101</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loader \u001b[39m=\u001b[39m LoadImage(reader, image_only, dtype, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/dictionary.py?line=101'>102</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(meta_key_postfix, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/dictionary.py?line=102'>103</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmeta_key_postfix must be a str but is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(meta_key_postfix)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_pytorch\\lib\\site-packages\\monai\\transforms\\io\\array.py:153\u001b[0m, in \u001b[0;36mLoadImage.__init__\u001b[1;34m(self, reader, image_only, dtype, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/array.py?line=150'>151</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister(the_reader())\n\u001b[0;32m    <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/array.py?line=151'>152</a>\u001b[0m \u001b[39melif\u001b[39;00m inspect\u001b[39m.\u001b[39misclass(_r):\n\u001b[1;32m--> <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/array.py?line=152'>153</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister(_r(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/array.py?line=153'>154</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/transforms/io/array.py?line=154'>155</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister(_r)  \u001b[39m# reader instance, ignoring the constructor args/kwargs\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_pytorch\\lib\\site-packages\\monai\\utils\\module.py:381\u001b[0m, in \u001b[0;36mrequire_pkg.<locals>._decorator.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/utils/module.py?line=378'>379</a>\u001b[0m err_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrequired package `\u001b[39m\u001b[39m{\u001b[39;00mpkg_name\u001b[39m}\u001b[39;00m\u001b[39m` is not installed or the version doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match requirement.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/utils/module.py?line=379'>380</a>\u001b[0m \u001b[39mif\u001b[39;00m raise_error:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/utils/module.py?line=380'>381</a>\u001b[0m     \u001b[39mraise\u001b[39;00m OptionalImportError(err_msg)\n\u001b[0;32m    <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/utils/module.py?line=381'>382</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Vincent%20Stirler/.conda/envs/env_pytorch/lib/site-packages/monai/utils/module.py?line=382'>383</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(err_msg)\n",
      "\u001b[1;31mOptionalImportError\u001b[0m: required package `itk` is not installed or the version doesn't match requirement."
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(prepare_data(), batch_size=1, shuffle=True)\n",
    "\n",
    "for batch in train_loader:\n",
    "    images, labels = batch[\"image\"], batch[\"label\"]\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmar = {\n",
    "        RemoteMMARKeys.ID: \"clara_pt_liver_and_tumor_ct_segmentation_1\",\n",
    "        RemoteMMARKeys.NAME: \"clara_pt_liver_and_tumor_ct_segmentation\",\n",
    "        RemoteMMARKeys.FILE_TYPE: \"zip\",\n",
    "        RemoteMMARKeys.HASH_TYPE: \"md5\",\n",
    "        RemoteMMARKeys.HASH_VAL: None,\n",
    "        RemoteMMARKeys.MODEL_FILE: os.path.join(\"models\", \"model.pt\"),\n",
    "        RemoteMMARKeys.CONFIG_FILE: os.path.join(\"config\", \"config_train.json\"),\n",
    "        RemoteMMARKeys.VERSION: 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  Net()\n",
    "img3d = torch.randn(1, 1, 96, 96, 96)\n",
    "print(\"Input shape: \", img3d.shape)\n",
    "\n",
    "preds = model(img3d)\n",
    "print(\"ViT3D output size:\", preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "batchsize_train = 1\n",
    "batchsize_eval = 1\n",
    "\n",
    "log_interval = 2\n",
    "count = 0\n",
    "min_loss = 1000\n",
    "max_acc = 0\n",
    "patience = 3\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "\n",
    "# Declare the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Set the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "train_dataset = prepare_data()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize_train, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS):\n",
    "    losses, acces = train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    loss = np.mean(losses)\n",
    "\n",
    "    # Save the state for the best model.\n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        count = 0\n",
    "        save_state_dict(model.state_dict(), \"testing\")\n",
    "    else:\n",
    "        count += 1\n",
    "        print(f\"Validation loss Not improved. > {min_loss}\")\n",
    "        print(\"Count ->\", count)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        # Save checkpoint of the model\n",
    "        save_checkpoint(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": loss,\n",
    "            },\n",
    "            \"testing_limits\",\n",
    "        )\n",
    "        print(f\"Saved checkpoint for epoch {epoch}\")\n",
    "\n",
    "    if count == patience:\n",
    "        print(f\"Early termination, {patience} epochs without improvement.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f89ae24ad59f77282abede215fe9a4582ae0cea7ce62a3c8ac808775133a3c1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('degyver')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
