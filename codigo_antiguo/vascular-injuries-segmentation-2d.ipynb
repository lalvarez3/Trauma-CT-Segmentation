{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djzZ-bffGr8X"
   },
   "source": [
    "#### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "biFwQV_yGr8X",
    "outputId": "caa6e216-4409-4de4-bc5a-ceec09bf6af7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.8.1\n",
      "Numpy version: 1.22.3\n",
      "Pytorch version: 1.11.0\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 71ff399a3ea07aef667b23653620a290364095b1\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.2\n",
      "Nibabel version: 3.2.2\n",
      "scikit-image version: 0.19.2\n",
      "Pillow version: 9.0.1\n",
      "Tensorboard version: 2.8.0\n",
      "gdown version: 4.4.0\n",
      "TorchVision version: 0.12.0\n",
      "tqdm version: 4.64.0\n",
      "lmdb version: 1.3.0\n",
      "psutil version: 5.9.0\n",
      "pandas version: 1.4.2\n",
      "einops version: 0.4.1\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ipywidgets.widgets import *\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning\n",
    "from monai.utils import set_determinism\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureType,\n",
    "    EnsureChannelFirstd,\n",
    "    RandFlipd,\n",
    "    RandRotated,\n",
    "    ToTensord,\n",
    "    Resized,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandRotate90d,\n",
    "    RandShiftIntensityd,\n",
    "    KeepLargestConnectedComponent,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandCropByLabelClassesd,\n",
    "    RandSpatialCropd,\n",
    "    ScaleIntensityd,\n",
    "    SpatialPadd,\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceFocalLoss, GeneralizedDiceLoss\n",
    "from monai.losses import DiceLoss, DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import (\n",
    "    CacheDataset,\n",
    "    list_data_collate,\n",
    "    decollate_batch,\n",
    "    Dataset,\n",
    "    LMDBDataset,\n",
    "    create_test_image_2d,\n",
    ")\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from monai.data import DataLoader\n",
    "import os\n",
    "import glob\n",
    "from monai.transforms.spatial.array import Resize\n",
    "\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from monai.config import DtypeLike, KeysCollection\n",
    "from monai.config.type_definitions import NdarrayOrTensor\n",
    "from monai.networks.layers import AffineTransform\n",
    "from monai.networks.layers.simplelayers import GaussianFilter\n",
    "from monai.transforms.croppad.array import CenterSpatialCrop, SpatialPad\n",
    "from monai.transforms.inverse import InvertibleTransform\n",
    "from monai.transforms.spatial.array import (\n",
    "    Resize,\n",
    ")\n",
    "from monai.transforms.transform import MapTransform, RandomizableTransform\n",
    "from monai.transforms.utils import create_grid\n",
    "from monai.utils import (\n",
    "    InterpolateMode,\n",
    "    ensure_tuple_rep,\n",
    ")\n",
    "from monai.utils.deprecate_utils import deprecated_arg\n",
    "from monai.utils.enums import TraceKeys\n",
    "from monai.utils.module import optional_import\n",
    "from monai.utils.type_conversion import convert_data_type, convert_to_dst_type\n",
    "from monai.apps import load_from_mmar\n",
    "from monai.apps.mmars import RemoteMMARKeys\n",
    "from monai.networks.utils import copy_model_state\n",
    "from monai.optimizers import generate_param_groups\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from pl_bolts.callbacks import PrintTableMetricsCallback\n",
    "from monai.visualize import matshow3d, blend_images\n",
    "import imageio\n",
    "\n",
    "print_config()\n",
    "from monai.losses import GeneralizedWassersteinDiceLoss\n",
    "import random\n",
    "import tempfile\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeepOnlyClass(MapTransform, InvertibleTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        class_to_keep: int,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.class_to_keep = class_to_keep\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            self.push_transform(d, key)\n",
    "            d[key] = np.where(d[key] == 255, 1, 0)\n",
    "            # values = d[key]\n",
    "            # print(\"NP UNIQUE\", np.unique(values))\n",
    "            # print('BEFORE EYE', d[key].shape)\n",
    "            # n_values = np.max(values) + 1\n",
    "            # d[key]= np.eye(n_values)[values]\n",
    "            # print('AFTER EYE', d[key].shape)\n",
    "            # d[key] = np.squeeze(d[key])\n",
    "            # print('AFTER squeeze', d[key].shape)\n",
    "            # print(\"NP UNIQUE AFTER\", np.unique(d[key]))\n",
    "            # if d[key].ndim == 2:\n",
    "            #     zeros = np.zeros(d[key].shape)\n",
    "            #     d[key] = np.stack([d[key], zeros], axis=-1)\n",
    "            #     print(d[key].shape)\n",
    "            # print(\"NP UNIQUE AFTER AFTER 0\", np.unique(d[key][:,:,0]))\n",
    "            # print(\"NP UNIQUE AFTER AFTER 1\", np.unique(d[key][:,:,1]))\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveAlpha(MapTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            if key == \"label\":\n",
    "                d[key] = d[key][...,:1]\n",
    "            else:\n",
    "                d[key] = d[key][...,:3]\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToGrayScale(MapTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        normalize: bool = False,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = d[key][..., :1]\n",
    "            if self.normalize:\n",
    "                d[key] = d[key] / 255\n",
    "\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED = False\n",
    "TRANSFER_LEARNING = False\n",
    "N_WORKERS_LOADER = 0\n",
    "N_WORKERS_CACHE = 0\n",
    "CACHE_RATE = 0\n",
    "SEED = 42\n",
    "BS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WzCJRUGGr8a"
   },
   "source": [
    "#### Define the LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spleen_data\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = \"spleen_data\"\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, train_img_size, val_img_size, n_output):\n",
    "        super().__init__()\n",
    "        self.train_img_size = train_img_size\n",
    "        self.val_img_size = val_img_size\n",
    "        self.n_output = n_output\n",
    "\n",
    "        self._model = UNet(\n",
    "            spatial_dims=2,\n",
    "            in_channels=1,\n",
    "            out_channels=2,\n",
    "            channels=(16, 32, 64, 128, 256),\n",
    "            strides=(2, 2, 2, 2),\n",
    "            num_res_units=2,\n",
    "            norm=Norm.BATCH,\n",
    "        )\n",
    "        self.loss_function = DiceCELoss(to_onehot_y=True, softmax=True, jaccard=True)\n",
    "        self.post_pred = Compose([EnsureType(), Activations(softmax=True), AsDiscrete(argmax=True, to_onehot=2)])\n",
    "        self.post_label = Compose([EnsureType(), AsDiscrete(to_onehot=2)])\n",
    "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
    "        self.train_dice_metric = DiceMetric(\n",
    "            include_background=True, reduction=\"mean_batch\",  get_not_nans=False\n",
    "        )\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "        self.save_hyperparameters()  # save hyperparameters\n",
    "\n",
    "        # self.logger.expe.init(self.hparams)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # tempdir = \"synthetic_data\"\n",
    "\n",
    "        # print(f\"generating synthetic data to {tempdir} (this may take a while)\")\n",
    "        # if not glob.glob(os.path.join(tempdir, \"img*.png\")):\n",
    "        #     for i in range(40):\n",
    "        #         im, seg = create_test_image_2d(512, 512, num_seg_classes=1)\n",
    "        #         Image.fromarray((im * 255).astype(\"uint8\")).save(os.path.join(tempdir, f\"img{i:d}.png\"))\n",
    "        #         Image.fromarray((seg * 255).astype(\"uint8\")).save(os.path.join(tempdir, f\"seg{i:d}.png\"))\n",
    "        \n",
    "        # lnp.lnp(f\"Tempdir: {tempdir}\")\n",
    "\n",
    "        # images = sorted(glob.glob(os.path.join(tempdir, \"img*.png\")))\n",
    "        # segs = sorted(glob.glob(os.path.join(tempdir, \"seg*.png\")))\n",
    "        \n",
    "        train_images = sorted(\n",
    "            glob.glob(\n",
    "                os.path.join(\n",
    "                    \"u://\",\n",
    "                    \"lauraalvarez\",\n",
    "                    \"data\",\n",
    "                    \"vascular_injuries\",\n",
    "                    \"png\",\n",
    "                    \"imagesTr\",\n",
    "                    \"*.png\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "\n",
    "        train_labels = [img.replace(\"imagesTr\", \"labelsTr\") for img in train_images]\n",
    "\n",
    "        data_dicts = [\n",
    "            {\"image\": image_name, \"label\": label_name}\n",
    "            for image_name, label_name in zip(train_images, train_labels)\n",
    "        ]\n",
    "\n",
    "        # # Codigo para train sintetico\n",
    "        # data_dicts = [\n",
    "        #     {\"image\": image_name, \"label\": label_name}\n",
    "        #     for image_name, label_name in zip(images, segs)\n",
    "        #     ]\n",
    "        \n",
    "\n",
    "        test_images = sorted(\n",
    "            glob.glob(\n",
    "                os.path.join(\n",
    "                    \"U:\\\\\",\n",
    "                    \"lauraalvarez\",\n",
    "                    \"data\",\n",
    "                    \"vascular_injuries\",\n",
    "                    \"png\",\n",
    "                    \"imagesTs\",\n",
    "                    \"*.png\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        test_labels = [img.replace(\"imagesTs\", \"labelsTs\") for img in test_images]\n",
    "\n",
    "        data_dicts_test = [\n",
    "            {\"image\": image_name, \"label\": label_name}\n",
    "            for image_name, label_name in zip(test_images, test_labels)\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Codigo para test sintetico\n",
    "        # if not glob.glob(os.path.join(tempdir, \"img_test*.png\")):\n",
    "        #     for i in range(5):\n",
    "        #         im_test, seg_test = create_test_image_2d(512, 512, num_seg_classes=1)\n",
    "        #         Image.fromarray((im_test * 255).astype(\"uint8\")).save(os.path.join(str(tempdir), f\"img_test{i:d}.png\"))\n",
    "        #         Image.fromarray((seg_test * 255).astype(\"uint8\")).save(os.path.join(str(tempdir), f\"seg_test{i:d}.png\"))\n",
    "        \n",
    "        # images_test = sorted(glob.glob(os.path.join(tempdir, \"img_test*.png\")))\n",
    "        # segs_test = sorted(glob.glob(os.path.join(tempdir, \"seg_test*.png\")))\n",
    "        \n",
    "        # data_dicts_test = [\n",
    "        #     {\"image\": image_name, \"label\": label_name}\n",
    "        #     for image_name, label_name in zip(images_test, segs_test)\n",
    "        # ]\n",
    "\n",
    "        random.shuffle(data_dicts)\n",
    "        train_files, val_files = data_dicts, data_dicts_test\n",
    "        print(\"len(train_files)\", len(train_files))\n",
    "        print(\"len(validation files)\", len(val_files))\n",
    "\n",
    "        # set deterministic training for reproducibility\n",
    "        set_determinism(seed=SEED)\n",
    "\n",
    "        train_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                RemoveAlpha(keys=[\"image\", \"label\"]),\n",
    "                ScaleIntensityd(keys=[\"image\"]),\n",
    "                KeepOnlyClass(keys=[\"label\"], class_to_keep=255),\n",
    "                ToGrayScale(keys=[\"image\"], normalize=False),\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "                # RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=(96, 96), max_roi_size = (96, 96),random_size=False),\n",
    "                RandCropByLabelClassesd(keys=[\"image\", \"label\"], label_key=\"label\", num_classes=2, spatial_size=(96, 96), num_samples=4),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[0],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[0],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[1],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandRotate90d(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    prob=0.10,\n",
    "                    max_k=3,\n",
    "                ),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # define the data transforms\n",
    "        val_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                RemoveAlpha(keys=[\"image\", \"label\"]),\n",
    "                ScaleIntensityd(keys=[\"image\"]),\n",
    "                KeepOnlyClass(keys=[\"label\"], class_to_keep=255),\n",
    "                ToGrayScale(keys=[\"image\"], normalize=False),\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.train_ds = Dataset(\n",
    "            data=train_files,\n",
    "            transform=train_transforms,\n",
    "        )\n",
    "\n",
    "        self.val_ds = Dataset(\n",
    "            data=val_files,\n",
    "            transform=val_transforms,\n",
    "        )\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "                self._model.parameters(), lr=0.1, momentum=0.99, nesterov=True, weight_decay=3e-05\n",
    "            )\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lambda epoch: (1 - self.current_epoch / 1000) ** 0.9,\n",
    "            last_epoch= self.current_epoch -1,\n",
    "            verbose=True,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=N_WORKERS_LOADER,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            collate_fn=list_data_collate,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=N_WORKERS_LOADER,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "        return val_loader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        predict_dataloader = torch.utils.data.DataLoader(\n",
    "            self.val_ds, batch_size=1, shuffle=False, num_workers=N_WORKERS_LOADER\n",
    "        )\n",
    "        return predict_dataloader\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        print(batch)\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        # lnp.lnp(images.shape)\n",
    "        output = self.forward(images)\n",
    "        # roi_size = (96, 96)\n",
    "        # sw_batch_size = 4\n",
    "        # output = sliding_window_inference(images, roi_size, sw_batch_size, self.forward)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(output)]\n",
    "\n",
    "        loss = self.loss_function(output, labels)\n",
    "\n",
    "        labels_1 = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(output)]\n",
    "        self.train_dice_metric(y_pred=outputs, y=labels_1)\n",
    "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        #  the function is called after every epoch is completed\n",
    "\n",
    "        dice_injure = self.train_dice_metric.aggregate()\n",
    "        dice_injure = dice_injure[0]\n",
    "        self.train_dice_metric.reset()\n",
    "        # calculating average loss\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "\n",
    "        # logging using tensorboard logger\n",
    "        self.log(\"dice loss\", avg_loss, logger=False)\n",
    "        # self.log(\"train liver dice\", dice_liver)\n",
    "        self.log(\"train injure dice\", dice_injure, logger=False)\n",
    "\n",
    "        self.logger.experiment.log({\"dice loss\": avg_loss})\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        # filenames = batch[\"path\"]\n",
    "        # post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=2)])\n",
    "        # post_label = Compose([EnsureType(), AsDiscrete(to_onehot=2)])\n",
    "        roi_size = (96, 96)\n",
    "        sw_batch_size = 4\n",
    "        outputs = sliding_window_inference(\n",
    "            images, roi_size, sw_batch_size, self.forward\n",
    "        )\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        predicition = {\n",
    "            \"output\": outputs,\n",
    "            \"image\": images,\n",
    "            \"label\": labels,\n",
    "        }\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [\n",
    "            self.post_label(i) for i in decollate_batch(labels)\n",
    "        ]\n",
    "\n",
    "        # labels = [post_label(i) for i in decollate_batch(labels)]\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        return {\n",
    "            \"dice_metric\": self.dice_metric,\n",
    "            \"val_number\": len(outputs),\n",
    "            \"prediction\": predicition,\n",
    "            \"val_loss\": loss,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        # val_loss, num_items = 0, 0\n",
    "        # for output in outputs:\n",
    "        #     val_loss += output[\"val_loss\"].sum().item()\n",
    "        #     num_items += output[\"val_number\"]\n",
    "        # mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "        mean_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "\n",
    "        post_pred_dice = Compose(\n",
    "            [\n",
    "                EnsureType(),\n",
    "                AsDiscrete(argmax=True, to_onehot=2),\n",
    "                KeepLargestConnectedComponent([1], is_onehot=True, independent=False),\n",
    "            ]\n",
    "        )\n",
    "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=2)])\n",
    "        dice_injure = self.dice_metric.aggregate()\n",
    "        dice_injure = dice_injure[0]\n",
    "        # lnp.lnp(dice_injure)\n",
    "\n",
    "        self.dice_metric.reset()\n",
    "        tensorboard_logs = {\n",
    "            \"dice_metric\": dice_injure,\n",
    "        }\n",
    "        predictions = [x[\"prediction\"] for x in outputs]\n",
    "\n",
    "        if dice_injure > self.best_val_dice:\n",
    "            self.best_val_dice = dice_injure.item()\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "        print(\n",
    "            f\"current epoch: {self.current_epoch} \"\n",
    "            # f\"current liver dice: {dice_liver:.4f}\"\n",
    "            f\"current injure  dice: {dice_injure:.4f}\"\n",
    "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
    "            f\"at epoch: {self.best_val_epoch}\"\n",
    "        )\n",
    "        # self.log(\"dice_metric_liver\", dice_liver.item(), prog_bar=True)\n",
    "        self.log(\"dice_metric_injure\", dice_injure.item(), prog_bar=True, logger=False)\n",
    "        self.log(\"val_loss\", mean_val_loss, prog_bar=True, logger=False)\n",
    "        return {\"log\": tensorboard_logs}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        print(\"predicting...\")\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=2)])\n",
    "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=2)])\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 2\n",
    "        outputs = sliding_window_inference(\n",
    "            images, roi_size, sw_batch_size, self.forward\n",
    "        )\n",
    "        predicition = {\"output\": outputs, \"image\": images, \"label\": labels}\n",
    "        outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
    "\n",
    "        labels = [\n",
    "            post_label(torch.unsqueeze(i, 0)).squeeze() for i in decollate_batch(labels)\n",
    "        ]\n",
    "        dice_metric = self.dice_metric(y_pred=outputs, y=labels)\n",
    "        return {\"prediction\": predicition, \"dice_metric\": dice_metric}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Gif Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(prediction, filename):\n",
    "    def _save_gif(volume, filename):\n",
    "        volume = volume.astype(np.float64) / np.max(volume) # normalize the data to 0 - 1\n",
    "        volume = 255 * volume # Now scale by 255\n",
    "        volume = volume.astype(np.uint8)\n",
    "        path_to_gif = os.path.join(\"gifs\", f'{filename}.gif')\n",
    "        if not os.path.exists(\"gifs\"):\n",
    "            print(\"Creating gifs directory\")\n",
    "            os.mkdir(\"gifs\")\n",
    "        imageio.mimsave(path_to_gif, volume)\n",
    "        return path_to_gif\n",
    "    post_pred_blending = Compose([EnsureType(), AsDiscrete(argmax=False),KeepLargestConnectedComponent([1,2], is_onehot=False, independent=False)])\n",
    "    prediction[\"output\"] = [post_pred_blending(i) for i in decollate_batch(prediction[\"output\"])]\n",
    "    selected = {\"output\": prediction[\"output\"][0], \"image\": prediction[\"image\"][0], \"label\": prediction[\"label\"][0]}\n",
    "\n",
    "    selected = Resized(keys=[\"image\", \"label\"], spatial_size=(160, 160, 160))(selected)\n",
    "    selected = Resized(keys=[\"output\"], spatial_size=(160, 160, 160))(selected)\n",
    "\n",
    "    selected = {\"output\": selected[\"output\"].unsqueeze(0), \"image\": selected[\"image\"].unsqueeze(0), \"label\": selected[\"label\"].unsqueeze(0)}\n",
    "\n",
    "    # print('true label:', selected['label'].shape)\n",
    "    pred = torch.argmax(selected['output'], dim=1).detach().cpu().numpy()\n",
    "    true_label = selected['label'][0].detach().cpu().numpy()\n",
    "    image = selected['image'][0].cpu().numpy()\n",
    "    # print('true label:', true_label.shape)\n",
    "    \n",
    "    blended_true_label = blend_images(image, true_label, alpha=0.7)\n",
    "    blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
    "\n",
    "    blended_prediction = blend_images(image, pred, alpha=0.7)\n",
    "    blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
    "\n",
    "    volume_pred = blended_final_prediction[:,:,:,:]\n",
    "    volume_label = blended_final_true_label[:,:,:,:]\n",
    "    volume_pred = np.squeeze(volume_pred).permute(3,0,1,2).cpu()\n",
    "    volume_label = np.squeeze(volume_label).permute(3,0,1,2).cpu()\n",
    "    volume_img = torch.tensor(image).permute(3,1,2,0).repeat(1,1,1,3).cpu()\n",
    "\n",
    "    volume = torch.hstack((volume_img, volume_pred, volume_label))\n",
    "\n",
    "    volume_path = _save_gif(volume.numpy(), f\"blended-{filename}\")\n",
    "       \n",
    "    \n",
    "    return volume_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_info(prediction):\n",
    "    post_label = Compose([EnsureType(), AsDiscrete(to_onehot=2)])\n",
    "\n",
    "    ground_truth = [\n",
    "            1 if (post_label(i)[2,:,:,:].cpu() == 1).any() else 0 for i in decollate_batch(prediction['label'])\n",
    "        ]\n",
    "\n",
    "    test = prediction['output'].cpu()\n",
    "    prediction_1 = torch.argmax(test, dim=1)\n",
    "\n",
    "    class_2_mask = (prediction_1 == 2).cpu()\n",
    "    if class_2_mask.any():\n",
    "        prediction = torch.max(test[:,2,:,:,:]).item()\n",
    "    else:\n",
    "        prediction = np.max(np.ma.masked_array(test[:,2,:,:,:], mask=class_2_mask))\n",
    "    \n",
    "    unique_values = torch.unique(prediction_1)\n",
    "    predicted_class = 1 if 2 in unique_values else 0\n",
    "    \n",
    "    return predicted_class, prediction, ground_truth\n",
    "\n",
    "def computeROC(predictions):\n",
    "    from sklearn.metrics import roc_curve, auc # roc curve tools\n",
    "    \n",
    "    g_truths = []\n",
    "    preds = []\n",
    "    for prediction in predictions:\n",
    "        _, predict, ground_truth = get_classification_info(prediction)\n",
    "        g_truths.extend(ground_truth)\n",
    "        preds.append(predict)\n",
    "\n",
    "    preds = np.asarray(preds)\n",
    "    ground_truth = np.asarray(g_truths)\n",
    "    fpr, tpr, _ = roc_curve(g_truths, preds)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhnD1E-uGr8c"
   },
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 0\n",
    "IMG_SIZE = (96,96,96)\n",
    "VAL_SIZE = (256,256,256)\n",
    "SAVE_PATH = \"lightning_logs/\"\n",
    "run_idx = len(os.listdir(\"wandb\"))\n",
    "RUN_NAME = f\"Predict_Segmentation_VI_{run_idx+1}\"\n",
    "pytorch_lightning.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, name):\n",
    "    file_path = \"checkpoints/\"\n",
    "    if not os.path.exists(file_path): \n",
    "        os.makedirs(file_path)\n",
    "    epoch = state[\"epoch\"]\n",
    "    save_dir = file_path + name + str(epoch)\n",
    "    torch.save(state, save_dir)\n",
    "    print(f\"Saving checkpoint for epoch {epoch} in: {save_dir}\")\n",
    "\n",
    "def save_state_dict(state, name):\n",
    "    file_path = \"checkpoints/\"\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    save_dir = file_path + f\"{name}_best\"\n",
    "    torch.save(state, save_dir)\n",
    "    print(f\"Best accuracy so far. Saving model to:{save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_and_print:\n",
    "    def __init__(self, run_name, tb_logger=None):\n",
    "        self.tb_logger = tb_logger\n",
    "        self.run_name = run_name\n",
    "        self.str_log = \"run_name\" + \"\\n  \\n\"\n",
    "\n",
    "    def lnp(self, tag):\n",
    "        print(self.run_name, time.asctime(), tag)\n",
    "        self.str_log += str(time.asctime()) + \" \" + str(tag) + \"  \\n\"\n",
    "\n",
    "    def dump_to_tensorboard(self):\n",
    "        if not self.tb_logger:\n",
    "            print(\"No tensorboard logger\")\n",
    "        self.tb_logger.experiment.add_text(\"log\", self.str_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wandb.finish()\n",
    "except:\n",
    "    print(\"Wandb not initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Segmentation_VI_305 Mon Jul 11 17:27:58 2022 Loggers start\n",
      "Predict_Segmentation_VI_305 Mon Jul 11 17:27:58 2022 ts_script: 1657553278.3470278\n"
     ]
    }
   ],
   "source": [
    "lnp = Log_and_print(RUN_NAME)\n",
    "lnp.lnp(\"Loggers start\")\n",
    "lnp.lnp(\"ts_script: \" + str(time.time()))\n",
    "\n",
    "wandb_logger = pytorch_lightning.loggers.WandbLogger(\n",
    "    project=\"traumaIA\",\n",
    "    name=RUN_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALLBACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Segmentation_VI_305 Mon Jul 11 18:04:37 2022 MAIN callbacks\n",
      "Predict_Segmentation_VI_305 Mon Jul 11 18:04:37 2022 checkpoint_dirpath: lightning_logs/checkpoints/\n",
      "Predict_Segmentation_VI_305 Mon Jul 11 18:04:37 2022 checkpoint_filename: lightning_logs_Predict_Segmentation_VI_305_Best\n",
      "Predict_Segmentation_VI_305 Mon Jul 11 18:04:37 2022 checkpoint_dirpath: lightning_logs/checkpoints/\n",
      "Predict_Segmentation_VI_305 Mon Jul 11 18:04:37 2022 checkpoint_filename: lightning_logs_Predict_Segmentation_VI_305_Last\n"
     ]
    }
   ],
   "source": [
    "lnp.lnp(\"MAIN callbacks\")\n",
    "l_callbacks = []\n",
    "cbEarlyStopping = pytorch_lightning.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=500, mode=\"max\"\n",
    ")\n",
    "l_callbacks.append(cbEarlyStopping)\n",
    "\n",
    "\n",
    "checkpoint_dirpath = SAVE_PATH + \"checkpoints/\"\n",
    "checkpoint_filename = SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"_Best\"\n",
    "lnp.lnp(\"checkpoint_dirpath: \" + checkpoint_dirpath)\n",
    "lnp.lnp(\"checkpoint_filename: \" + checkpoint_filename)\n",
    "cbModelCheckpoint = pytorch_lightning.callbacks.ModelCheckpoint(\n",
    "    monitor=\"dice_metric_injure\", mode=\"max\", dirpath=checkpoint_dirpath, filename=checkpoint_filename, \n",
    ")\n",
    "l_callbacks.append(cbModelCheckpoint)\n",
    "\n",
    "\n",
    "checkpoint_dirpath = SAVE_PATH + \"checkpoints/\"\n",
    "checkpoint_filename = SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"_Last\"\n",
    "lnp.lnp(\"checkpoint_dirpath: \" + checkpoint_dirpath)\n",
    "lnp.lnp(\"checkpoint_filename: \" + checkpoint_filename)\n",
    "cbModelCheckpointLast = pytorch_lightning.callbacks.ModelCheckpoint(\n",
    "   every_n_epochs = 1, dirpath=checkpoint_dirpath, filename=checkpoint_filename, \n",
    ")\n",
    "l_callbacks.append(cbModelCheckpointLast)\n",
    "\n",
    "l_callbacks.append(PrintTableMetricsCallback())\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "l_callbacks.append(lr_monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | _model        | UNet       | 1.6 M \n",
      "1 | loss_function | DiceCELoss | 0     \n",
      "---------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.510     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Segmentation_VI_305 Mon Jul 11 18:07:00 2022  Start Trainining process...\n",
      "len(train_files) 1105\n",
      "len(validation files) 69\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fdd3d4866f460db1ebbb5d29bc5569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[[0.3137, 0.1255, 0.3020,  ..., 0.0000, 0.2824, 0.3961],\n",
      "          [0.2039, 0.1569, 0.4039,  ..., 0.0000, 0.0196, 0.1216],\n",
      "          [0.3647, 0.2863, 0.3725,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.3098, 0.3765, 0.3176,  ..., 0.3137, 0.2510, 0.1804],\n",
      "          [0.3059, 0.2784, 0.1725,  ..., 0.1804, 0.1725, 0.2510],\n",
      "          [0.2980, 0.2549, 0.1490,  ..., 0.4863, 0.4431, 0.2157]]],\n",
      "\n",
      "\n",
      "        [[[0.3647, 0.2863, 0.3725,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3608, 0.4118, 0.3804,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3765, 0.4431, 0.3569,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.2980, 0.2549, 0.1490,  ..., 0.4863, 0.4431, 0.2157],\n",
      "          [0.2431, 0.1647, 0.0667,  ..., 0.5333, 0.3490, 0.1490],\n",
      "          [0.2745, 0.0784, 0.0000,  ..., 0.3569, 0.2784, 0.2039]]],\n",
      "\n",
      "\n",
      "        [[[0.2745, 0.3569, 0.3882,  ..., 0.8627, 0.8588, 0.9176],\n",
      "          [0.4588, 0.4863, 0.3647,  ..., 0.8471, 1.0000, 0.9961],\n",
      "          [0.4314, 0.4235, 0.3882,  ..., 0.5765, 0.7294, 0.5765],\n",
      "          ...,\n",
      "          [0.1176, 0.2353, 0.2314,  ..., 0.3412, 0.3490, 0.3294],\n",
      "          [0.0000, 0.0000, 0.2039,  ..., 0.2824, 0.3804, 0.3961],\n",
      "          [0.0000, 0.0000, 0.1373,  ..., 0.2471, 0.3804, 0.3725]]],\n",
      "\n",
      "\n",
      "        [[[0.3137, 0.1255, 0.3020,  ..., 0.0000, 0.2824, 0.3961],\n",
      "          [0.2039, 0.1569, 0.4039,  ..., 0.0000, 0.0196, 0.1216],\n",
      "          [0.3647, 0.2863, 0.3725,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.3098, 0.3765, 0.3176,  ..., 0.3137, 0.2510, 0.1804],\n",
      "          [0.3059, 0.2784, 0.1725,  ..., 0.1804, 0.1725, 0.2510],\n",
      "          [0.2980, 0.2549, 0.1490,  ..., 0.4863, 0.4431, 0.2157]]]],\n",
      "       device='cuda:0'), 'label': tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]]], device='cuda:0', dtype=torch.int32), 'image_meta_dict': {'format': ['PNG', 'PNG', 'PNG', 'PNG'], 'mode': ['RGBA', 'RGBA', 'RGBA', 'RGBA'], 'width': tensor([108, 108, 108, 108], device='cuda:0'), 'height': tensor([144, 144, 144, 144], device='cuda:0'), 'spatial_shape': tensor([[108, 144],\n",
      "        [108, 144],\n",
      "        [108, 144],\n",
      "        [108, 144]], device='cuda:0', dtype=torch.int32), 'original_channel_dim': tensor([-1, -1, -1, -1], device='cuda:0'), 'filename_or_obj': ['u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_L110016_5.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_L110016_5.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_L110016_5.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_L110016_5.png'], 'patch_index': tensor([0, 1, 2, 3], device='cuda:0')}, 'label_meta_dict': {'format': ['PNG', 'PNG', 'PNG', 'PNG'], 'mode': ['RGBA', 'RGBA', 'RGBA', 'RGBA'], 'width': tensor([108, 108, 108, 108], device='cuda:0'), 'height': tensor([144, 144, 144, 144], device='cuda:0'), 'spatial_shape': tensor([[108, 144],\n",
      "        [108, 144],\n",
      "        [108, 144],\n",
      "        [108, 144]], device='cuda:0', dtype=torch.int32), 'original_channel_dim': tensor([-1, -1, -1, -1], device='cuda:0'), 'filename_or_obj': ['u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_L110016_5.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_L110016_5.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_L110016_5.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_L110016_5.png'], 'patch_index': tensor([0, 1, 2, 3], device='cuda:0')}, 'label_transforms': [{'class': ['KeepOnlyClass', 'KeepOnlyClass', 'KeepOnlyClass', 'KeepOnlyClass'], 'id': tensor([2281342128384, 2281342128384, 2281342128384, 2281342128384],\n",
      "       device='cuda:0'), 'orig_size': [tensor([144, 144, 144, 144], device='cuda:0'), tensor([1, 1, 1, 1], device='cuda:0')]}, {'class': ['RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd'], 'id': tensor([2281342128816, 2281342128816, 2281342128816, 2281342128816],\n",
      "       device='cuda:0'), 'orig_size': [tensor([108, 108, 108, 108], device='cuda:0'), tensor([144, 144, 144, 144], device='cuda:0')], 'extra_info': {'center': [tensor([48, 50, 60, 48], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')]}}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128864, 2281342128864, 2281342128864, 2281342128864],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False,  True, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128912, 2281342128912, 2281342128912, 2281342128912],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342129152, 2281342129152, 2281342129152, 2281342129152],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandRotate90d', 'RandRotate90d', 'RandRotate90d', 'RandRotate90d'], 'id': tensor([2281342129392, 2281342129392, 2281342129392, 2281342129392],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'extra_info': {'rand_k': tensor([3, 1, 3, 2], device='cuda:0')}, 'do_transforms': tensor([False, False,  True, False], device='cuda:0')}, {'class': ['ToTensord', 'ToTensord', 'ToTensord', 'ToTensord'], 'id': tensor([2281342129632, 2281342129632, 2281342129632, 2281342129632],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')]}], 'image_transforms': [{'class': ['RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd'], 'id': tensor([2281342128816, 2281342128816, 2281342128816, 2281342128816],\n",
      "       device='cuda:0'), 'orig_size': [tensor([108, 108, 108, 108], device='cuda:0'), tensor([144, 144, 144, 144], device='cuda:0')], 'extra_info': {'center': [tensor([48, 50, 60, 48], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')]}}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128864, 2281342128864, 2281342128864, 2281342128864],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False,  True, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128912, 2281342128912, 2281342128912, 2281342128912],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342129152, 2281342129152, 2281342129152, 2281342129152],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandRotate90d', 'RandRotate90d', 'RandRotate90d', 'RandRotate90d'], 'id': tensor([2281342129392, 2281342129392, 2281342129392, 2281342129392],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'extra_info': {'rand_k': tensor([3, 1, 3, 2], device='cuda:0')}, 'do_transforms': tensor([False, False,  True, False], device='cuda:0')}, {'class': ['ToTensord', 'ToTensord', 'ToTensord', 'ToTensord'], 'id': tensor([2281342129632, 2281342129632, 2281342129632, 2281342129632],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')]}]}\n",
      "{'image': tensor([[[[0.6588, 0.6706, 0.5098,  ..., 0.0000, 0.5333, 0.4275],\n",
      "          [0.7020, 0.7098, 0.5765,  ..., 0.0000, 0.4706, 0.3137],\n",
      "          [0.8039, 0.8588, 0.6314,  ..., 0.0000, 0.4118, 0.4941],\n",
      "          ...,\n",
      "          [0.6392, 0.5294, 0.3490,  ..., 0.2039, 0.1882, 0.1569],\n",
      "          [0.6941, 0.4392, 0.1255,  ..., 0.3451, 0.4314, 0.3961],\n",
      "          [0.7725, 0.5922, 0.3137,  ..., 0.6353, 0.7176, 0.5843]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.1333,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.1255,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0784, 0.0824,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0157, 0.0000, 0.1647,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1725, 0.1412, 0.0314,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1529, 0.0392, 0.0431,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.8588, 0.5451, 0.1882,  ..., 0.6196, 1.0000, 1.0000],\n",
      "          [0.9451, 0.7412, 0.2353,  ..., 0.1412, 0.5922, 1.0000],\n",
      "          [0.7843, 0.7451, 0.3176,  ..., 0.3020, 0.6039, 0.6353],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.4275,  ..., 0.3569, 0.4824, 0.3804],\n",
      "          [0.0000, 0.1255, 0.4706,  ..., 0.4431, 0.5647, 0.4275],\n",
      "          [0.4588, 0.4745, 0.5725,  ..., 0.2314, 0.4392, 0.3804]]],\n",
      "\n",
      "\n",
      "        [[[0.1529, 0.1725, 0.0157,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0392, 0.1412, 0.0000,  ..., 0.0784, 0.0000, 0.0000],\n",
      "          [0.0431, 0.0314, 0.1647,  ..., 0.0824, 0.1255, 0.1333],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n",
      "       device='cuda:0'), 'label': tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]]], device='cuda:0', dtype=torch.int32), 'image_meta_dict': {'format': ['PNG', 'PNG', 'PNG', 'PNG'], 'mode': ['RGBA', 'RGBA', 'RGBA', 'RGBA'], 'width': tensor([132, 132, 132, 132], device='cuda:0'), 'height': tensor([107, 107, 107, 107], device='cuda:0'), 'spatial_shape': tensor([[132, 107],\n",
      "        [132, 107],\n",
      "        [132, 107],\n",
      "        [132, 107]], device='cuda:0', dtype=torch.int32), 'original_channel_dim': tensor([-1, -1, -1, -1], device='cuda:0'), 'filename_or_obj': ['u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_L110027_72.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_L110027_72.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_L110027_72.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_L110027_72.png'], 'patch_index': tensor([0, 1, 2, 3], device='cuda:0')}, 'label_meta_dict': {'format': ['PNG', 'PNG', 'PNG', 'PNG'], 'mode': ['RGBA', 'RGBA', 'RGBA', 'RGBA'], 'width': tensor([132, 132, 132, 132], device='cuda:0'), 'height': tensor([107, 107, 107, 107], device='cuda:0'), 'spatial_shape': tensor([[132, 107],\n",
      "        [132, 107],\n",
      "        [132, 107],\n",
      "        [132, 107]], device='cuda:0', dtype=torch.int32), 'original_channel_dim': tensor([-1, -1, -1, -1], device='cuda:0'), 'filename_or_obj': ['u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_L110027_72.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_L110027_72.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_L110027_72.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_L110027_72.png'], 'patch_index': tensor([0, 1, 2, 3], device='cuda:0')}, 'label_transforms': [{'class': ['KeepOnlyClass', 'KeepOnlyClass', 'KeepOnlyClass', 'KeepOnlyClass'], 'id': tensor([2281342128384, 2281342128384, 2281342128384, 2281342128384],\n",
      "       device='cuda:0'), 'orig_size': [tensor([107, 107, 107, 107], device='cuda:0'), tensor([1, 1, 1, 1], device='cuda:0')]}, {'class': ['RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd'], 'id': tensor([2281342128816, 2281342128816, 2281342128816, 2281342128816],\n",
      "       device='cuda:0'), 'orig_size': [tensor([132, 132, 132, 132], device='cuda:0'), tensor([107, 107, 107, 107], device='cuda:0')], 'extra_info': {'center': [tensor([75, 48, 84, 48], device='cuda:0'), tensor([59, 59, 54, 59], device='cuda:0')]}}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128864, 2281342128864, 2281342128864, 2281342128864],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False,  True, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128912, 2281342128912, 2281342128912, 2281342128912],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342129152, 2281342129152, 2281342129152, 2281342129152],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandRotate90d', 'RandRotate90d', 'RandRotate90d', 'RandRotate90d'], 'id': tensor([2281342129392, 2281342129392, 2281342129392, 2281342129392],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'extra_info': {'rand_k': tensor([1, 3, 1, 3], device='cuda:0')}, 'do_transforms': tensor([False, False, False,  True], device='cuda:0')}, {'class': ['ToTensord', 'ToTensord', 'ToTensord', 'ToTensord'], 'id': tensor([2281342129632, 2281342129632, 2281342129632, 2281342129632],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')]}], 'image_transforms': [{'class': ['RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd'], 'id': tensor([2281342128816, 2281342128816, 2281342128816, 2281342128816],\n",
      "       device='cuda:0'), 'orig_size': [tensor([132, 132, 132, 132], device='cuda:0'), tensor([107, 107, 107, 107], device='cuda:0')], 'extra_info': {'center': [tensor([75, 48, 84, 48], device='cuda:0'), tensor([59, 59, 54, 59], device='cuda:0')]}}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128864, 2281342128864, 2281342128864, 2281342128864],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False,  True, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128912, 2281342128912, 2281342128912, 2281342128912],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342129152, 2281342129152, 2281342129152, 2281342129152],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandRotate90d', 'RandRotate90d', 'RandRotate90d', 'RandRotate90d'], 'id': tensor([2281342129392, 2281342129392, 2281342129392, 2281342129392],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'extra_info': {'rand_k': tensor([1, 3, 1, 3], device='cuda:0')}, 'do_transforms': tensor([False, False, False,  True], device='cuda:0')}, {'class': ['ToTensord', 'ToTensord', 'ToTensord', 'ToTensord'], 'id': tensor([2281342129632, 2281342129632, 2281342129632, 2281342129632],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')]}]}\n",
      "{'image': tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.5020, 0.5059, 0.4431],\n",
      "          [0.0000, 0.0000, 0.0431,  ..., 0.5843, 0.6980, 0.5882],\n",
      "          [0.0000, 0.0000, 0.1569,  ..., 0.5569, 0.6275, 0.5529],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2784, 0.1569, 0.2549,  ..., 0.6784, 0.7804, 0.6353],\n",
      "          [0.0000, 0.1333, 0.2863,  ..., 0.7529, 0.7529, 0.5020],\n",
      "          [0.0392, 0.0471, 0.2784,  ..., 0.7922, 0.5098, 0.6314],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.6471,  ..., 0.5843, 0.8588, 0.7137],\n",
      "          [0.0039, 0.5059, 0.8118,  ..., 0.6000, 0.6510, 0.6000],\n",
      "          [0.4549, 0.7922, 0.8902,  ..., 0.5569, 0.5137, 0.6824]]],\n",
      "\n",
      "\n",
      "        [[[0.2980, 0.4000, 0.3922,  ..., 0.5333, 0.0000, 0.0000],\n",
      "          [0.2863, 0.3569, 0.5647,  ..., 1.0000, 0.3725, 0.3804],\n",
      "          [0.2784, 0.3176, 0.4667,  ..., 1.0000, 1.0000, 0.5608],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.7882, 1.0000, 1.0000,  ..., 0.3725, 0.1882, 0.2549],\n",
      "          [0.4118, 0.4745, 0.4118,  ..., 0.3608, 0.3020, 0.3098],\n",
      "          [0.2471, 0.3373, 0.2471,  ..., 0.3176, 0.4431, 0.4353],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n",
      "       device='cuda:0'), 'label': tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]]], device='cuda:0', dtype=torch.int32), 'image_meta_dict': {'format': ['PNG', 'PNG', 'PNG', 'PNG'], 'mode': ['RGBA', 'RGBA', 'RGBA', 'RGBA'], 'width': tensor([120, 120, 120, 120], device='cuda:0'), 'height': tensor([154, 154, 154, 154], device='cuda:0'), 'spatial_shape': tensor([[120, 154],\n",
      "        [120, 154],\n",
      "        [120, 154],\n",
      "        [120, 154]], device='cuda:0', dtype=torch.int32), 'original_channel_dim': tensor([-1, -1, -1, -1], device='cuda:0'), 'filename_or_obj': ['u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_M110135_14.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_M110135_14.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_M110135_14.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\imagesTr\\\\VI_M110135_14.png'], 'patch_index': tensor([0, 1, 2, 3], device='cuda:0')}, 'label_meta_dict': {'format': ['PNG', 'PNG', 'PNG', 'PNG'], 'mode': ['RGBA', 'RGBA', 'RGBA', 'RGBA'], 'width': tensor([120, 120, 120, 120], device='cuda:0'), 'height': tensor([154, 154, 154, 154], device='cuda:0'), 'spatial_shape': tensor([[120, 154],\n",
      "        [120, 154],\n",
      "        [120, 154],\n",
      "        [120, 154]], device='cuda:0', dtype=torch.int32), 'original_channel_dim': tensor([-1, -1, -1, -1], device='cuda:0'), 'filename_or_obj': ['u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_M110135_14.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_M110135_14.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_M110135_14.png', 'u:\\\\lauraalvarez\\\\data\\\\vascular_injuries\\\\png\\\\labelsTr\\\\VI_M110135_14.png'], 'patch_index': tensor([0, 1, 2, 3], device='cuda:0')}, 'label_transforms': [{'class': ['KeepOnlyClass', 'KeepOnlyClass', 'KeepOnlyClass', 'KeepOnlyClass'], 'id': tensor([2281342128384, 2281342128384, 2281342128384, 2281342128384],\n",
      "       device='cuda:0'), 'orig_size': [tensor([154, 154, 154, 154], device='cuda:0'), tensor([1, 1, 1, 1], device='cuda:0')]}, {'class': ['RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd'], 'id': tensor([2281342128816, 2281342128816, 2281342128816, 2281342128816],\n",
      "       device='cuda:0'), 'orig_size': [tensor([120, 120, 120, 120], device='cuda:0'), tensor([154, 154, 154, 154], device='cuda:0')], 'extra_info': {'center': [tensor([48, 72, 48, 48], device='cuda:0'), tensor([99, 48, 69, 48], device='cuda:0')]}}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128864, 2281342128864, 2281342128864, 2281342128864],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128912, 2281342128912, 2281342128912, 2281342128912],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342129152, 2281342129152, 2281342129152, 2281342129152],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandRotate90d', 'RandRotate90d', 'RandRotate90d', 'RandRotate90d'], 'id': tensor([2281342129392, 2281342129392, 2281342129392, 2281342129392],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'extra_info': {'rand_k': tensor([2, 3, 1, 3], device='cuda:0')}, 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['ToTensord', 'ToTensord', 'ToTensord', 'ToTensord'], 'id': tensor([2281342129632, 2281342129632, 2281342129632, 2281342129632],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')]}], 'image_transforms': [{'class': ['RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd', 'RandCropByLabelClassesd'], 'id': tensor([2281342128816, 2281342128816, 2281342128816, 2281342128816],\n",
      "       device='cuda:0'), 'orig_size': [tensor([120, 120, 120, 120], device='cuda:0'), tensor([154, 154, 154, 154], device='cuda:0')], 'extra_info': {'center': [tensor([48, 72, 48, 48], device='cuda:0'), tensor([99, 48, 69, 48], device='cuda:0')]}}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128864, 2281342128864, 2281342128864, 2281342128864],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342128912, 2281342128912, 2281342128912, 2281342128912],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandFlipd', 'RandFlipd', 'RandFlipd', 'RandFlipd'], 'id': tensor([2281342129152, 2281342129152, 2281342129152, 2281342129152],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['RandRotate90d', 'RandRotate90d', 'RandRotate90d', 'RandRotate90d'], 'id': tensor([2281342129392, 2281342129392, 2281342129392, 2281342129392],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')], 'extra_info': {'rand_k': tensor([2, 3, 1, 3], device='cuda:0')}, 'do_transforms': tensor([False, False, False, False], device='cuda:0')}, {'class': ['ToTensord', 'ToTensord', 'ToTensord', 'ToTensord'], 'id': tensor([2281342129632, 2281342129632, 2281342129632, 2281342129632],\n",
      "       device='cuda:0'), 'orig_size': [tensor([96, 96, 96, 96], device='cuda:0'), tensor([96, 96, 96, 96], device='cuda:0')]}]}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "applying transform <monai.transforms.compose.Compose object at 0x000002132A965670>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\transforms\\transform.py:89\u001b[0m, in \u001b[0;36mapply_transform\u001b[1;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[39mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data]\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items)\n\u001b[0;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\transforms\\transform.py:53\u001b[0m, in \u001b[0;36m_apply_transform\u001b[1;34m(transform, parameters, unpack_parameters)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m transform(\u001b[39m*\u001b[39mparameters)\n\u001b[1;32m---> 53\u001b[0m \u001b[39mreturn\u001b[39;00m transform(parameters)\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\transforms\\croppad\\dictionary.py:1346\u001b[0m, in \u001b[0;36mRandCropByLabelClassesd.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1344\u001b[0m indices \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39mpop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices_key, \u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices_key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1346\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandomize(label, indices, image)\n\u001b[0;32m   1347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspatial_size, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\transforms\\croppad\\dictionary.py:1336\u001b[0m, in \u001b[0;36mRandCropByLabelClassesd.randomize\u001b[1;34m(self, label, indices, image)\u001b[0m\n\u001b[0;32m   1335\u001b[0m     indices_ \u001b[39m=\u001b[39m indices\n\u001b[1;32m-> 1336\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcenters \u001b[39m=\u001b[39m generate_label_classes_crop_centers(\n\u001b[0;32m   1337\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspatial_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_samples, label\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m:], indices_, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mratios, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mR, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallow_smaller\n\u001b[0;32m   1338\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\transforms\\utils.py:562\u001b[0m, in \u001b[0;36mgenerate_label_classes_crop_centers\u001b[1;34m(spatial_size, num_samples, label_spatial_shape, indices, ratios, rand_state, allow_smaller)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[39m# shift center to range of valid centers\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m     centers\u001b[39m.\u001b[39mappend(correct_crop_centers(center, spatial_size, label_spatial_shape, allow_smaller))\n\u001b[0;32m    564\u001b[0m \u001b[39mreturn\u001b[39;00m centers\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\transforms\\utils.py:434\u001b[0m, in \u001b[0;36mcorrect_crop_centers\u001b[1;34m(centers, spatial_size, label_spatial_shape, allow_smaller)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_smaller:\n\u001b[1;32m--> 434\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe size of the proposed random crop ROI is larger than the image size.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    435\u001b[0m spatial_size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mmin\u001b[39m(l, s) \u001b[39mfor\u001b[39;00m l, s \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(label_spatial_shape, spatial_size))\n",
      "\u001b[1;31mValueError\u001b[0m: The size of the proposed random crop ROI is larger than the image size.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\transforms\\transform.py:89\u001b[0m, in \u001b[0;36mapply_transform\u001b[1;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[39mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data]\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items)\n\u001b[0;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\transforms\\transform.py:53\u001b[0m, in \u001b[0;36m_apply_transform\u001b[1;34m(transform, parameters, unpack_parameters)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m transform(\u001b[39m*\u001b[39mparameters)\n\u001b[1;32m---> 53\u001b[0m \u001b[39mreturn\u001b[39;00m transform(parameters)\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\transforms\\compose.py:173\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, input_)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mfor\u001b[39;00m _transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m--> 173\u001b[0m     input_ \u001b[39m=\u001b[39m apply_transform(_transform, input_, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap_items, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munpack_items, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_stats)\n\u001b[0;32m    174\u001b[0m \u001b[39mreturn\u001b[39;00m input_\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\transforms\\transform.py:113\u001b[0m, in \u001b[0;36mapply_transform\u001b[1;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[0;32m    112\u001b[0m         _log_stats(data\u001b[39m=\u001b[39mdata)\n\u001b[1;32m--> 113\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapplying transform \u001b[39m\u001b[39m{\u001b[39;00mtransform\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: applying transform <monai.transforms.croppad.dictionary.RandCropByLabelClassesd object at 0x000002132A9652B0>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mu:\\lauraalvarez\\traumaAI\\Liver_Segmentation\\vascular-injuries-segmentation-2d.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular-injuries-segmentation-2d.ipynb#ch0000025?line=9'>10</a>\u001b[0m trainer \u001b[39m=\u001b[39m pytorch_lightning\u001b[39m.\u001b[39mTrainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular-injuries-segmentation-2d.ipynb#ch0000025?line=10'>11</a>\u001b[0m     default_root_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlightning_logs/checkpoints\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular-injuries-segmentation-2d.ipynb#ch0000025?line=11'>12</a>\u001b[0m     gpus\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular-injuries-segmentation-2d.ipynb#ch0000025?line=19'>20</a>\u001b[0m     callbacks\u001b[39m=\u001b[39ml_callbacks,\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular-injuries-segmentation-2d.ipynb#ch0000025?line=20'>21</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular-injuries-segmentation-2d.ipynb#ch0000025?line=22'>23</a>\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular-injuries-segmentation-2d.ipynb#ch0000025?line=23'>24</a>\u001b[0m result_pred2 \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mfit(net)\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular-injuries-segmentation-2d.ipynb#ch0000025?line=24'>25</a>\u001b[0m wandb\u001b[39m.\u001b[39malert(\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular-injuries-segmentation-2d.ipynb#ch0000025?line=25'>26</a>\u001b[0m     title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain finished\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular-injuries-segmentation-2d.ipynb#ch0000025?line=26'>27</a>\u001b[0m     text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe train has finished\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/lauraalvarez/traumaAI/Liver_Segmentation/vascular-injuries-segmentation-2d.ipynb#ch0000025?line=27'>28</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[0;32m    735\u001b[0m     rank_zero_deprecation(\n\u001b[0;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    738\u001b[0m     )\n\u001b[0;32m    739\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[1;32m--> 740\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    741\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    742\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 685\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    686\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[0;32m    776\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m--> 777\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    779\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    780\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m   1198\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[1;32m-> 1199\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[0;32m   1201\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1279\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1288\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1319\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1318\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:234\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m data_fetcher \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(dataloader)\n\u001b[0;32m    233\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(data_fetcher)\n\u001b[0;32m    236\u001b[0m     \u001b[39m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[39m# as they expect that the same step is used when logging epoch end metrics even when the batch loop has\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     \u001b[39m# finished. this means the attribute does not exactly track the number of optimizer steps applied.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[39m# TODO(@carmocca): deprecate and rename so users don't get confused\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:156\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_check_val_fx(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch):\n\u001b[0;32m    153\u001b[0m     \u001b[39m# skip training and run validation in `on_advance_end`\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m batch_idx, (batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch) \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataloader_iter)\n\u001b[0;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mtrain_data_fetcher\u001b[39m.\u001b[39mstore_on_device:\n\u001b[0;32m    159\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mtraining_batch_to_device\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:203\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:270\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     yield_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpop_batch()\n\u001b[1;32m--> 270\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch()\n\u001b[0;32m    272\u001b[0m     \u001b[39m# wait for batch to be available.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait()\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:300\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[0;32m    299\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_profiler(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfetch_next_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstage\u001b[39m}\u001b[39;00m\u001b[39m_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 300\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[0;32m    301\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_end(batch, data)\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:550\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    545\u001b[0m     \u001b[39m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \n\u001b[0;32m    547\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[39m        a collections of batch data\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_iters)\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:562\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[1;34m(loader_iters)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    553\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    554\u001b[0m     \u001b[39m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \n\u001b[0;32m    556\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39m        Any: a collections of batch data\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_to_collection(loader_iters, Iterator, \u001b[39mnext\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\pytorch_lightning\\utilities\\apply_func.py:96\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[1;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[0;32m    100\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\data\\dataset.py:97\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(index, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[0;32m     95\u001b[0m     \u001b[39m# dataset[[1, 3, 4]]\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m Subset(dataset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, indices\u001b[39m=\u001b[39mindex)\n\u001b[1;32m---> 97\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(index)\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\data\\dataset.py:83\u001b[0m, in \u001b[0;36mDataset._transform\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39mFetch single data item from `self.data`.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m data_i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index]\n\u001b[1;32m---> 83\u001b[0m \u001b[39mreturn\u001b[39;00m apply_transform(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform, data_i) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m data_i\n",
      "File \u001b[1;32mc:\\Users\\Vincent Stirler\\.conda\\envs\\env_trauma\\lib\\site-packages\\monai\\transforms\\transform.py:113\u001b[0m, in \u001b[0;36mapply_transform\u001b[1;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         _log_stats(data\u001b[39m=\u001b[39mdata)\n\u001b[1;32m--> 113\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapplying transform \u001b[39m\u001b[39m{\u001b[39;00mtransform\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: applying transform <monai.transforms.compose.Compose object at 0x000002132A965670>"
     ]
    }
   ],
   "source": [
    "# initialise the LightningModule\n",
    "lnp.lnp(\" Start Trainining process...\")\n",
    "net = Net(train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=1)#.load_from_checkpoint(\"lightning_logs/checkpoints/lightning_logs_Predict_Segmentation_222_Last.ckpt\", train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=3)\n",
    "wandb_logger.watch(net)\n",
    "\n",
    "# set up loggers and checkpoints\n",
    "log_dir = os.path.join(root_dir, \"logs\")\n",
    "\n",
    "# initialise Lightning's trainer.\n",
    "trainer = pytorch_lightning.Trainer(\n",
    "    default_root_dir=\"lightning_logs/checkpoints\",\n",
    "    gpus=[0],\n",
    "    max_epochs=1000,\n",
    "    # fast_dev_run=True,\n",
    "    auto_lr_find=False,\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=l_callbacks,\n",
    "    gradient_clip_val=5\n",
    "    \n",
    ")\n",
    "\n",
    "# train\n",
    "result_pred2 = trainer.fit(net)\n",
    "wandb.alert(\n",
    "    title=\"Train finished\",\n",
    "    text=\"The train has finished\"\n",
    ")\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rmdir(net.tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net(train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=3).load_from_checkpoint(\"lightning_logs/checkpoints/lightning_logs_Predict_Segmentation_168_Best.ckpt\", train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pytorch_lightning.Trainer(\n",
    "#     default_root_dir=\"lightning_logs/checkpoints\",\n",
    "#     gpus=[0],\n",
    "#     max_epochs=1000,\n",
    "#     auto_lr_find=False,\n",
    "#     logger=wandb_logger,\n",
    "#     enable_checkpointing=True,\n",
    "#     num_sanity_val_steps=0,\n",
    "#     log_every_n_steps=1,\n",
    "#     callbacks=l_callbacks,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = trainer.predict(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_pred = Compose([EnsureType(), AsDiscrete(argmax=True),KeepLargestConnectedComponent([1,2], is_onehot=False, independent=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_one = [post_pred(i) for i in decollate_batch(predictions[1][\"prediction\"][\"output\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _save_gif(volume, filename):\n",
    "#     volume = volume.astype(np.float64) / np.max(volume) # normalize the data to 0 - 1\n",
    "#     volume = 255 * volume # Now scale by 255\n",
    "#     volume = volume.astype(np.uint8)\n",
    "#     path_to_gif = os.path.join(\"gifs\", f'{filename}.gif')\n",
    "#     if not os.path.exists(\"gifs\"):\n",
    "#         print(\"Creating gifs directory\")\n",
    "#         os.mkdir(\"gifs\")\n",
    "#     imageio.mimsave(path_to_gif, volume)\n",
    "#     return path_to_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _save_gif(output_one[0].permute(3,1,2,0).cpu().numpy(), \"test-after\")\n",
    "# _save_gif(predictions[1][\"prediction\"][\"output\"][0].permute(3,1,2,0).cpu().numpy(), \"test-before\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests Gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = predictions[1][\"prediction\"]\n",
    "# selected = {\"output\":  output_one[0], \"image\": prediction[\"image\"][0], \"label\": prediction[\"label\"][0]}\n",
    "# selected = Resized(keys=[\"image\", \"label\"], spatial_size=(160, 160, 160))(selected)\n",
    "# selected = Resized(keys=[\"output\"], spatial_size=(160, 160, 160))(selected)\n",
    "# selected = {\"output\": selected[\"output\"].unsqueeze(0), \"image\": selected[\"image\"].unsqueeze(0), \"label\": selected[\"label\"].unsqueeze(0)}\n",
    "# # print('true label:', selected['label'].shape)\n",
    "# pred = selected[\"output\"][0].detach().cpu().numpy()\n",
    "# true_label = selected['label'][0].detach().cpu().numpy()\n",
    "# image = selected['image'][0].cpu().numpy()\n",
    "# # print('true label:', true_label.shape)\n",
    "\n",
    "# blended_true_label = blend_images(image, true_label, alpha=0.3)\n",
    "# blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
    "\n",
    "# blended_prediction = blend_images(image, pred, alpha=0.3)\n",
    "# blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
    "\n",
    "# volume_pred = blended_final_prediction[:,:,:,:]\n",
    "# volume_label = blended_final_true_label[:,:,:,:]\n",
    "# volume_pred = np.squeeze(volume_pred).permute(3,0,1,2).cpu()\n",
    "# volume_label = np.squeeze(volume_label).permute(3,0,1,2).cpu()\n",
    "# volume_img = torch.tensor(image).permute(3,1,2,0).repeat(1,1,1,3).cpu()\n",
    "\n",
    "# volume = torch.hstack((volume_img, volume_pred, volume_label))\n",
    "\n",
    "# volume_path = _save_gif(volume.numpy(), f\"blended-test-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = predictions[1][\"prediction\"]\n",
    "\n",
    "# selected = {\"output\": prediction[\"output\"][0], \"image\": prediction[\"image\"][0], \"label\": prediction[\"label\"][0]}\n",
    "# selected = Resized(keys=[\"image\", \"label\"], spatial_size=(160, 160, 160))(selected)\n",
    "# selected = Resized(keys=[\"output\"], spatial_size=(160, 160, 160))(selected)\n",
    "# selected = {\"output\": torch.argmax(selected[\"output\"], 0).unsqueeze(0), \"image\": selected[\"image\"].unsqueeze(0), \"label\": selected[\"label\"].unsqueeze(0)}\n",
    "\n",
    "# pred = selected[\"output\"].detach().cpu().numpy()\n",
    "# true_label = selected['label'][0].detach().cpu().numpy()\n",
    "# image = selected['image'][0].cpu().numpy()\n",
    "\n",
    "# blended_true_label = blend_images(image, true_label, alpha=0.3)\n",
    "# blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
    "\n",
    "# blended_prediction = blend_images(image, pred, alpha=0.3)\n",
    "# blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
    "\n",
    "# volume_pred = blended_final_prediction[:,:,:,:]\n",
    "# volume_label = blended_final_true_label[:,:,:,:]\n",
    "# volume_pred = np.squeeze(volume_pred).permute(3,0,1,2).cpu()\n",
    "# volume_label = np.squeeze(volume_label).permute(3,0,1,2).cpu()\n",
    "# volume_img = torch.tensor(image).permute(3,1,2,0).repeat(1,1,1,3).cpu()\n",
    "\n",
    "# volume = torch.hstack((volume_img, volume_pred, volume_label))\n",
    "\n",
    "# volume_path = _save_gif(volume.numpy(), f\"blended-test-1-org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"_Last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"Last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spleen_segmentation_3d_lightning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd953d996cbefa82e0aa9bb4569a5a9d74ea146024a460b967761e6c3fe194fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
