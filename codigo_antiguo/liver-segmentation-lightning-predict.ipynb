{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djzZ-bffGr8X"
   },
   "source": [
    "#### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "biFwQV_yGr8X",
    "outputId": "caa6e216-4409-4de4-bc5a-ceec09bf6af7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.8.1\n",
      "Numpy version: 1.22.3\n",
      "Pytorch version: 1.11.0\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 71ff399a3ea07aef667b23653620a290364095b1\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.8\n",
      "Nibabel version: 3.2.2\n",
      "scikit-image version: 0.19.2\n",
      "Pillow version: 9.0.1\n",
      "Tensorboard version: 2.9.0\n",
      "gdown version: 4.4.0\n",
      "TorchVision version: 0.12.0\n",
      "tqdm version: 4.64.0\n",
      "lmdb version: 1.3.0\n",
      "psutil version: 5.9.0\n",
      "pandas version: 1.4.2\n",
      "einops version: 0.3.2\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from monai.data.image_reader import ImageReader, ITKReader\n",
    "from ipywidgets.widgets import *\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning\n",
    "from monai.utils import set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureType,\n",
    "    EnsureChannelFirstd,\n",
    "    RandFlipd,\n",
    "    RandRotated,\n",
    "    ToTensord,\n",
    "    Resized,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandRotate90d,\n",
    "    RandShiftIntensityd,\n",
    "    KeepLargestConnectedComponent,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandCropByLabelClassesd,\n",
    "    SpatialPadd,\n",
    "    RandGaussianNoised,\n",
    "    RandScaleIntensityd,\n",
    "    \n",
    ")\n",
    "\n",
    "# from monailabel.scribbles.transforms import (\n",
    "#     MakeISegUnaryd,\n",
    "#     ApplyCRFOptimisationd,\n",
    "# )\n",
    "from monai.networks.blocks import CRF\n",
    "import wandb\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceFocalLoss, GeneralizedDiceLoss\n",
    "from monai.losses import DiceLoss, DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, list_data_collate, decollate_batch, Dataset, LMDBDataset\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from monai.data import DataLoader\n",
    "import os\n",
    "import glob\n",
    "from monai.transforms.spatial.array import Resize\n",
    "\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from monai.config import DtypeLike, KeysCollection\n",
    "from monai.config.type_definitions import NdarrayOrTensor\n",
    "from monai.networks.layers import AffineTransform\n",
    "from monai.networks.layers.simplelayers import GaussianFilter\n",
    "from monai.transforms.croppad.array import CenterSpatialCrop, SpatialPad\n",
    "from monai.transforms.inverse import InvertibleTransform\n",
    "from monai.transforms.spatial.array import (\n",
    "    Resize,\n",
    ")\n",
    "from monai.transforms.transform import MapTransform, RandomizableTransform\n",
    "from monai.transforms.utils import create_grid\n",
    "from monai.utils import (\n",
    "    InterpolateMode,\n",
    "    ensure_tuple_rep,\n",
    ")\n",
    "from monai.utils.deprecate_utils import deprecated_arg\n",
    "from monai.utils.enums import TraceKeys\n",
    "from monai.utils.module import optional_import\n",
    "from monai.utils.type_conversion import convert_data_type, convert_to_dst_type\n",
    "from monai.apps import load_from_mmar\n",
    "from monai.apps.mmars import RemoteMMARKeys\n",
    "from monai.networks.utils import copy_model_state\n",
    "from monai.optimizers import generate_param_groups\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from pl_bolts.callbacks import PrintTableMetricsCallback\n",
    "from monai.visualize import matshow3d, blend_images\n",
    "import imageio\n",
    "print_config()\n",
    "from monai.losses import GeneralizedWassersteinDiceLoss\n",
    "import random\n",
    "\n",
    "from monai.transforms.intensity.array import (\n",
    "    ScaleIntensityRange,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolateMode(Enum):\n",
    "    NEAREST = \"nearest\"\n",
    "    LINEAR = \"linear\"\n",
    "    BILINEAR = \"bilinear\"\n",
    "    BICUBIC = \"bicubic\"\n",
    "    TRILINEAR = \"trilinear\"\n",
    "    AREA = \"area\"\n",
    "\n",
    "\n",
    "InterpolateModeSequence = Union[\n",
    "    Sequence[Union[InterpolateMode, str]], InterpolateMode, str\n",
    "]\n",
    "\n",
    "class ResizedC(MapTransform, InvertibleTransform):\n",
    "\n",
    "    backend = Resize.backend\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        spatial_size: Union[Sequence[int], int],\n",
    "        size_mode: str = \"all\",\n",
    "        mode: InterpolateModeSequence = InterpolateMode.AREA,\n",
    "        align_corners: Union[Sequence[Optional[bool]], Optional[bool]] = None,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.mode = ensure_tuple_rep(mode, len(self.keys))\n",
    "        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))\n",
    "        self.resizer = Resize(spatial_size=spatial_size, size_mode=size_mode)\n",
    "        self.spatial_size = spatial_size\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key, mode, align_corners in self.key_iterator(\n",
    "            d, self.mode, self.align_corners\n",
    "        ):\n",
    "            self.push_transform(\n",
    "                d,\n",
    "                key,\n",
    "                extra_info={\n",
    "                    \"mode\": mode.value if isinstance(mode, Enum) else mode,\n",
    "                    \"align_corners\": align_corners\n",
    "                    if align_corners is not None\n",
    "                    else TraceKeys.NONE,\n",
    "                },\n",
    "            )\n",
    "            init_slice = int(d[key].shape[-1]*0.15)\n",
    "            end_slice = int(d[key].shape[-1]*0.1)\n",
    "            # Reduce Size in Memory\n",
    "            if key == \"label\":\n",
    "                d[key] = d[key].astype(np.int8)\n",
    "                if d[key].shape[-1] > 600: d[key] = d[key][:,:,:,init_slice:-end_slice] #\n",
    "\n",
    "                if d[\"image_meta_dict\"].get(\"PatientName\", None) and d[\"image_meta_dict\"][\"PatientName\"].startswith(\"NI\") and len(d[key].shape) != 4:\n",
    "                    # print(d[key].shape)\n",
    "                    liver_channel = np.where((d[key] != 6), 0, d[key])\n",
    "                    liver_channel = np.where((liver_channel == 6), 1, liver_channel)\n",
    "                    # liver_channel = np.expand_dims(liver_channel, 0)\n",
    "                    w, h, z = self.spatial_size\n",
    "                    liver_channel = self.resizer(liver_channel, align_corners=align_corners)\n",
    "                    background = np.ones((1, z, w, h), dtype=np.float16) - liver_channel\n",
    "                    empty_injures = np.zeros((1, z, w, h), dtype=np.float16)\n",
    "                    resized = [background, liver_channel, empty_injures]\n",
    "                    d[key] = np.stack(resized).astype(np.int8).squeeze()\n",
    "\n",
    "                else:\n",
    "                    label = d[key]\n",
    "                    w, h, z = self.spatial_size\n",
    "                    resized = list()\n",
    "                    background = np.ones((1, w, h, z), dtype=np.int8)\n",
    "                    for i, channel in enumerate([0, 2]):  # TODO: desharcodead\n",
    "                        resized.append(\n",
    "                            self.resizer(\n",
    "                                np.expand_dims(label[channel, :, :, :], 0),\n",
    "                                align_corners=align_corners,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                    background -= resized[0] # + resized[1]\n",
    "                    resized = [background] + resized\n",
    "                    d[key] = np.stack(resized).astype(np.int8).squeeze()\n",
    "            else:\n",
    "                if d[key].shape[-1] > 600: d[key] = d[key][:,:,:,init_slice:-end_slice]\n",
    "                d[key] = self.resizer(d[key], align_corners=align_corners)\n",
    "\n",
    "        keys = ['spacing', 'original_affine', 'affine', 'spatial_shape', 'original_channel_dim', 'filename_or_obj']\n",
    "        new_label_metadata = dict()\n",
    "        for key in keys:\n",
    "            new_label_metadata[key] = d[\"label_meta_dict\"].get(key, 0)\n",
    "\n",
    "        d[\"label_meta_dict\"] = new_label_metadata\n",
    "\n",
    "        if \"PatientID\" not in d[\"image_meta_dict\"]:\n",
    "            d[\"image_meta_dict\"][\"PatientID\"] = \"0\"\n",
    "        if \"PatientName\" not in d[\"image_meta_dict\"]:\n",
    "            d[\"image_meta_dict\"][\"PatientName\"] = \"0\"\n",
    "        if \"SliceThickness\" not in d[\"image_meta_dict\"]:\n",
    "            d[\"image_meta_dict\"][\"SliceThickness\"] = \"0\"\n",
    "        return d\n",
    "\n",
    "    def inverse(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            transform = self.get_most_recent_transform(d, key)\n",
    "            orig_size = transform[TraceKeys.ORIG_SIZE]\n",
    "            mode = transform[TraceKeys.EXTRA_INFO][\"mode\"]\n",
    "            align_corners = transform[TraceKeys.EXTRA_INFO][\"align_corners\"]\n",
    "            # Create inverse transform\n",
    "            inverse_transform = Resize(\n",
    "                spatial_size=orig_size,\n",
    "                mode=mode,\n",
    "                align_corners=None\n",
    "                if align_corners == TraceKeys.NONE\n",
    "                else align_corners,\n",
    "            )\n",
    "            # Apply inverse transform\n",
    "            d[key] = inverse_transform(d[key])\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adaptOverlay(MapTransform, InvertibleTransform):\n",
    "\n",
    "    backend = Resize.backend\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        size_mode: str = \"all\",\n",
    "        mode: InterpolateModeSequence = InterpolateMode.AREA,\n",
    "        align_corners: Union[Sequence[Optional[bool]], Optional[bool]] = None,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.mode = ensure_tuple_rep(mode, len(self.keys))\n",
    "        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))\n",
    "\n",
    "    def __adapt_overlay__(self, overlay_path, mha_path, label):\n",
    "        import SimpleITK as sitk\n",
    "        if label.shape[-1] == 6:\n",
    "            return label\n",
    "        # Load the mha\n",
    "        mha_data = sitk.ReadImage(mha_path)\n",
    "        mha_org = mha_data.GetOrigin()[-1]\n",
    "        # Load the mha image\n",
    "        mha_img = sitk.GetArrayFromImage(mha_data)\n",
    "        original_z_size = mha_img.shape[0]\n",
    "\n",
    "        # Load the overlay\n",
    "        overlay_data = sitk.ReadImage(overlay_path)\n",
    "        overlay_org = overlay_data.GetOrigin()[-1]\n",
    "\n",
    "        overlay_init = np.abs(1/mha_data.GetSpacing()[-1]*(mha_org-overlay_org) )\n",
    "\n",
    "        lower_bound = int(overlay_init)\n",
    "        upper_bound = label.shape[-1]\n",
    "        zeros_up = lower_bound\n",
    "        zeros_down = original_z_size - (upper_bound + lower_bound)\n",
    "        new = list()\n",
    "\n",
    "        if zeros_up > 0:\n",
    "            new.append(np.zeros((label.shape[0], label.shape[1], zeros_up), dtype=label.dtype))\n",
    "\n",
    "        new.append(label)\n",
    "\n",
    "        if zeros_down > 0:\n",
    "            new.append(np.zeros((label.shape[0], label.shape[1], zeros_down), dtype=label.dtype))\n",
    "\n",
    "        label = np.concatenate(new, axis=2)\n",
    "\n",
    "        return label\n",
    "\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key, mode, align_corners in self.key_iterator(\n",
    "            d, self.mode, self.align_corners\n",
    "        ):\n",
    "            self.push_transform(\n",
    "                d,\n",
    "                key,\n",
    "                extra_info={\n",
    "                    \"mode\": mode.value if isinstance(mode, Enum) else mode,\n",
    "                    \"align_corners\": align_corners\n",
    "                    if align_corners is not None\n",
    "                    else TraceKeys.NONE,\n",
    "                },\n",
    "            )\n",
    "            # Reduce Size in Memory\n",
    "            if key == \"label\":\n",
    "                d[key] = d[key].astype(np.int8)\n",
    "                if d[\"image_meta_dict\"].get(\"PatientName\", None) and d[\"image_meta_dict\"][\"PatientName\"].startswith(\"NI\"):\n",
    "                    file_path = d[\"label_meta_dict\"][\"filename_or_obj\"]\n",
    "                    data_path = d[\"image_meta_dict\"][\"filename_or_obj\"]\n",
    "                    d[key] = self.__adapt_overlay__(file_path, data_path, d[key])\n",
    "        return d\n",
    "\n",
    "    def inverse(\n",
    "        self, data: Mapping[Hashable, NdarrayOrTensor]\n",
    "    ) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            transform = self.get_most_recent_transform(d, key)\n",
    "            orig_size = transform[TraceKeys.ORIG_SIZE]\n",
    "            mode = transform[TraceKeys.EXTRA_INFO][\"mode\"]\n",
    "            align_corners = transform[TraceKeys.EXTRA_INFO][\"align_corners\"]\n",
    "            # Create inverse transform\n",
    "            inverse_transform = Resize(\n",
    "                spatial_size=orig_size,\n",
    "                mode=mode,\n",
    "                align_corners=None\n",
    "                if align_corners == TraceKeys.NONE\n",
    "                else align_corners,\n",
    "            )\n",
    "            # Apply inverse transform\n",
    "            d[key] = inverse_transform(d[key])\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeepOnlyClass(MapTransform, InvertibleTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        class_to_keep: int,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.class_to_keep = class_to_keep\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        print(d[\"label\"].shape)\n",
    "        for key in self.key_iterator(d):\n",
    "            self.push_transform(d, key)\n",
    "            d[key] = np.where((d[key] != self.class_to_keep), 0, d[key])\n",
    "            d[key] = np.where((d[key] == self.class_to_keep), 1, d[key])\n",
    "            d[key] = d[key][d[key] == self.class_to_keep]\n",
    "        return d\n",
    "\n",
    "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = d[key]\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveDicts(MapTransform, InvertibleTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        allow_missing_keys: bool = False,\n",
    "        verbose:bool = False\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            self.push_transform(d, key)\n",
    "        # print(d[\"image_meta_dict\"][\"filename_or_obj\"])\n",
    "        a = {\"image\": d[\"image\"], \"label\": d[\"label\"], \"path\": d[\"image_meta_dict\"][\"filename_or_obj\"]}\n",
    "        if self.verbose:\n",
    "            print(a[\"path\"])\n",
    "        d = a\n",
    "        return d\n",
    "\n",
    "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = d[key]\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintInfo(MapTransform, InvertibleTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            self.push_transform(d, key)\n",
    "        # print(d[\"image_meta_dict\"][\"filename_or_obj\"])\n",
    "        # a = {\"image\": d[\"image\"], \"label\": d[\"label\"], \"path\": d[\"image_meta_dict\"][\"filename_or_obj\"]}\n",
    "        print(d[\"path\"])\n",
    "        # d = a\n",
    "        return d\n",
    "\n",
    "    def inverse(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:\n",
    "        d = deepcopy(dict(data))\n",
    "        for key in self.key_iterator(d):\n",
    "            d[key] = d[key]\n",
    "            # Remove the applied transform\n",
    "            self.pop_transform(d, key)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNUnetScaleIntensity(MapTransform):\n",
    "    \"\"\"\n",
    "    Dictionary-based wrapper of :py:class:`monai.transforms.ScaleIntensityRange`.\n",
    "\n",
    "    Args:\n",
    "        keys: keys of the corresponding items to be transformed.\n",
    "            See also: monai.transforms.MapTransform\n",
    "        a_min: intensity original range min.\n",
    "        a_max: intensity original range max.\n",
    "        b_min: intensity target range min.\n",
    "        b_max: intensity target range max.\n",
    "        clip: whether to perform clip after scaling.\n",
    "        dtype: output data type, if None, same as input image. defaults to float32.\n",
    "        allow_missing_keys: don't raise exception if key is missing.\n",
    "    \"\"\"\n",
    "    def _compute_stats(self, volume, mask):\n",
    "        volume = volume.copy()\n",
    "        mask = np.greater(mask, 0) # get only non-zero positive pixels/labels\n",
    "        volume = volume * mask\n",
    "        volume = np.ma.masked_equal(volume,0).compressed()\n",
    "        if len(volume) == 0:\n",
    "            return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "        median = np.median(volume)\n",
    "        mean = np.mean(volume)\n",
    "        std = np.std(volume)\n",
    "        mn = np.min(volume)\n",
    "        mx = np.max(volume)\n",
    "        percentile_99_5 = np.percentile(volume, 99.5)\n",
    "        percentile_00_5 = np.percentile(volume, 00.5)\n",
    "        # print(median, mean, std, mn, mx, percentile_99_5, percentile_00_5)\n",
    "        return median, mean, std, mn, mx, percentile_99_5, percentile_00_5\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        dtype: DtypeLike = np.float32,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            stats = self._compute_stats(d[key], d['label'])\n",
    "            d[key] = np.clip(d[key], stats[6], stats[5])\n",
    "            d[key] = (d[key] - stats[1]) / stats[2]\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED = True\n",
    "TRANSFER_LEARNING = True\n",
    "N_WORKERS_LOADER = 0\n",
    "N_WORKERS_CACHE = 0\n",
    "CACHE_RATE = 0\n",
    "SEED = 42\n",
    "BS = 1\n",
    "MAX_EPOCHS = 1000\n",
    "PATCH_SIZE = (96, 96, 96)\n",
    "HOME = \"/mnt/chansey\" #\"U:\\\\\" #\"/mnt/netcache/diag\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WzCJRUGGr8a"
   },
   "source": [
    "#### Define the LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spleen_data\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = \"spleen_data\"\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, train_img_size, val_img_size, n_output):\n",
    "        super().__init__()\n",
    "        self.train_img_size = train_img_size\n",
    "        self.val_img_size = val_img_size\n",
    "        self.n_output = n_output\n",
    "        if PRETRAINED:\n",
    "            print(\"using a pretrained model.\")\n",
    "            unet_model = load_from_mmar(\n",
    "                item=mmar[RemoteMMARKeys.NAME],\n",
    "                mmar_dir=root_dir,\n",
    "                # map_location=device,\n",
    "                pretrained=True,\n",
    "            )\n",
    "            self._model = unet_model\n",
    "            # copy all the pretrained weights except for variables whose name matches \"model.0.conv.unit0\"\n",
    "            if TRANSFER_LEARNING:\n",
    "                pretrained_dict, updated_keys, unchanged_keys = copy_model_state(\n",
    "                    self._model,\n",
    "                    unet_model,  # exclude_vars=\"model.[0-2].conv.unit[0-3]\"\n",
    "                )\n",
    "                print(\n",
    "                    \"num. var. using the pretrained\",\n",
    "                    len(updated_keys),\n",
    "                    \", random init\",\n",
    "                    len(unchanged_keys),\n",
    "                    \"variables.\",\n",
    "                )\n",
    "\n",
    "                self._model.load_state_dict(pretrained_dict)\n",
    "                # stop gradients for the pretrained weights\n",
    "                for x in self._model.named_parameters():\n",
    "                    if x[0] in updated_keys:\n",
    "                        x[1].requires_grad = True\n",
    "                params = generate_param_groups(\n",
    "                    network=self._model,\n",
    "                    layer_matches=[lambda x: x[0] in updated_keys],\n",
    "                    match_types=[\"filter\"],\n",
    "                    lr_values=[1e-4],\n",
    "                    include_others=False,\n",
    "                )\n",
    "                self.params = params  # + list(self.crf.parameters())\n",
    "\n",
    "        else:\n",
    "            self._model = UNet(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=3,\n",
    "                channels=(16, 32, 64, 128, 256),\n",
    "                strides=(2, 2, 2, 2),\n",
    "                num_res_units=2,\n",
    "                norm=Norm.BATCH,\n",
    "            )\n",
    "        # self.loss_function = DiceFocalLoss(to_onehot_y=True, softmax=True, jaccard=True)\n",
    "        # weights = [1.0, 1.1, 10]\n",
    "        # class_weights = torch.FloatTensor(weights)\n",
    "        self.loss_function = DiceCELoss(softmax=True, to_onehot_y=True, jaccard =False)\n",
    "        self.post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3)])\n",
    "        self.post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
    "        self.train_dice_metric = DiceMetric(\n",
    "            include_background=False, reduction=\"mean_batch\"\n",
    "        )\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "\n",
    "        # dist_mat = np.array([[0.0, 1.0, 1.0], [1.0, 0.0, 0.5], [1.0, 0.5, 0.0]], dtype=np.float32)\n",
    "        # self.loss_function = GeneralizedWassersteinDiceLoss(dist_matrix=dist_mat)\n",
    "\n",
    "        self.save_hyperparameters()  # save hyperparameters\n",
    "\n",
    "        # self.logger.expe.init(self.hparams)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     out = self._model(x)\n",
    "    #     out = self.crf(out, x)\n",
    "    #     return out\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # set up the correct data path\n",
    "        train_images = sorted(\n",
    "            glob.glob(\n",
    "                os.path.join(\n",
    "                    HOME,\n",
    "                    \"lauraalvarez\",\n",
    "                    \"nnunet\",\n",
    "                    \"nnUNet_raw_data\",\n",
    "                    \"Task510_LiverTraumaDGX\",\n",
    "                    \"imagesTr\",\n",
    "                    \"*.nii.gz\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        train_labels = [img.replace(\"imagesTr\", \"labelsTr\") for img in train_images]\n",
    "        train_labels = [img.replace(\"_0000\", \"\") for img in train_labels]\n",
    "\n",
    "\n",
    "        data_dicts = [\n",
    "            {\"image\": image_name, \"label\": label_name}\n",
    "            for image_name, label_name in zip(train_images, train_labels)\n",
    "        ]\n",
    "\n",
    "        test_images = sorted(\n",
    "            glob.glob(\n",
    "                os.path.join(\n",
    "                    HOME,\n",
    "                    \"lauraalvarez\",\n",
    "                    \"nnunet\",\n",
    "                    \"nnUNet_raw_data\",\n",
    "                    \"Task510_LiverTraumaDGX\",\n",
    "                    \"imagesTs\",\n",
    "                    \"*.nii.gz\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        test_labels = [img.replace(\"imagesTs\", \"labelsTs\") for img in test_images]\n",
    "        train_labels = [img.replace(\"_0000\", \"\") for img in test_labels]\n",
    "\n",
    "\n",
    "        data_dicts_test = [\n",
    "            {\"image\": image_name, \"label\": label_name}\n",
    "            for image_name, label_name in zip(test_images, test_labels)\n",
    "        ]\n",
    "\n",
    "        random.shuffle(data_dicts)\n",
    "        validation_lim = int(len(data_dicts) * 0.9)\n",
    "        train_files, val_files, test_files = data_dicts[:validation_lim], data_dicts[validation_lim:], data_dicts_test\n",
    "        print(\"validation files\", val_files)\n",
    "        print(\"len(train_files)\", len(train_files))\n",
    "        print(\"len(validation files)\", len(val_files))\n",
    "\n",
    "        # set deterministic training for reproducibility\n",
    "        set_determinism(seed=SEED)\n",
    "\n",
    "        # define the data transforms\n",
    "\n",
    "        # Computed for the randomCropByLabel transformation based on outputs\n",
    "        if self.n_output == 3:\n",
    "            ratios = [1, 1, 2]\n",
    "        else:\n",
    "            ratios = [1, 1]\n",
    "\n",
    "        train_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                RemoveDicts(keys=[\"image\", \"label\"]),\n",
    "                AddChanneld(keys=[\"image\", \"label\"]),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.5, 1.5, 2.0),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                NNUnetScaleIntensity(keys=[\"image\"]),\n",
    "                RandCropByLabelClassesd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    label_key=\"label\",\n",
    "                    spatial_size=PATCH_SIZE,\n",
    "                    ratios=ratios,\n",
    "                    num_classes=self.n_output,\n",
    "                    num_samples=4,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[0],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[1],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[2],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandRotate90d(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    prob=0.10,\n",
    "                    max_k=3,\n",
    "                ),\n",
    "                RandShiftIntensityd(\n",
    "                    keys=[\"image\"],\n",
    "                    offsets=0.10,\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # define the data transforms\n",
    "        val_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                RemoveDicts(keys=[\"image\", \"label\"]),\n",
    "                AddChanneld(keys=[\"image\", \"label\"]),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.5, 1.5, 2.0),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                NNUnetScaleIntensity(keys=[\"image\"]),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.train_ds = LMDBDataset(#Dataset(\n",
    "            data=train_files,\n",
    "            transform=train_transforms,\n",
    "            cache_dir=os.path.join(\n",
    "                HOME,\n",
    "                \"lauraalvarez\",\n",
    "                \"data\",\n",
    "                \"lmbd\",\n",
    "                \"LiverTr\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.val_ds = LMDBDataset( #Dataset(\n",
    "            data=val_files,\n",
    "            transform=val_transforms,\n",
    "            cache_dir=os.path.join(\n",
    "                HOME,\n",
    "                \"lauraalvarez\",\n",
    "                \"data\",\n",
    "                \"lmbd\",\n",
    "                \"LiverVs\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self._model.parameters(), 1e-3)\n",
    "        if PRETRAINED:\n",
    "            optimizer = torch.optim.SGD(\n",
    "                self.params, lr=0.1, momentum=0.99, nesterov=True, weight_decay=3e-05\n",
    "            )\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lambda epoch: (1 - self.current_epoch / MAX_EPOCHS) ** 0.9,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"interval\": \"epoch\",\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=BS,\n",
    "            shuffle=True,\n",
    "            num_workers=N_WORKERS_LOADER,\n",
    "            collate_fn=list_data_collate,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=N_WORKERS_LOADER,\n",
    "        )\n",
    "        return val_loader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        predict_dataloader = torch.utils.data.DataLoader(\n",
    "            self.val_ds, batch_size=1, shuffle=False, num_workers=N_WORKERS_LOADER\n",
    "        )\n",
    "        return predict_dataloader\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        output = self.forward(images)\n",
    "\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(output)]\n",
    "\n",
    "        labels_1 = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "\n",
    "        self.train_dice_metric(y_pred=outputs, y=labels_1)\n",
    "\n",
    "        loss = self.loss_function(output, labels)\n",
    "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        #  the function is called after every epoch is completed\n",
    "\n",
    "        dice_liver, dice_injure = self.train_dice_metric.aggregate()\n",
    "        self.train_dice_metric.reset()\n",
    "        # calculating average loss\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "\n",
    "        # logging using tensorboard logger\n",
    "        self.log(\"dice loss\", avg_loss)\n",
    "        # lnp.lnp(f\"Dice Loss: {avg_loss}\")\n",
    "        # self.log(\"train background dice\", dice_background)\n",
    "        self.log(\"train liver dice\", dice_liver)\n",
    "        self.log(\"train injure dice\", dice_injure)\n",
    "\n",
    "        self.logger.experiment.log({\"dice loss\": avg_loss})\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        # filenames = batch[\"image_meta_dict\"][\"filename_or_obj\"]\n",
    "        filenames = batch[\"path\"]\n",
    "        post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3)])\n",
    "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "\n",
    "        roi_size = PATCH_SIZE\n",
    "        sw_batch_size = 4\n",
    "        outputs = sliding_window_inference(\n",
    "            images, roi_size, sw_batch_size, self.forward\n",
    "        )\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        predicition = {\n",
    "            \"output\": torch.nn.Softmax()(outputs),\n",
    "            \"image\": images,\n",
    "            \"label\": labels,\n",
    "            \"filename\": filenames,\n",
    "        }\n",
    "        outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
    "\n",
    "        labels = [post_label(i) for i in decollate_batch(labels)]\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        return {\n",
    "            \"dice_metric\": self.dice_metric,\n",
    "            \"val_number\": len(outputs),\n",
    "            \"prediction\": predicition,\n",
    "            \"val_loss\": loss,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        val_loss, num_items = 0, 0\n",
    "        for output in outputs:\n",
    "            val_loss += output[\"val_loss\"].sum().item()\n",
    "            num_items += output[\"val_number\"]\n",
    "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "\n",
    "        post_pred_dice = Compose(\n",
    "            [\n",
    "                EnsureType(),\n",
    "                AsDiscrete(argmax=True, to_onehot=3),\n",
    "                KeepLargestConnectedComponent(\n",
    "                    [1, 2], is_onehot=True, independent=False, connectivity=2\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "        dice_liver, dice_injure = self.dice_metric.aggregate()\n",
    "\n",
    "        self.dice_metric.reset()\n",
    "        tensorboard_logs = {\n",
    "            \"dice_metric\": dice_injure,\n",
    "        }\n",
    "\n",
    "        predictions = [x[\"prediction\"] for x in outputs]\n",
    "\n",
    "        if (\n",
    "            self.current_epoch % 25 == 0\n",
    "            or dice_injure - self.best_val_dice > 0.1\n",
    "            or self.current_epoch == self.trainer.max_epochs - 1\n",
    "        ):\n",
    "            test_dt = wandb.Table(\n",
    "                columns=[\n",
    "                    \"epoch\",\n",
    "                    \"filename\",\n",
    "                    \"combined output\",\n",
    "                    \"dice_value_liver\",\n",
    "                    \"dice_value_injure\",\n",
    "                    \"ground_truth\",\n",
    "                    \"class predicted\",\n",
    "                ]\n",
    "            )\n",
    "            # figure = computeROC(predictions)\n",
    "            # self.logger.experiment.log({\"ROC\": figure, \"epoch\": self.current_epoch})\n",
    "\n",
    "            for i, prediction in enumerate(predictions):\n",
    "                filename = os.path.basename(prediction[\"filename\"][0])\n",
    "                output_one = [\n",
    "                    post_pred_dice(i) for i in decollate_batch(prediction[\"output\"])\n",
    "                ]\n",
    "                label_one = [\n",
    "                    post_label(i) for i in decollate_batch(prediction[\"label\"])\n",
    "                ]\n",
    "                self.dice_metric(y_pred=output_one, y=label_one)\n",
    "                dice_value_liver, dice_value_injure = self.dice_metric.aggregate()\n",
    "                self.dice_metric.reset()\n",
    "                class_predicted, _, ground_truth = get_classification_info(prediction)\n",
    "                blended = make_gif(prediction, filename=i)\n",
    "                row = [\n",
    "                    self.current_epoch,\n",
    "                    filename,\n",
    "                    wandb.Image(blended),\n",
    "                    dice_value_liver,\n",
    "                    dice_value_injure,\n",
    "                    int(ground_truth[0]),\n",
    "                    class_predicted,\n",
    "                ]\n",
    "                test_dt.add_data(*row)\n",
    "\n",
    "            self.logger.experiment.log({f\"SUMMARY_EPOCH_{self.current_epoch}\": test_dt})\n",
    "\n",
    "        if dice_injure > self.best_val_dice:\n",
    "            self.best_val_dice = dice_injure.item()\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "        print(\n",
    "            f\"current epoch: {self.current_epoch} \"\n",
    "            f\"current liver dice: {dice_liver:.4f}\"\n",
    "            f\"current injure  dice: {dice_injure:.4f}\"\n",
    "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
    "            f\"at epoch: {self.best_val_epoch}\"\n",
    "        )\n",
    "        self.log(\"dice_metric_liver\", dice_liver.item(), prog_bar=True)\n",
    "        self.log(\"dice_metric_injure\", dice_injure.item(), prog_bar=True)\n",
    "        self.log(\"val_loss\", mean_val_loss, prog_bar=True)\n",
    "        return {\"log\": tensorboard_logs}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        print(\"predicting...\")\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=3)])\n",
    "        post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 2\n",
    "        outputs = sliding_window_inference(\n",
    "            images, roi_size, sw_batch_size, self.forward\n",
    "        )\n",
    "        predicition = {\"output\": outputs, \"image\": images, \"label\": labels}\n",
    "        outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
    "\n",
    "        labels = [\n",
    "            post_label(torch.unsqueeze(i, 0)).squeeze() for i in decollate_batch(labels)\n",
    "        ]\n",
    "        dice_metric = self.dice_metric(y_pred=outputs, y=labels)\n",
    "        return {\"prediction\": predicition, \"dice_metric\": dice_metric}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gif Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(prediction, filename):\n",
    "    def _save_gif(volume, filename):\n",
    "        volume = volume.astype(np.float64) / np.max(volume) # normalize the data to 0 - 1\n",
    "        volume = 255 * volume # Now scale by 255\n",
    "        volume = volume.astype(np.uint8)\n",
    "        path_to_gif = os.path.join(\"gifs\", f'{filename}.gif')\n",
    "        if not os.path.exists(\"gifs\"):\n",
    "            print(\"Creating gifs directory\")\n",
    "            os.mkdir(\"gifs\")\n",
    "        imageio.mimsave(path_to_gif, volume)\n",
    "        return path_to_gif\n",
    "    post_pred_blending = Compose([EnsureType(), AsDiscrete(argmax=True),KeepLargestConnectedComponent([1,2], is_onehot=False, independent=False, connectivity=2)])\n",
    "    prediction[\"output\"] = [post_pred_blending(i) for i in decollate_batch(prediction[\"output\"])]\n",
    "    selected = {\"output\": prediction[\"output\"][0], \"image\": prediction[\"image\"][0], \"label\": prediction[\"label\"][0]}\n",
    "\n",
    "    selected = Resized(keys=[\"image\", \"label\"], spatial_size=(160, 160, 160))(selected)\n",
    "    selected = Resized(keys=[\"output\"], spatial_size=(160, 160, 160))(selected)\n",
    "\n",
    "    selected = {\"output\": selected[\"output\"], \"image\": selected[\"image\"].unsqueeze(0), \"label\": selected[\"label\"].unsqueeze(0)}\n",
    "\n",
    "\n",
    "    pred = selected['output'].detach().cpu().numpy()\n",
    "    true_label = selected['label'][0].detach().cpu().numpy()\n",
    "    image = selected['image'][0].cpu().numpy()\n",
    "    \n",
    "    blended_true_label = blend_images(image, true_label, alpha=0.7)\n",
    "    blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
    "\n",
    "    blended_prediction = blend_images(image, pred, alpha=0.7)\n",
    "\n",
    "    blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
    "\n",
    "    volume_pred = blended_final_prediction[:,:,:,:]\n",
    "    volume_label = blended_final_true_label[:,:,:,:]\n",
    "    volume_pred = volume_pred.permute(3,0,1,2).cpu()\n",
    "    volume_label = np.squeeze(volume_label).permute(3,0,1,2).cpu()\n",
    "    volume_img = torch.tensor(image).permute(3,1,2,0).repeat(1,1,1,3).cpu()\n",
    "\n",
    "    volume = torch.hstack((volume_img, volume_pred, volume_label))\n",
    "\n",
    "    volume_path = _save_gif(volume.numpy(), f\"blended-{filename}\")\n",
    "       \n",
    "    \n",
    "    return volume_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_info(prediction):\n",
    "    post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "\n",
    "    ground_truth = [\n",
    "            1 if (post_label(i)[2,:,:,:].cpu() == 1).any() else 0 for i in decollate_batch(prediction['label'])\n",
    "        ]\n",
    "\n",
    "    test = prediction['output'].cpu()\n",
    "    prediction_1 = torch.argmax(test, dim=1)\n",
    "\n",
    "    class_2_mask = (prediction_1 == 2).cpu()\n",
    "    if class_2_mask.any():\n",
    "        prediction = torch.max(test[:,2,:,:,:]).item()\n",
    "    else:\n",
    "        prediction = np.max(np.ma.masked_array(test[:,2,:,:,:], mask=class_2_mask))\n",
    "    \n",
    "    unique_values = torch.unique(prediction_1)\n",
    "    predicted_class = 1 if 2 in unique_values else 0\n",
    "    \n",
    "    return predicted_class, prediction, ground_truth\n",
    "\n",
    "def computeROC(predictions):\n",
    "    from sklearn.metrics import roc_curve, auc # roc curve tools\n",
    "    \n",
    "    g_truths = []\n",
    "    preds = []\n",
    "    for prediction in predictions:\n",
    "        _, predict, ground_truth = get_classification_info(prediction)\n",
    "        g_truths.extend(ground_truth)\n",
    "        preds.append(predict)\n",
    "\n",
    "    preds = np.asarray(preds)\n",
    "    ground_truth = np.asarray(g_truths)\n",
    "    fpr, tpr, _ = roc_curve(g_truths, preds)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhnD1E-uGr8c"
   },
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 0\n",
    "IMG_SIZE = (96,96,96)\n",
    "VAL_SIZE = (256,256,256)\n",
    "SAVE_PATH = os.path.join(HOME, \"lauraalvarez\", \"traumaAI\", \"Liver_Segmentation\", \"lightning_logs\")\n",
    "run_idx = len(os.listdir(os.path.join(HOME, \"lauraalvarez\", \"traumaAI\", \"Liver_Segmentation\", \"wandb\")))\n",
    "RUN_NAME = f\"Predict_Segmentation_UNETPRE_{run_idx+1}\"\n",
    "pytorch_lightning.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmar = {\n",
    "    RemoteMMARKeys.ID: \"clara_pt_liver_and_tumor_ct_segmentation_1\",\n",
    "    RemoteMMARKeys.NAME: \"clara_pt_liver_and_tumor_ct_segmentation\",\n",
    "    RemoteMMARKeys.FILE_TYPE: \"zip\",\n",
    "    RemoteMMARKeys.HASH_TYPE: \"md5\",\n",
    "    RemoteMMARKeys.HASH_VAL: None,\n",
    "    RemoteMMARKeys.MODEL_FILE: os.path.join(\"models\", \"model.pt\"),\n",
    "    RemoteMMARKeys.CONFIG_FILE: os.path.join(\"config\", \"config_train.json\"),\n",
    "    RemoteMMARKeys.VERSION: 1,\n",
    "}\n",
    "\n",
    "def save_checkpoint(state, name):\n",
    "    file_path = \"checkpoints/\"\n",
    "    if not os.path.exists(file_path): \n",
    "        os.makedirs(file_path)\n",
    "    epoch = state[\"epoch\"]\n",
    "    save_dir = file_path + name + str(epoch)\n",
    "    torch.save(state, save_dir)\n",
    "    print(f\"Saving checkpoint for epoch {epoch} in: {save_dir}\")\n",
    "\n",
    "def save_state_dict(state, name):\n",
    "    file_path = \"checkpoints/\"\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    save_dir = file_path + f\"{name}_best\"\n",
    "    torch.save(state, save_dir)\n",
    "    print(f\"Best accuracy so far. Saving model to:{save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_and_print:\n",
    "    def __init__(self, run_name, tb_logger=None):\n",
    "        self.tb_logger = tb_logger\n",
    "        self.run_name = run_name\n",
    "        self.str_log = \"run_name\" + \"\\n  \\n\"\n",
    "\n",
    "    def lnp(self, tag):\n",
    "        print(self.run_name, time.asctime(), tag)\n",
    "        self.str_log += str(time.asctime()) + \" \" + str(tag) + \"  \\n\"\n",
    "\n",
    "    def dump_to_tensorboard(self):\n",
    "        if not self.tb_logger:\n",
    "            print(\"No tensorboard logger\")\n",
    "        self.tb_logger.experiment.add_text(\"log\", self.str_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wandb.finish()\n",
    "except:\n",
    "    print(\"Wandb not initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Segmentation_UNETPRE_309 Wed Jul 20 14:56:42 2022 Loggers start\n",
      "Predict_Segmentation_UNETPRE_309 Wed Jul 20 14:56:42 2022 ts_script: 1658321802.0866246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlalvarez\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/wandb/run-20220720_145644-2spe73t3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/lalvarez/traumaIA/runs/2spe73t3\" target=\"_blank\">Predict_Segmentation_UNETPRE_309</a></strong> to <a href=\"https://wandb.ai/lalvarez/traumaIA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lnp = Log_and_print(RUN_NAME)\n",
    "lnp.lnp(\"Loggers start\")\n",
    "lnp.lnp(\"ts_script: \" + str(time.time()))\n",
    "\n",
    "wandb_logger = pytorch_lightning.loggers.WandbLogger(\n",
    "    project=\"traumaIA\",\n",
    "    name=RUN_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALLBACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Segmentation_UNETPRE_309 Wed Jul 20 14:56:50 2022 MAIN callbacks\n",
      "Predict_Segmentation_UNETPRE_309 Wed Jul 20 14:56:50 2022 checkpoint_dirpath: /mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/lightning_logscheckpoints/\n",
      "Predict_Segmentation_UNETPRE_309 Wed Jul 20 14:56:50 2022 checkpoint_filename: /mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/lightning_log_Predict_Segmentation_UNETPRE_309_Best\n",
      "Predict_Segmentation_UNETPRE_309 Wed Jul 20 14:56:50 2022 checkpoint_dirpath: /mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/lightning_logscheckpoints/\n",
      "Predict_Segmentation_UNETPRE_309 Wed Jul 20 14:56:50 2022 checkpoint_filename: /mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/lightning_log_Predict_Segmentation_UNETPRE_309_Last\n"
     ]
    }
   ],
   "source": [
    "lnp.lnp(\"MAIN callbacks\")\n",
    "l_callbacks = []\n",
    "cbEarlyStopping = pytorch_lightning.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=500, mode=\"max\"\n",
    ")\n",
    "l_callbacks.append(cbEarlyStopping)\n",
    "\n",
    "\n",
    "checkpoint_dirpath = SAVE_PATH + \"checkpoints/\"\n",
    "checkpoint_filename = SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"_Best\"\n",
    "lnp.lnp(\"checkpoint_dirpath: \" + checkpoint_dirpath)\n",
    "lnp.lnp(\"checkpoint_filename: \" + checkpoint_filename)\n",
    "cbModelCheckpoint = pytorch_lightning.callbacks.ModelCheckpoint(\n",
    "    monitor=\"dice_metric_injure\", mode=\"max\", dirpath=checkpoint_dirpath, filename=checkpoint_filename, \n",
    ")\n",
    "l_callbacks.append(cbModelCheckpoint)\n",
    "\n",
    "\n",
    "checkpoint_dirpath = SAVE_PATH + \"checkpoints/\"\n",
    "checkpoint_filename = SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"_Last\"\n",
    "lnp.lnp(\"checkpoint_dirpath: \" + checkpoint_dirpath)\n",
    "lnp.lnp(\"checkpoint_filename: \" + checkpoint_filename)\n",
    "cbModelCheckpointLast = pytorch_lightning.callbacks.ModelCheckpoint(\n",
    "   every_n_epochs = 1, dirpath=checkpoint_dirpath, filename=checkpoint_filename, \n",
    ")\n",
    "l_callbacks.append(cbModelCheckpointLast)\n",
    "\n",
    "l_callbacks.append(PrintTableMetricsCallback())\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "l_callbacks.append(lr_monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Segmentation_UNETPRE_309 Wed Jul 20 14:56:50 2022  Start Trainining process...\n",
      "using a pretrained model.\n",
      "2022-07-20 14:56:52,148 - INFO - Expected md5 is None, skip md5 check for file spleen_data/clara_pt_liver_and_tumor_ct_segmentation_4.1.zip.\n",
      "2022-07-20 14:56:52,151 - INFO - File exists: spleen_data/clara_pt_liver_and_tumor_ct_segmentation_4.1.zip, skipped downloading.\n",
      "2022-07-20 14:56:52,159 - INFO - Non-empty folder exists in spleen_data/clara_pt_liver_and_tumor_ct_segmentation, skipped extracting.\n",
      "2022-07-20 14:56:52,162 - INFO - \n",
      "*** \"clara_pt_liver_and_tumor_ct_segmentation\" available at spleen_data/clara_pt_liver_and_tumor_ct_segmentation.\n",
      "2022-07-20 14:57:01,138 - INFO - *** Model: <class 'monai.networks.nets.unet.UNet'>\n",
      "2022-07-20 14:57:01,190 - INFO - *** Model params: {'dimensions': 3, 'in_channels': 1, 'out_channels': 3, 'channels': [16, 32, 64, 128, 256], 'strides': [2, 2, 2, 2], 'num_res_units': 2, 'norm': 'batch'}\n",
      "2022-07-20 14:57:01,215 - INFO - \n",
      "---\n",
      "2022-07-20 14:57:01,216 - INFO - For more information, please visit https://ngc.nvidia.com/catalog/models/nvidia:med:clara_pt_liver_and_tumor_ct_segmentation\n",
      "\n",
      "'dst' model updated: 148 of 148 variables.\n",
      "num. var. using the pretrained 148 , random init 0 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation files [{'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data/Task510_LiverTraumaDGX/imagesTr/TLIV_038_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data/Task510_LiverTraumaDGX/labelsTr/TLIV_038.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data/Task510_LiverTraumaDGX/imagesTr/TLIV_018_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data/Task510_LiverTraumaDGX/labelsTr/TLIV_018.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data/Task510_LiverTraumaDGX/imagesTr/TLIV_002_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data/Task510_LiverTraumaDGX/labelsTr/TLIV_002.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data/Task510_LiverTraumaDGX/imagesTr/TLIV_031_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data/Task510_LiverTraumaDGX/labelsTr/TLIV_031.nii.gz'}, {'image': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data/Task510_LiverTraumaDGX/imagesTr/TLIV_028_0000.nii.gz', 'label': '/mnt/chansey/lauraalvarez/nnunet/nnUNet_raw_data/Task510_LiverTraumaDGX/labelsTr/TLIV_028.nii.gz'}]\n",
      "len(train_files) 40\n",
      "len(validation files) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [23:11<00:00, 34.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing lmdb file: /mnt/chansey/lauraalvarez/data/lmbd/LiverTr/monai_cache.lmdb.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:48<00:00, 33.70s/it]\n",
      "Checkpoint directory /mnt/chansey/lauraalvarez/traumaAI/Liver_Segmentation/lightning_logscheckpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Found unsupported keys in the optimizer configuration: {'interval'}\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | _model        | UNet       | 4.8 M \n",
      "1 | loss_function | DiceCELoss | 0     \n",
      "---------------------------------------------\n",
      "4.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 M     Total params\n",
      "19.240    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing lmdb file: /mnt/chansey/lauraalvarez/data/lmbd/LiverVs/monai_cache.lmdb.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 28 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 28 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dd9a4fbf2846f0aeb9719016eb921e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# initialise the LightningModule\n",
    "lnp.lnp(\" Start Trainining process...\")\n",
    "net = Net(train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=3)#.load_from_checkpoint(\"lightning_logs/checkpoints/lightning_logs_Predict_Segmentation_240_Last-v1.ckpt\", train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=3)\n",
    "wandb_logger.watch(net)\n",
    "\n",
    "# set up loggers and checkpoints\n",
    "log_dir = os.path.join(root_dir, \"logs\")\n",
    "\n",
    "# initialise Lightning's trainer.\n",
    "trainer = pytorch_lightning.Trainer(\n",
    "    default_root_dir=\"lightning_logs/checkpoints\",\n",
    "    gpus=[0],\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    fast_dev_run=False,\n",
    "    auto_lr_find=False,\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=l_callbacks,\n",
    ")\n",
    "\n",
    "# train\n",
    "result_pred2 = trainer.fit(net)\n",
    "wandb.alert(\n",
    "    title=\"Train finished\", \n",
    "    text=\"The train has finished\"\n",
    ")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net(train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=3).load_from_checkpoint(\"lightning_logs/checkpoints/lightning_logs_Predict_Segmentation_168_Best.ckpt\", train_img_size=IMG_SIZE, val_img_size=VAL_SIZE, n_output=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pytorch_lightning.Trainer(\n",
    "#     default_root_dir=\"lightning_logs/checkpoints\",\n",
    "#     gpus=[0],\n",
    "#     max_epochs=1000,\n",
    "#     auto_lr_find=False,\n",
    "#     logger=wandb_logger,\n",
    "#     enable_checkpointing=True,\n",
    "#     num_sanity_val_steps=0,\n",
    "#     log_every_n_steps=1,\n",
    "#     callbacks=l_callbacks,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = trainer.predict(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_pred = Compose([EnsureType(), AsDiscrete(argmax=True),KeepLargestConnectedComponent([1,2], is_onehot=False, independent=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_one = [post_pred(i) for i in decollate_batch(predictions[1][\"prediction\"][\"output\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _save_gif(volume, filename):\n",
    "#     volume = volume.astype(np.float64) / np.max(volume) # normalize the data to 0 - 1\n",
    "#     volume = 255 * volume # Now scale by 255\n",
    "#     volume = volume.astype(np.uint8)\n",
    "#     path_to_gif = os.path.join(\"gifs\", f'{filename}.gif')\n",
    "#     if not os.path.exists(\"gifs\"):\n",
    "#         print(\"Creating gifs directory\")\n",
    "#         os.mkdir(\"gifs\")\n",
    "#     imageio.mimsave(path_to_gif, volume)\n",
    "#     return path_to_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _save_gif(output_one[0].permute(3,1,2,0).cpu().numpy(), \"test-after\")\n",
    "# _save_gif(predictions[1][\"prediction\"][\"output\"][0].permute(3,1,2,0).cpu().numpy(), \"test-before\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests Gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = predictions[1][\"prediction\"]\n",
    "# selected = {\"output\":  output_one[0], \"image\": prediction[\"image\"][0], \"label\": prediction[\"label\"][0]}\n",
    "# selected = Resized(keys=[\"image\", \"label\"], spatial_size=(160, 160, 160))(selected)\n",
    "# selected = Resized(keys=[\"output\"], spatial_size=(160, 160, 160))(selected)\n",
    "# selected = {\"output\": selected[\"output\"].unsqueeze(0), \"image\": selected[\"image\"].unsqueeze(0), \"label\": selected[\"label\"].unsqueeze(0)}\n",
    "# # print('true label:', selected['label'].shape)\n",
    "# pred = selected[\"output\"][0].detach().cpu().numpy()\n",
    "# true_label = selected['label'][0].detach().cpu().numpy()\n",
    "# image = selected['image'][0].cpu().numpy()\n",
    "# # print('true label:', true_label.shape)\n",
    "\n",
    "# blended_true_label = blend_images(image, true_label, alpha=0.3)\n",
    "# blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
    "\n",
    "# blended_prediction = blend_images(image, pred, alpha=0.3)\n",
    "# blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
    "\n",
    "# volume_pred = blended_final_prediction[:,:,:,:]\n",
    "# volume_label = blended_final_true_label[:,:,:,:]\n",
    "# volume_pred = np.squeeze(volume_pred).permute(3,0,1,2).cpu()\n",
    "# volume_label = np.squeeze(volume_label).permute(3,0,1,2).cpu()\n",
    "# volume_img = torch.tensor(image).permute(3,1,2,0).repeat(1,1,1,3).cpu()\n",
    "\n",
    "# volume = torch.hstack((volume_img, volume_pred, volume_label))\n",
    "\n",
    "# volume_path = _save_gif(volume.numpy(), f\"blended-test-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = predictions[1][\"prediction\"]\n",
    "\n",
    "# selected = {\"output\": prediction[\"output\"][0], \"image\": prediction[\"image\"][0], \"label\": prediction[\"label\"][0]}\n",
    "# selected = Resized(keys=[\"image\", \"label\"], spatial_size=(160, 160, 160))(selected)\n",
    "# selected = Resized(keys=[\"output\"], spatial_size=(160, 160, 160))(selected)\n",
    "# selected = {\"output\": torch.argmax(selected[\"output\"], 0).unsqueeze(0), \"image\": selected[\"image\"].unsqueeze(0), \"label\": selected[\"label\"].unsqueeze(0)}\n",
    "\n",
    "# pred = selected[\"output\"].detach().cpu().numpy()\n",
    "# true_label = selected['label'][0].detach().cpu().numpy()\n",
    "# image = selected['image'][0].cpu().numpy()\n",
    "\n",
    "# blended_true_label = blend_images(image, true_label, alpha=0.3)\n",
    "# blended_final_true_label = torch.from_numpy(blended_true_label).permute(1,2,0,3)\n",
    "\n",
    "# blended_prediction = blend_images(image, pred, alpha=0.3)\n",
    "# blended_final_prediction = torch.from_numpy(blended_prediction).permute(1,2,0,3)\n",
    "\n",
    "# volume_pred = blended_final_prediction[:,:,:,:]\n",
    "# volume_label = blended_final_true_label[:,:,:,:]\n",
    "# volume_pred = np.squeeze(volume_pred).permute(3,0,1,2).cpu()\n",
    "# volume_label = np.squeeze(volume_label).permute(3,0,1,2).cpu()\n",
    "# volume_img = torch.tensor(image).permute(3,1,2,0).repeat(1,1,1,3).cpu()\n",
    "\n",
    "# volume = torch.hstack((volume_img, volume_pred, volume_label))\n",
    "\n",
    "# volume_path = _save_gif(volume.numpy(), f\"blended-test-1-org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_checkpoint(SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"_Last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE_PATH[:-1] + \"_\" + RUN_NAME + \"Last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spleen_segmentation_3d_lightning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env_trauma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "16d90db93701a4e14595aea1a6791a3dd0d33758ed6b394279d759beaff9b73f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
